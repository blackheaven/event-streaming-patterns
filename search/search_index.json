{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems.","title":"Welcome to Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems.","title":"Welcome to Event Streaming Patterns"},{"location":"compositional-patterns/event-collaboration/","text":"Event Collaboration Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states. Problem How can I build a distributed workflow in a way that allows components to evolve independently? Solution Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. Stream Processing Applications observe the Events and trigger subsequent actions and resulting Events. The process repeats until the complex workflow is complete. Considerations Events need to be correlated through the complex distributed workflow. The Correlation Identifier pattern describes a method of coupling Events when processed asyncronously by way of a global identifier which traverses the workflow within the events. References TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#event-collaboration","text":"Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states.","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#problem","text":"How can I build a distributed workflow in a way that allows components to evolve independently?","title":"Problem"},{"location":"compositional-patterns/event-collaboration/#solution","text":"Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. Stream Processing Applications observe the Events and trigger subsequent actions and resulting Events. The process repeats until the complex workflow is complete.","title":"Solution"},{"location":"compositional-patterns/event-collaboration/#considerations","text":"Events need to be correlated through the complex distributed workflow. The Correlation Identifier pattern describes a method of coupling Events when processed asyncronously by way of a global identifier which traverses the workflow within the events.","title":"Considerations"},{"location":"compositional-patterns/event-collaboration/#references","text":"TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"compositional-patterns/event-stream-observer/","text":"Event Stream Observer Many architectures have streams of events deployed across multiple datacenters spanning boundaries of event streaming platforms, datacenters, or geo-regions. In these situations, it may be useful for client applications in one datacenter to have access to events produced in another datacenter. All clients shouldn't be forced to read from the source datacenter, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be mirrored to the destination datacenter and then the client there can \"observe\" the data. Problem How can multiple event streaming platforms be connected so that events available in one site are also available on the others? Solution Create a connection between the two systems, enabling the destination system to read from the source system. Ideally this is done in realtime such that as new events are published in the source datacenter, they can be immediately copied, byte for byte, to the destination datacenter. This allows the client applications in the destination to leverage the same set of data. Implementation Practically, replication is not enabled on two complete datacenters, as there are always exceptions, organizational limitations, technical constraints, or other reasons why you wouldn't want to copy absolutely everything. Instead, you can do this on a per topic basis, where you can map a source topic to a destination topic. With Apache Kafka, you can do this in one of several ways. Option 1: Cluster Linking Cluster Linking enables easy data sharing between datacenters, mirroring topics across clusters. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Option 2: MirrorMaker Operators can set up such inter-cluster data flows with Kafka's MirrorMaker (version 2), a tool to replicate data between different Kafka environments. Unlike Cluster Linking, it's a separate service built upon Kafka Connect, with built-in producers and consumers. Considerations Note that this type of replication between clusters is asynchronous, which means an event that is recorded in the source cluster may be available before that event is recorded in the destination cluster. There is also synchronous replication across clusters (e.g. Multi Region Clusters ) but this is often limited to when the datacenters are in the same operational domain. References This pattern is derived from Message Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Stream Observer"},{"location":"compositional-patterns/event-stream-observer/#event-stream-observer","text":"Many architectures have streams of events deployed across multiple datacenters spanning boundaries of event streaming platforms, datacenters, or geo-regions. In these situations, it may be useful for client applications in one datacenter to have access to events produced in another datacenter. All clients shouldn't be forced to read from the source datacenter, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be mirrored to the destination datacenter and then the client there can \"observe\" the data.","title":"Event Stream Observer"},{"location":"compositional-patterns/event-stream-observer/#problem","text":"How can multiple event streaming platforms be connected so that events available in one site are also available on the others?","title":"Problem"},{"location":"compositional-patterns/event-stream-observer/#solution","text":"Create a connection between the two systems, enabling the destination system to read from the source system. Ideally this is done in realtime such that as new events are published in the source datacenter, they can be immediately copied, byte for byte, to the destination datacenter. This allows the client applications in the destination to leverage the same set of data.","title":"Solution"},{"location":"compositional-patterns/event-stream-observer/#implementation","text":"Practically, replication is not enabled on two complete datacenters, as there are always exceptions, organizational limitations, technical constraints, or other reasons why you wouldn't want to copy absolutely everything. Instead, you can do this on a per topic basis, where you can map a source topic to a destination topic. With Apache Kafka, you can do this in one of several ways. Option 1: Cluster Linking Cluster Linking enables easy data sharing between datacenters, mirroring topics across clusters. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Option 2: MirrorMaker Operators can set up such inter-cluster data flows with Kafka's MirrorMaker (version 2), a tool to replicate data between different Kafka environments. Unlike Cluster Linking, it's a separate service built upon Kafka Connect, with built-in producers and consumers.","title":"Implementation"},{"location":"compositional-patterns/event-stream-observer/#considerations","text":"Note that this type of replication between clusters is asynchronous, which means an event that is recorded in the source cluster may be available before that event is recorded in the destination cluster. There is also synchronous replication across clusters (e.g. Multi Region Clusters ) but this is often limited to when the datacenters are in the same operational domain.","title":"Considerations"},{"location":"compositional-patterns/event-stream-observer/#references","text":"This pattern is derived from Message Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event/correlation-identifier/","text":"Correlation Identifier Event Processing Applications may want to implement an Event Collaboration pattern where Events will serve as both requests and responses. Decoupled Event Processors will need to be able to asynchronously correlate Events. Problem How does an application that has requested information and received a reply know which request this is the reply for? Solution Add a globally unique identifier to an Event that acts as the request. When a responding Event Processor generates a response Event, it attaches the corresponding identifier to the response Event allowing the requesting processor to correlate the request and response Events. Considerations TODO: Technology specific reflection on implementing the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References This pattern is derived from Correlation Identifier in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf For a case study on coordinating microservices twoards higher level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB Correlation Identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#correlation-identifier","text":"Event Processing Applications may want to implement an Event Collaboration pattern where Events will serve as both requests and responses. Decoupled Event Processors will need to be able to asynchronously correlate Events.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#problem","text":"How does an application that has requested information and received a reply know which request this is the reply for?","title":"Problem"},{"location":"event/correlation-identifier/#solution","text":"Add a globally unique identifier to an Event that acts as the request. When a responding Event Processor generates a response Event, it attaches the corresponding identifier to the response Event allowing the requesting processor to correlate the request and response Events.","title":"Solution"},{"location":"event/correlation-identifier/#considerations","text":"TODO: Technology specific reflection on implementing the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event/correlation-identifier/#references","text":"This pattern is derived from Correlation Identifier in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf For a case study on coordinating microservices twoards higher level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB Correlation Identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution.","title":"References"},{"location":"event/event-standardizer/","text":"Event Standardizer A variety of traditional and Event Processing Applications will need to exchange Events across Event Streams . Downstream Event Processing Applications will require standardized data formats in order to properly process Events of different formats. Problem How do I process events that are semantically equivalent, but arrive in different formats? Solution Source all the input Event Streams into an Event Standardizer which routes Events to a specialized Event Translator which converts the Event to a normalized form expected by the downstream Event Processors . Implementation A Kafka Streams Toplogy can read from multiple input Event Streams and map the values to a new type. This map function can act as the event router, directing the Event to the proper Event Translator before forwading it to the output stream using the to function. SpecificAvroSerde<SpecificRecord> inputValueSerde = ... builder .stream(List.of(\"inputStreamA\", \"inputStreamB\", \"inputStreamC\"), Consumed.with(Serdes.String(), inputValueSerde)) .mapValues((k, v) -> { if (v.getClass() == TypeA.class) return typeATranslator.normalize(v); else if (v.getClass() == TypeB.class) return typeBTranslator.normalize(v); else if (v.getClass() == TypeC.class) return typeCTranslator.normalize(v); else { // exception or dead letter stream } }) .to(\"outputStream\", Produced.with(Serdes.String(), outputSerdeType); Considerations TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References This pattern is derived from Normalizer in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Kafka Streams map stateless transformation documentation","title":"Event Standardizer"},{"location":"event/event-standardizer/#event-standardizer","text":"A variety of traditional and Event Processing Applications will need to exchange Events across Event Streams . Downstream Event Processing Applications will require standardized data formats in order to properly process Events of different formats.","title":"Event Standardizer"},{"location":"event/event-standardizer/#problem","text":"How do I process events that are semantically equivalent, but arrive in different formats?","title":"Problem"},{"location":"event/event-standardizer/#solution","text":"Source all the input Event Streams into an Event Standardizer which routes Events to a specialized Event Translator which converts the Event to a normalized form expected by the downstream Event Processors .","title":"Solution"},{"location":"event/event-standardizer/#implementation","text":"A Kafka Streams Toplogy can read from multiple input Event Streams and map the values to a new type. This map function can act as the event router, directing the Event to the proper Event Translator before forwading it to the output stream using the to function. SpecificAvroSerde<SpecificRecord> inputValueSerde = ... builder .stream(List.of(\"inputStreamA\", \"inputStreamB\", \"inputStreamC\"), Consumed.with(Serdes.String(), inputValueSerde)) .mapValues((k, v) -> { if (v.getClass() == TypeA.class) return typeATranslator.normalize(v); else if (v.getClass() == TypeB.class) return typeBTranslator.normalize(v); else if (v.getClass() == TypeC.class) return typeCTranslator.normalize(v); else { // exception or dead letter stream } }) .to(\"outputStream\", Produced.with(Serdes.String(), outputSerdeType);","title":"Implementation"},{"location":"event/event-standardizer/#considerations","text":"TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event/event-standardizer/#references","text":"This pattern is derived from Normalizer in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Kafka Streams map stateless transformation documentation","title":"References"},{"location":"event/event/","text":"Event Events represent facts and can help facilitate two decoupled applications exchanging data across an Event Streaming Platform . Problem How do I represent a fact about something that has happened? Solution An event is an immutable fact about something that has happened. It is produced and consumed from an Event Stream . Events contain data, timestamps, and may contain various metadata about the event (event headers, location in the stream, etc). Considerations Events are often created in reference to a schema (TODO:pattern-ref) commonly defined in Avro , Protobuf , or JSON schema . The emerging standard: Cloud Events provides a standardized envelope that wraps event data, making common event properties such as source, type, time, ID, and more, universally accessible regardless of the event payload. While events are necessarily a 'fact', in many cases they also imply movement: the communicating of facts about the world from one piece of software to another. References This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event"},{"location":"event/event/#event","text":"Events represent facts and can help facilitate two decoupled applications exchanging data across an Event Streaming Platform .","title":"Event"},{"location":"event/event/#problem","text":"How do I represent a fact about something that has happened?","title":"Problem"},{"location":"event/event/#solution","text":"An event is an immutable fact about something that has happened. It is produced and consumed from an Event Stream . Events contain data, timestamps, and may contain various metadata about the event (event headers, location in the stream, etc).","title":"Solution"},{"location":"event/event/#considerations","text":"Events are often created in reference to a schema (TODO:pattern-ref) commonly defined in Avro , Protobuf , or JSON schema . The emerging standard: Cloud Events provides a standardized envelope that wraps event data, making common event properties such as source, type, time, ID, and more, universally accessible regardless of the event payload. While events are necessarily a 'fact', in many cases they also imply movement: the communicating of facts about the world from one piece of software to another.","title":"Considerations"},{"location":"event/event/#references","text":"This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processors may need to operate over a subset of Events over a particular Event Stream . Problem How can an application discard uninteresting events? Solution Implementation The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processors may need to operate over a subset of Events over a particular Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa. Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Implementation Using standard Kafka producer, you can use an abstract Mapper concept to construct an Event instance ( PublicationEvent ) representing the Domain Model ( Publication ) prior to producing to Kafka. private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author/*key*/, mapper.map(newPub)); An application wishing to convert PublicationEvent instances to Domain Object updates, can do so with a Mapper that can do the reverse operation: private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = mapper.map(pubEvent); domainStore.update(newPub); TODO: Rick Feedback Request: How could we use ksqlDB or Kafka Streams her? The following KStreams example doesn't work to me because it's a stream processing application, but Mapper means to convert between domain objects and events. I'm not sure there is a logical way to \"push\" an event to a toplogy manually withouth producing to a topic. IMapper mapper = mapperFactory.buildMapper(Publication.class); builder.stream(inputTopic, Consumed.with(Serdes.String(), publicationSerde)) .map((name, publication) -> mapper.map(publication)) .to(outputTopic, Produced.with(Serdes.String(), publicationEventSerde)); Considerations TODO: Considerations? References This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf TODO: Reference to Event-Carried State Transfer? TODO: Reference to Document Message / Event vs Command Message / Event? TODO: Is Database Write Through / CDC a valid reference?","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution"},{"location":"event-processing/event-mapper/#implementation","text":"Using standard Kafka producer, you can use an abstract Mapper concept to construct an Event instance ( PublicationEvent ) representing the Domain Model ( Publication ) prior to producing to Kafka. private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author/*key*/, mapper.map(newPub)); An application wishing to convert PublicationEvent instances to Domain Object updates, can do so with a Mapper that can do the reverse operation: private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = mapper.map(pubEvent); domainStore.update(newPub); TODO: Rick Feedback Request: How could we use ksqlDB or Kafka Streams her? The following KStreams example doesn't work to me because it's a stream processing application, but Mapper means to convert between domain objects and events. I'm not sure there is a logical way to \"push\" an event to a toplogy manually withouth producing to a topic. IMapper mapper = mapperFactory.buildMapper(Publication.class); builder.stream(inputTopic, Consumed.with(Serdes.String(), publicationSerde)) .map((name, publication) -> mapper.map(publication)) .to(outputTopic, Produced.with(Serdes.String(), publicationEventSerde));","title":"Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"TODO: Considerations?","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf TODO: Reference to Event-Carried State Transfer? TODO: Reference to Document Message / Event vs Command Message / Event? TODO: Is Database Write Through / CDC a valid reference?","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application An Event Processing Application uses one or more Event Processor instances to handle streaming event driven data. Problem How can I build an application to work with streaming event data? Solution Using an Event Processing Application allows you to tie together indpendant event processors for working with streaming event recorods and are not aware of each other. Implementation StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); .... KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties); kafkaStreams.start() Considerations When building an Event Processing Application, it's important to generally confine the application to one problem domain. While it's true the application can have any number of event processors, they should be closely related. References","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"An Event Processing Application uses one or more Event Processor instances to handle streaming event driven data.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"How can I build an application to work with streaming event data?","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"Using an Event Processing Application allows you to tie together indpendant event processors for working with streaming event recorods and are not aware of each other.","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); .... KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties); kafkaStreams.start()","title":"Implementation"},{"location":"event-processing/event-processing-application/#considerations","text":"When building an Event Processing Application, it's important to generally confine the application to one problem domain. While it's true the application can have any number of event processors, they should be closely related.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor An event processor is a component of a larger Event Processing Application . The event processor applies discrete, idempotent operations to an event object. Problem How do I gain insight from event data? For example, how can I quickly address a customer issue? Solution You can define any number of event processors inside an Event Processing Application to perform such tasks as mapping an event type to a domain object, triggering alerts, real-time report updates, and writing out results for consumption by other applications. Implementation StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); stream.filter((key, value)-> value.contains(\"special-code\")) .mapValues(value -> to domain object) .to(\"special-output-events\"); Considerations While it could be tempting to build a \"multi-purpose\" event processor, it's important that processor performs a discrete, idempotent action. By building processors this way, it's easier to reason about what each processor does and by extension what the application does. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"An event processor is a component of a larger Event Processing Application . The event processor applies discrete, idempotent operations to an event object.","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"How do I gain insight from event data? For example, how can I quickly address a customer issue?","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"You can define any number of event processors inside an Event Processing Application to perform such tasks as mapping an event type to a domain object, triggering alerts, real-time report updates, and writing out results for consumption by other applications.","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); stream.filter((key, value)-> value.contains(\"special-code\")) .mapValues(value -> to domain object) .to(\"special-output-events\");","title":"Implementation"},{"location":"event-processing/event-processor/#considerations","text":"While it could be tempting to build a \"multi-purpose\" event processor, it's important that processor performs a discrete, idempotent action. By building processors this way, it's easier to reason about what each processor does and by extension what the application does.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Event Streams may contain Events which can be separated logically by some attribute. The routing of Events to dedicated Streams may allow for simplified Event Processing and Event Sink solutions. Problem How can I isolate Events into a dedicated Event Stream based on some attribute of the Events? Solution Implementation With ksqlDB , continuously routing events to a different stream is as simple as using the CREATE STREAM syntax with the appropriate WHERE filter. CREATE STREAM actingevents_drama AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='drama'; CREATE STREAM actingevents_fantasy AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='fantasy'; If using Kafka Streams, the provided TopicNameExtractor interface can redirect events to topics. The TopicNameExtractor has one method, extract , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, ando ther contextual information about the event. You can use any of the given parameters to return the destination topic name, and Kafka Streams will complete the routing. GenreTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.genre) { case \"drama\": return \"drama-topic\"; case \"fantasy\": return \"fantasy-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new GenreTopicExtractor()); Considerations Event Routers should not modify the Event contents and instead only provide the proper Event routing. References This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of dynamically routing events at runtime","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"Event Streams may contain Events which can be separated logically by some attribute. The routing of Events to dedicated Streams may allow for simplified Event Processing and Event Sink solutions.","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How can I isolate Events into a dedicated Event Stream based on some attribute of the Events?","title":"Problem"},{"location":"event-processing/event-router/#solution","text":"","title":"Solution"},{"location":"event-processing/event-router/#implementation","text":"With ksqlDB , continuously routing events to a different stream is as simple as using the CREATE STREAM syntax with the appropriate WHERE filter. CREATE STREAM actingevents_drama AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='drama'; CREATE STREAM actingevents_fantasy AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='fantasy'; If using Kafka Streams, the provided TopicNameExtractor interface can redirect events to topics. The TopicNameExtractor has one method, extract , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, ando ther contextual information about the event. You can use any of the given parameters to return the destination topic name, and Kafka Streams will complete the routing. GenreTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.genre) { case \"drama\": return \"drama-topic\"; case \"fantasy\": return \"fantasy-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new GenreTopicExtractor());","title":"Implementation"},{"location":"event-processing/event-router/#considerations","text":"Event Routers should not modify the Event contents and instead only provide the proper Event routing.","title":"Considerations"},{"location":"event-processing/event-router/#references","text":"This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of dynamically routing events at runtime","title":"References"},{"location":"event-processing/event-splitter/","text":"Event Splitter Event Splitter takes a single event and splits it into multiple events. Problem How can I process an event if it contains multiple events, each of which may need to be processed in a different way? Solution Pattern First, split the original event into multiple child event. Most event processing technologies support this operation, for example the EXPLODE() table function in ksqlDB or the flatMap() operator in Kafka Streams. Then publish one event per child event. Implementation KStream<Long, String> stream = ...; KStream<String, Integer> transformed = stream.flatMap( // Here, we generate two output records for each input record. // We also change the key and value types. // Example: (345L, \"Hello\") -> (\"HELLO\", 1000), (\"hello\", 9000) (key, value) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(value.toUpperCase(), 1000)); result.add(KeyValue.pair(value.toLowerCase(), 9000)); return result; } ); Considerations Think about where the child events should be routed to, the same stream or a different stream. See Event Router on how to route events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: Your use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#event-splitter","text":"Event Splitter takes a single event and splits it into multiple events.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#problem","text":"How can I process an event if it contains multiple events, each of which may need to be processed in a different way?","title":"Problem"},{"location":"event-processing/event-splitter/#solution-pattern","text":"First, split the original event into multiple child event. Most event processing technologies support this operation, for example the EXPLODE() table function in ksqlDB or the flatMap() operator in Kafka Streams. Then publish one event per child event.","title":"Solution Pattern"},{"location":"event-processing/event-splitter/#implementation","text":"KStream<Long, String> stream = ...; KStream<String, Integer> transformed = stream.flatMap( // Here, we generate two output records for each input record. // We also change the key and value types. // Example: (345L, \"Hello\") -> (\"HELLO\", 1000), (\"hello\", 9000) (key, value) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(value.toUpperCase(), 1000)); result.add(KeyValue.pair(value.toLowerCase(), 9000)); return result; } );","title":"Implementation"},{"location":"event-processing/event-splitter/#considerations","text":"Think about where the child events should be routed to, the same stream or a different stream. See Event Router on how to route events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: Your use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier.","title":"Considerations"},{"location":"event-processing/event-streaming-api/","text":"Event Streaming API Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way. Problem How can my application connect to an Event Streaming Platform to send and receive Events ? Solution The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The highler level libraries allow the application to focus on business logic leaving the details of the platform communication to the API. References This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#event-streaming-api","text":"Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#problem","text":"How can my application connect to an Event Streaming Platform to send and receive Events ?","title":"Problem"},{"location":"event-processing/event-streaming-api/#solution","text":"The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The highler level libraries allow the application to focus on business logic leaving the details of the platform communication to the API.","title":"Solution"},{"location":"event-processing/event-streaming-api/#references","text":"This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"References"},{"location":"event-processing/event-translator/","text":"Event Translator Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them. Problem How can systems using different data formats communicate with each other using Events ? Solution An Event Translator converts a data format into a standard format familiar to down stream consumers. Implementation ksqlDB provides the ability to create Event Streams natively using an ANSI SQL inspired syntax. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream Considerations The Event Standardizer pattern ties together an Event Router and multiple Event Translators allowing disparate systems with multiple Event formats to communicate. References This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Translator"},{"location":"event-processing/event-translator/#event-translator","text":"Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them.","title":"Event Translator"},{"location":"event-processing/event-translator/#problem","text":"How can systems using different data formats communicate with each other using Events ?","title":"Problem"},{"location":"event-processing/event-translator/#solution","text":"An Event Translator converts a data format into a standard format familiar to down stream consumers.","title":"Solution"},{"location":"event-processing/event-translator/#implementation","text":"ksqlDB provides the ability to create Event Streams natively using an ANSI SQL inspired syntax. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream","title":"Implementation"},{"location":"event-processing/event-translator/#considerations","text":"The Event Standardizer pattern ties together an Event Router and multiple Event Translators allowing disparate systems with multiple Event formats to communicate.","title":"Considerations"},{"location":"event-processing/event-translator/#references","text":"This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations. Problem How can applications or external systems, like a databases, connect to an event streaming platform so that it can receive events? Solution Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system. Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the event streaming platform, performs any necessary transformations, and writes the events to the specified Event Sink. Event Sink Connector is a specific implementation of an Event Sink . Considerations There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References Confluent Connector Hub TODO: add sink connector KT once made public","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations.","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can applications or external systems, like a databases, connect to an event streaming platform so that it can receive events?","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution","text":"Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system.","title":"Solution"},{"location":"event-sink/event-sink-connector/#implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the event streaming platform, performs any necessary transformations, and writes the events to the specified Event Sink. Event Sink Connector is a specific implementation of an Event Sink .","title":"Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"Confluent Connector Hub TODO: add sink connector KT once made public","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink A component that reads or receives events Problem How can an application consume events? Solution The event sink is an application capable of consuming events from an event streaming platform. This application can be a generic consumer or a more complex Event Processing Application such as Kafka Streams or ksqlDB. Implementation Generic Kafka Consumer application: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } ksqlDB streaming query: CREATE STREAM CLICKS (IP_ADDRESS VARCHAR, URL VARCHAR, TIMESTAMP VARCHAR) WITH (KAFKA_TOPIC = 'CLICKS', VALUE_FORMAT = 'JSON', TIMESTAMP = 'TIMESTAMP', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX', PARTITIONS = 1); References See this Kafka Tutorial for a full Kafka consumer example application","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"A component that reads or receives events","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can an application consume events?","title":"Problem"},{"location":"event-sink/event-sink/#solution","text":"The event sink is an application capable of consuming events from an event streaming platform. This application can be a generic consumer or a more complex Event Processing Application such as Kafka Streams or ksqlDB.","title":"Solution"},{"location":"event-sink/event-sink/#implementation","text":"Generic Kafka Consumer application: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } ksqlDB streaming query: CREATE STREAM CLICKS (IP_ADDRESS VARCHAR, URL VARCHAR, TIMESTAMP VARCHAR) WITH (KAFKA_TOPIC = 'CLICKS', VALUE_FORMAT = 'JSON', TIMESTAMP = 'TIMESTAMP', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX', PARTITIONS = 1);","title":"Implementation"},{"location":"event-sink/event-sink/#references","text":"See this Kafka Tutorial for a full Kafka consumer example application","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Applications which write directly to a database may want to produce an associated event to the Event Streaming Platform for each write operation allowing downstream Event Processing Applications to be notified and consume the Event . Problem How do I update a value in a database and create an associated event? Solution Within a transaction, write the data to the database and produce an Event to the Event Streaming Platform . If the produce to the Event Streaming Platform fails, abort the transaction. This pattern provides an atomic dual commit. Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"Applications which write directly to a database may want to produce an associated event to the Event Streaming Platform for each write operation allowing downstream Event Processing Applications to be notified and consume the Event .","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event?","title":"Problem"},{"location":"event-source/database-write-aside/#solution","text":"Within a transaction, write the data to the database and produce an Event to the Event Streaming Platform . If the produce to the Event Streaming Platform fails, abort the transaction. This pattern provides an atomic dual commit.","title":"Solution"},{"location":"event-source/database-write-aside/#implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-through/","text":"Database Write Through For architectural or legacy purposes, data centric applications may write directly to a database. Event Processing Applications will need to reliably consume data from these systems using Events on Event Streams . Problem How do I update a value in a database and create an associated Event with at-least-once guarantees? Solution Applications write directly to a database table, which is the Event Source . Deploy a Change Data Capture (CDC) solution to continuously capture writes (inserts, updates, deletes) to that table and produce them as Events onto an Event Stream. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Implementation TODO: Implementation for this pattern? Kafka Connect config / create example? Considerations This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an Event Source are captured in an Event Streaming Platform. The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the CDC and Database technology utilizied. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the Event Source Connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References See Integrate External Systems to Kafka on Confluent documentation for information on source connectors. TODO: What about well known CDC providers, like Debezium?","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"For architectural or legacy purposes, data centric applications may write directly to a database. Event Processing Applications will need to reliably consume data from these systems using Events on Event Streams .","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do I update a value in a database and create an associated Event with at-least-once guarantees?","title":"Problem"},{"location":"event-source/database-write-through/#solution","text":"Applications write directly to a database table, which is the Event Source . Deploy a Change Data Capture (CDC) solution to continuously capture writes (inserts, updates, deletes) to that table and produce them as Events onto an Event Stream. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution"},{"location":"event-source/database-write-through/#implementation","text":"TODO: Implementation for this pattern? Kafka Connect config / create example?","title":"Implementation"},{"location":"event-source/database-write-through/#considerations","text":"This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an Event Source are captured in an Event Streaming Platform. The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the CDC and Database technology utilizied. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the Event Source Connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"See Integrate External Systems to Kafka on Confluent documentation for information on source connectors. TODO: What about well known CDC providers, like Debezium?","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Event Processing Applications may want to consume data from existing data systems which are not themselves Event Sources . Problem How can I connect traditional applications or systems, like a Database, to an event streaming platform, converting it's data at rest to data in motion with Events . Solution When connecting a system like a relational database to Kafka , the most common option is to use Kafka Connect . The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform. Implementation ksqlDB provides an ability to manage Kafka Connect with a SQL like syntax. CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' ); Considerations End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform. References This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"Event Processing Applications may want to consume data from existing data systems which are not themselves Event Sources .","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can I connect traditional applications or systems, like a Database, to an event streaming platform, converting it's data at rest to data in motion with Events .","title":"Problem"},{"location":"event-source/event-source-connector/#solution","text":"When connecting a system like a relational database to Kafka , the most common option is to use Kafka Connect . The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform.","title":"Solution"},{"location":"event-source/event-source-connector/#implementation","text":"ksqlDB provides an ability to manage Kafka Connect with a SQL like syntax. CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' );","title":"Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, and can include databases, Event Processing Applications , and web services. Problem How can I record and distribute events generated by my application? Solution Implementation ksqlDB provides a builtin INSERT syntax to directly write new Events directly to the Event Stream . INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, and can include databases, Event Processing Applications , and web services.","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/event-source/#solution","text":"","title":"Solution"},{"location":"event-source/event-source/#implementation","text":"ksqlDB provides a builtin INSERT syntax to directly write new Events directly to the Event Stream . INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"},{"location":"event-source/schema-validator/","text":"Schema Validator Events are written to Event Streams in various data formats. In order for an Event Processing Application to consume the Event, it must be able to decode Event data formats. Problem How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream? Solution A Schema Validator enforces the data format for Events prior to them being written to an Event Stream allowing Event Processing Applications to read Events based on a known schema. Implementation With Confluent, Schema Validation is enabled on the brokers by pairing them with a Schema Registry : confluent.schema.registry.url=http://schema-registry:8081 Once the Schema Registry is enabled, topics can be configured to enforce schemas with a basic configuration: kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 \\ --partitions 1 --topic movies \\ --config confluent.value.schema.validation=true Considerations Schema Validator is a data governence implementation of \"Schema on Write\", enforcing data conformance prior to Event publication. An alternative strategy is Schema On Read where data formats are not enforced on write and consuming Event Processing Applications are required to validate data formats as they read each event. References See the Schema Compatibility for information on how schemas can be verified The Schema Validation with Confluent Platform blog describes data governance on the Confluent Platform Subject Name Strategy documentation describes the method used by Confluent's Schema Registry to map event streams to schemas.","title":"Schema Validator"},{"location":"event-source/schema-validator/#schema-validator","text":"Events are written to Event Streams in various data formats. In order for an Event Processing Application to consume the Event, it must be able to decode Event data formats.","title":"Schema Validator"},{"location":"event-source/schema-validator/#problem","text":"How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream?","title":"Problem"},{"location":"event-source/schema-validator/#solution","text":"A Schema Validator enforces the data format for Events prior to them being written to an Event Stream allowing Event Processing Applications to read Events based on a known schema.","title":"Solution"},{"location":"event-source/schema-validator/#implementation","text":"With Confluent, Schema Validation is enabled on the brokers by pairing them with a Schema Registry : confluent.schema.registry.url=http://schema-registry:8081 Once the Schema Registry is enabled, topics can be configured to enforce schemas with a basic configuration: kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 \\ --partitions 1 --topic movies \\ --config confluent.value.schema.validation=true","title":"Implementation"},{"location":"event-source/schema-validator/#considerations","text":"Schema Validator is a data governence implementation of \"Schema on Write\", enforcing data conformance prior to Event publication. An alternative strategy is Schema On Read where data formats are not enforced on write and consuming Event Processing Applications are required to validate data formats as they read each event.","title":"Considerations"},{"location":"event-source/schema-validator/#references","text":"See the Schema Compatibility for information on how schemas can be verified The Schema Validation with Confluent Platform blog describes data governance on the Confluent Platform Subject Name Strategy documentation describes the method used by Confluent's Schema Registry to map event streams to schemas.","title":"References"},{"location":"event-storage/compacted-event-stream/","text":"Compacted Event Stream Event Streams often represent keyed snapshots of state. That is, the Events contain a primary key (identifier) and data that represents the latest snapshot of the business entity related to the Event. Event Processing Applications will need to process these Events to determine the current state of the business entity, however, processing the entire Event Stream history is not practical. Problem How can a (keyed) table be stored in an Event Stream forever using the minimum amount of space? Solution Remove events from the Event Stream that represent outdated information and have been superseded by new Events. Implementation Apache Kafka provides Log Compaction natively. A stream (topic in Kafka) is scanned periodically and old events are removed if they have been superseded (based on their key). It's worth nothing that this is an aysnchronous process, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted event stream with Kafka: kafka-topics --create --bootstrap-server <bootstrap-url> --replication-factor 3 --partitions 3 --topic topic-name --config cleanup.policy=compact Created topic topic-name. The kafka-topics command can also verify the current topics configuration: kafka-topics --bootstrap-server localhost:9092 --topic topic-name --describe Topic: topic-name PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: topic-name Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: topic-name Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: topic-name Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline: Considerations Compacted event streams allow for some optimizations: First they allow the Event Streaming Platform to slow down the growth of the Event Stream in a data-specific way (opposed to removing Events temporally). Second having smaller Event Streams allows for faster recovery or system migration strategies. References Compacted topics work a bit like simple Log Structured Merge Trees .","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#compacted-event-stream","text":"Event Streams often represent keyed snapshots of state. That is, the Events contain a primary key (identifier) and data that represents the latest snapshot of the business entity related to the Event. Event Processing Applications will need to process these Events to determine the current state of the business entity, however, processing the entire Event Stream history is not practical.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#problem","text":"How can a (keyed) table be stored in an Event Stream forever using the minimum amount of space?","title":"Problem"},{"location":"event-storage/compacted-event-stream/#solution","text":"Remove events from the Event Stream that represent outdated information and have been superseded by new Events.","title":"Solution"},{"location":"event-storage/compacted-event-stream/#implementation","text":"Apache Kafka provides Log Compaction natively. A stream (topic in Kafka) is scanned periodically and old events are removed if they have been superseded (based on their key). It's worth nothing that this is an aysnchronous process, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted event stream with Kafka: kafka-topics --create --bootstrap-server <bootstrap-url> --replication-factor 3 --partitions 3 --topic topic-name --config cleanup.policy=compact Created topic topic-name. The kafka-topics command can also verify the current topics configuration: kafka-topics --bootstrap-server localhost:9092 --topic topic-name --describe Topic: topic-name PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: topic-name Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: topic-name Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: topic-name Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline:","title":"Implementation"},{"location":"event-storage/compacted-event-stream/#considerations","text":"Compacted event streams allow for some optimizations: First they allow the Event Streaming Platform to slow down the growth of the Event Stream in a data-specific way (opposed to removing Events temporally). Second having smaller Event Streams allows for faster recovery or system migration strategies.","title":"Considerations"},{"location":"event-storage/compacted-event-stream/#references","text":"Compacted topics work a bit like simple Log Structured Merge Trees .","title":"References"},{"location":"event-storage/infinite-retention-event-stream/","text":"Infinite Retention Event Stream Storing all Events over time of an Event Stream enables systems to be agile and evolve by providing the capability of rebuilding global historical state. Problem How can an operator ensure that events in a stream are retained forever? Solution Implementation Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power. confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.<storage-provider>.bucket=<BUCKET_NAME> confluent.tier.<storage-provider>.region=<REGION> References An Event Sink Connector can be used to implement an infinite retention event stream by loading the event records into permanent external storage.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#infinite-retention-event-stream","text":"Storing all Events over time of an Event Stream enables systems to be agile and evolve by providing the capability of rebuilding global historical state.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#problem","text":"How can an operator ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infinite-retention-event-stream/#solution","text":"","title":"Solution"},{"location":"event-storage/infinite-retention-event-stream/#implementation","text":"Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power. confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.<storage-provider>.bucket=<BUCKET_NAME> confluent.tier.<storage-provider>.region=<REGION>","title":"Implementation"},{"location":"event-storage/infinite-retention-event-stream/#references","text":"An Event Sink Connector can be used to implement an infinite retention event stream by loading the event records into permanent external storage.","title":"References"},{"location":"event-stream/event-broker/","text":"Event Broker Loosely coupled components allow my applications to change with minimal impact on dependent systems. This loose coupling also allows my development teams to efficiently work asynchronously with respect to one another. Problem How can I decouple event sources from event destinations? Solution Use a central Event Broker that routes Events to related Event Streams . Producers of Events are isolated from the consumers of them. The components are coupled by a shared understanding of the name and data format of the shared Event Stream only. The Event Broker handles the complications of client applications connecting and disconnecting, and consumers are responsible for their individual progress of processing from the Event Stream. Implementation Apache Kafka is an open-source distributed Event Streaming Platform which implements this Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. In parallel, Event Processing Applications produce, consume, and process Events from the cluster using a loosely coupled but coordinated design. Considerations Counter to traditional message brokers, Event Brokers provide a data persistence layer that allows client applications to initiate and resume Event production and consumption independently. References This pattern is derived from Message Broker in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Broker"},{"location":"event-stream/event-broker/#event-broker","text":"Loosely coupled components allow my applications to change with minimal impact on dependent systems. This loose coupling also allows my development teams to efficiently work asynchronously with respect to one another.","title":"Event Broker"},{"location":"event-stream/event-broker/#problem","text":"How can I decouple event sources from event destinations?","title":"Problem"},{"location":"event-stream/event-broker/#solution","text":"Use a central Event Broker that routes Events to related Event Streams . Producers of Events are isolated from the consumers of them. The components are coupled by a shared understanding of the name and data format of the shared Event Stream only. The Event Broker handles the complications of client applications connecting and disconnecting, and consumers are responsible for their individual progress of processing from the Event Stream.","title":"Solution"},{"location":"event-stream/event-broker/#implementation","text":"Apache Kafka is an open-source distributed Event Streaming Platform which implements this Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. In parallel, Event Processing Applications produce, consume, and process Events from the cluster using a loosely coupled but coordinated design.","title":"Implementation"},{"location":"event-stream/event-broker/#considerations","text":"Counter to traditional message brokers, Event Brokers provide a data persistence layer that allows client applications to initiate and resume Event production and consumption independently.","title":"Considerations"},{"location":"event-stream/event-broker/#references","text":"This pattern is derived from Message Broker in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-stream/event-stream/","text":"Event Stream Events will typically be categorized by some attribute and will need to be grouped in order to be processed logically by Event Processors . Problem How does one Event Processing Application communicate events to other applications in an ordered, scalable, and reliable way? Solution Event Streams are a set of events recorded in the order in which they were written. Many Event Streaming applications can produce to or consume from an event stream. Event streams can either be consumed in their entirity or events can be distributed across a set of consuming event streaming application instances. (In Apache Kafka this is called a Consumer Group.) Implementation ksqlDB supports STREAM processing natively and creating a stream from an existing stream can be accomplished using basic declarative SQL like syntax. CREATE STREAM filtered AS SELECT col1, col2, col3 FROM source_stream; References This pattern is derived from Message Channel and Publish-Subscribe Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Stream"},{"location":"event-stream/event-stream/#event-stream","text":"Events will typically be categorized by some attribute and will need to be grouped in order to be processed logically by Event Processors .","title":"Event Stream"},{"location":"event-stream/event-stream/#problem","text":"How does one Event Processing Application communicate events to other applications in an ordered, scalable, and reliable way?","title":"Problem"},{"location":"event-stream/event-stream/#solution","text":"Event Streams are a set of events recorded in the order in which they were written. Many Event Streaming applications can produce to or consume from an event stream. Event streams can either be consumed in their entirity or events can be distributed across a set of consuming event streaming application instances. (In Apache Kafka this is called a Consumer Group.)","title":"Solution"},{"location":"event-stream/event-stream/#implementation","text":"ksqlDB supports STREAM processing natively and creating a stream from an existing stream can be accomplished using basic declarative SQL like syntax. CREATE STREAM filtered AS SELECT col1, col2, col3 FROM source_stream;","title":"Implementation"},{"location":"event-stream/event-stream/#references","text":"This pattern is derived from Message Channel and Publish-Subscribe Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-stream/event-streaming-platform/","text":"Event Streaming Platform Problem What is an architecture that enables separate applications to work together, but in a decoupled fashion such that applications can be easily added or removed without affecting the others? References This pattern is derived from Message Bus in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Streaming Platform"},{"location":"event-stream/event-streaming-platform/#event-streaming-platform","text":"","title":"Event Streaming Platform"},{"location":"event-stream/event-streaming-platform/#problem","text":"What is an architecture that enables separate applications to work together, but in a decoupled fashion such that applications can be easily added or removed without affecting the others?","title":"Problem"},{"location":"event-stream/event-streaming-platform/#references","text":"This pattern is derived from Message Bus in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-stream/schema-compatibility/","text":"Schema Compatibility Schemas are like contracts in that they set the terms that guarantee applications can process the data it receives. A natural behavior of applications and data is that they evolve over time, so it's important to have a policy about they are allowed to evolve and what compatibility rules are between old and new versions. Problem How do I ensure that a new schema is backward-compatible with a previous schema such that consumers do not need to make code changes to consume events? Solution Backward schema compatibility means that consumers referencing the new schema version of the schema can read data produced with the previous version of the schema. Two types of backward compatible changes: Removal of a mandatory field : a new consumer that was developed to process events without the field will be able to process events written with the previous schema that contain the field \u2013 the consumer will just ignore the field. Addition of an optional field : a new consumer that was developed to process events with this optional field will be able to process events written with the previous schema that do not contain the field \u2013 the consumer will not error because the field is optional. Implementation Using Avro as the serialization format, if the original schema is {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"long\"}, {\"name\": \"field2\", \"type\": \"string\"} ] } Examples of backward compatible changes: Removal of a mandatory field : notice field2 is removed {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"long\"} ] } Addition of an optional field : notice field3 is added with a default value of 0. {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"long\"}, {\"pame\": \"field2\", \"type\": \"string\"}, {\"pame\": \"field3\", \"type\": \"int\", \"default\": 0} ] } Considerations You could use an interface of a fully-managed Schema Registry service, with built-in compatibility checking, to centralize your schemas and check compatibility of new schema versions against previous versions (or a plugin ). curl -X POST --data @filename.avsc https://<schema-registry>/<subject>/versions After updated schemas that pass the schema compatibility check, be thoughtful about the order of upgrading applications. In some cases you should upgrade producers first, in other cases you should upgrade consumers first. See Compatibility Types for more details. References Schema compatibility : backward, forward, full","title":"Schema Compatibility"},{"location":"event-stream/schema-compatibility/#schema-compatibility","text":"Schemas are like contracts in that they set the terms that guarantee applications can process the data it receives. A natural behavior of applications and data is that they evolve over time, so it's important to have a policy about they are allowed to evolve and what compatibility rules are between old and new versions.","title":"Schema Compatibility"},{"location":"event-stream/schema-compatibility/#problem","text":"How do I ensure that a new schema is backward-compatible with a previous schema such that consumers do not need to make code changes to consume events?","title":"Problem"},{"location":"event-stream/schema-compatibility/#solution","text":"Backward schema compatibility means that consumers referencing the new schema version of the schema can read data produced with the previous version of the schema. Two types of backward compatible changes: Removal of a mandatory field : a new consumer that was developed to process events without the field will be able to process events written with the previous schema that contain the field \u2013 the consumer will just ignore the field. Addition of an optional field : a new consumer that was developed to process events with this optional field will be able to process events written with the previous schema that do not contain the field \u2013 the consumer will not error because the field is optional.","title":"Solution"},{"location":"event-stream/schema-compatibility/#implementation","text":"Using Avro as the serialization format, if the original schema is {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"long\"}, {\"name\": \"field2\", \"type\": \"string\"} ] } Examples of backward compatible changes: Removal of a mandatory field : notice field2 is removed {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"long\"} ] } Addition of an optional field : notice field3 is added with a default value of 0. {\"namespace\": \"io.confluent.examples.client\", \"type\": \"record\", \"name\": \"Event\", \"fields\": [ {\"name\": \"field1\", \"type\": \"long\"}, {\"pame\": \"field2\", \"type\": \"string\"}, {\"pame\": \"field3\", \"type\": \"int\", \"default\": 0} ] }","title":"Implementation"},{"location":"event-stream/schema-compatibility/#considerations","text":"You could use an interface of a fully-managed Schema Registry service, with built-in compatibility checking, to centralize your schemas and check compatibility of new schema versions against previous versions (or a plugin ). curl -X POST --data @filename.avsc https://<schema-registry>/<subject>/versions After updated schemas that pass the schema compatibility check, be thoughtful about the order of upgrading applications. In some cases you should upgrade producers first, in other cases you should upgrade consumers first. See Compatibility Types for more details.","title":"Considerations"},{"location":"event-stream/schema-compatibility/#references","text":"Schema compatibility : backward, forward, full","title":"References"},{"location":"event-stream/schema-evolution/","text":"Schema Evolution An important aspect of data management is schema evolution. Similar to how APIs evolve and need to be compatible for all applications that rely on old and new versions of the API, schemas also evolve and likewise need to be compatible for all applications that rely on old and new versions of a schema. Problem How do I restructure or add new information to an event in a way that ensures Schema Compatibility ? Solution One approach for evolving this schema is \"in-place\" (shown above), in which the single stream can have both new and previous schema versions in it, and the compatibility checks ensure that client applications can read schemas in both formats. Another approach is \"dual schema upgrades\" (shown above), in which the event producers write to two streams, one stream with the new schema version and one stream with the previous schema version, and client applications consume from the one that they are compatible with. Once all consumers are upgraded to the new schema, the old stream can be retired. Implementation TODO: what implementation do we want to show? Considerations TODO: considerations References Schema evolution and compatibility","title":"Schema Evolution"},{"location":"event-stream/schema-evolution/#schema-evolution","text":"An important aspect of data management is schema evolution. Similar to how APIs evolve and need to be compatible for all applications that rely on old and new versions of the API, schemas also evolve and likewise need to be compatible for all applications that rely on old and new versions of a schema.","title":"Schema Evolution"},{"location":"event-stream/schema-evolution/#problem","text":"How do I restructure or add new information to an event in a way that ensures Schema Compatibility ?","title":"Problem"},{"location":"event-stream/schema-evolution/#solution","text":"One approach for evolving this schema is \"in-place\" (shown above), in which the single stream can have both new and previous schema versions in it, and the compatibility checks ensure that client applications can read schemas in both formats. Another approach is \"dual schema upgrades\" (shown above), in which the event producers write to two streams, one stream with the new schema version and one stream with the previous schema version, and client applications consume from the one that they are compatible with. Once all consumers are upgraded to the new schema, the old stream can be retired.","title":"Solution"},{"location":"event-stream/schema-evolution/#implementation","text":"TODO: what implementation do we want to show?","title":"Implementation"},{"location":"event-stream/schema-evolution/#considerations","text":"TODO: considerations","title":"Considerations"},{"location":"event-stream/schema-evolution/#references","text":"Schema evolution and compatibility","title":"References"},{"location":"stream-processing/event-grouper/","text":"Event Grouper An event grouper is a specialized form of an Event Processor that groups events together by a common field such as a key in a key-value pair. Problem How can an application group individual but related events from the same stream/table so that they can subsequently be processed as a whole? Solution Using a group-by Event Processor in an Event Processing Application will group the related events together. Additionally you can use a windowing event processor to group events containing a timestamp within the size of the window. Implementation ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; Considerations If the field you want to group events with is embedded in the value, you can use a select-key event-processor to rey key the event-stream References Session Windows Hopping Windows Tumbling Windows","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"An event grouper is a specialized form of an Event Processor that groups events together by a common field such as a key in a key-value pair.","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can an application group individual but related events from the same stream/table so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution","text":"Using a group-by Event Processor in an Event Processing Application will group the related events together. Additionally you can use a windowing event processor to group events containing a timestamp within the size of the window.","title":"Solution"},{"location":"stream-processing/event-grouper/#implementation","text":"ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Implementation"},{"location":"stream-processing/event-grouper/#considerations","text":"If the field you want to group events with is embedded in the value, you can use a select-key event-processor to rey key the event-stream","title":"Considerations"},{"location":"stream-processing/event-grouper/#references","text":"Session Windows Hopping Windows Tumbling Windows","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Problem How do I process events from an Event Source irrespective of the timestamps when they were created originally at the source. Solution Depending on the use case, Event Processors may use the time when the event was ingested, receive on the event stream, or from a field provided by the Event itself. Implementation CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. Every record in ksqlDB has a system column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. References See the ksqlDB documentation on time semantics See this Kafka Tutorial for a full example on event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time.","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"How do I process events from an Event Source irrespective of the timestamps when they were created originally at the source.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution","text":"Depending on the use case, Event Processors may use the time when the event was ingested, receive on the event stream, or from a field provided by the Event itself.","title":"Solution"},{"location":"stream-processing/wallclock-time/#implementation","text":"CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. Every record in ksqlDB has a system column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"See the ksqlDB documentation on time semantics See this Kafka Tutorial for a full example on event-time semantics","title":"References"}]}