{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Text about event streaming patterns","title":"Welcome to Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Text about event streaming patterns","title":"Welcome to Event Streaming Patterns"},{"location":"db-write-through/","text":"Database Write Through (Change Data Capture) Problem How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform. Solution Pattern Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Example Implementation TODO: Example for CDC? Considerations The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"Database Write Through (Change Data Capture)"},{"location":"db-write-through/#database-write-through-change-data-capture","text":"","title":"Database Write Through (Change Data Capture)"},{"location":"db-write-through/#problem","text":"How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform.","title":"Problem"},{"location":"db-write-through/#solution-pattern","text":"Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution Pattern"},{"location":"db-write-through/#example-implementation","text":"TODO: Example for CDC?","title":"Example Implementation"},{"location":"db-write-through/#considerations","text":"The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"db-write-through/#references","text":"TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"References"},{"location":"event-filter/","text":"Event Filter Problem How can an application discard uninteresting events? Solution Pattern The Kafka Streams DSL provides a filter operator, where only records matching a given predicate continue to progress in the event stream. Example Implementation KStream<String, Event> eventStream = builder.stream(.....); KStream<String, Event> eventStream = ....; eventStream.filter((key, value) -> value.eventCode() == 200)..to(...); References Kafka Tutorial : How to filter a stream of events","title":"Event Filter"},{"location":"event-filter/#event-filter","text":"","title":"Event Filter"},{"location":"event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-filter/#solution-pattern","text":"The Kafka Streams DSL provides a filter operator, where only records matching a given predicate continue to progress in the event stream.","title":"Solution Pattern"},{"location":"event-filter/#example-implementation","text":"KStream<String, Event> eventStream = builder.stream(.....); KStream<String, Event> eventStream = ....; eventStream.filter((key, value) -> value.eventCode() == 200)..to(...);","title":"Example Implementation"},{"location":"event-filter/#references","text":"Kafka Tutorial : How to filter a stream of events","title":"References"},{"location":"event-router/","text":"Event Router Problem How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems? Solution Pattern Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes. Example Implementation CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor()); References Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"Event Router"},{"location":"event-router/#event-router","text":"","title":"Event Router"},{"location":"event-router/#problem","text":"How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems?","title":"Problem"},{"location":"event-router/#solution-pattern","text":"Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes.","title":"Solution Pattern"},{"location":"event-router/#example-implementation","text":"CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor());","title":"Example Implementation"},{"location":"event-router/#references","text":"Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"References"},{"location":"event-source/","text":"Event Source Problem How can I record and distribute events generated by my application? Solution Pattern Example Implementation INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/#event-source","text":"","title":"Event Source"},{"location":"event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/#solution-pattern","text":"","title":"Solution Pattern"},{"location":"event-source/#example-implementation","text":"INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Example Implementation"},{"location":"event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"}]}