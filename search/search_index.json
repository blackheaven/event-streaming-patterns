{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Text about event streaming patterns","title":"Welcome to Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Text about event streaming patterns","title":"Welcome to Event Streaming Patterns"},{"location":"event-filter/","text":"Event Filter Problem How can an application discard uninteresting events? Solution Pattern The Kafka Streams DSL provides a filter operator, where only records matching a given predicate continue to progress in the event stream. Example Implementation KStream<String, Event> eventStream = builder.stream(.....); KStream<String, Event> eventStream = ....; eventStream.filter((key, value) -> value.eventCode() == 200)..to(...); References Kafka Tutorial : How to filter a stream of events","title":"Event Filter"},{"location":"event-filter/#event-filter","text":"","title":"Event Filter"},{"location":"event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-filter/#solution-pattern","text":"The Kafka Streams DSL provides a filter operator, where only records matching a given predicate continue to progress in the event stream.","title":"Solution Pattern"},{"location":"event-filter/#example-implementation","text":"KStream<String, Event> eventStream = builder.stream(.....); KStream<String, Event> eventStream = ....; eventStream.filter((key, value) -> value.eventCode() == 200)..to(...);","title":"Example Implementation"},{"location":"event-filter/#references","text":"Kafka Tutorial : How to filter a stream of events","title":"References"},{"location":"event-router/","text":"Event Router Problem How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems? Solution Pattern Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes. Example Implementation CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor()); References Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"Event Router"},{"location":"event-router/#event-router","text":"","title":"Event Router"},{"location":"event-router/#problem","text":"How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems?","title":"Problem"},{"location":"event-router/#solution-pattern","text":"Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes.","title":"Solution Pattern"},{"location":"event-router/#example-implementation","text":"CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor());","title":"Example Implementation"},{"location":"event-router/#references","text":"Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"References"},{"location":"event-source/","text":"Event Source Problem How can I record and distribute events generated by my application? Solution Pattern Example Implementation INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/#event-source","text":"","title":"Event Source"},{"location":"event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/#solution-pattern","text":"","title":"Solution Pattern"},{"location":"event-source/#example-implementation","text":"INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Example Implementation"},{"location":"event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"}]}