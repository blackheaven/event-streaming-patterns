{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems.","title":"Welcome to Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems.","title":"Welcome to Event Streaming Patterns"},{"location":"event/event/","text":"Event Events represent facts and can help facilitate two decoupled applications exchanging data across an Event Streaming Platform . Problem How do I represent a fact about something that has happened? Solution An event is an immutable fact about something that has happened. It is produced and consumed from an Event Stream . Events contain data, timestamps, and may contain various metadata about the event (event headers, location in the stream, etc). Considerations Events are often created in reference to a schema (TODO:pattern-ref) commonly defined in Avro , Protobuf , or JSON schema . The emerging standard: Cloud Events provides a standardized envelope that wraps event data, making common event properties such as source, type, time, ID, and more, universally accessible regardless of the event payload. While events are necessarily a 'fact', in many cases they also imply movement: the communicating of facts about the world from one piece of software to another. References This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event"},{"location":"event/event/#event","text":"Events represent facts and can help facilitate two decoupled applications exchanging data across an Event Streaming Platform .","title":"Event"},{"location":"event/event/#problem","text":"How do I represent a fact about something that has happened?","title":"Problem"},{"location":"event/event/#solution","text":"An event is an immutable fact about something that has happened. It is produced and consumed from an Event Stream . Events contain data, timestamps, and may contain various metadata about the event (event headers, location in the stream, etc).","title":"Solution"},{"location":"event/event/#considerations","text":"Events are often created in reference to a schema (TODO:pattern-ref) commonly defined in Avro , Protobuf , or JSON schema . The emerging standard: Cloud Events provides a standardized envelope that wraps event data, making common event properties such as source, type, time, ID, and more, universally accessible regardless of the event payload. While events are necessarily a 'fact', in many cases they also imply movement: the communicating of facts about the world from one piece of software to another.","title":"Considerations"},{"location":"event/event/#references","text":"This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processors may need to operate over a subset of Events over a particular Event Stream . Problem How can an application discard uninteresting events? Solution Implementation The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processors may need to operate over a subset of Events over a particular Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa. Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Implementation Using standard Kafka producer, you can use an abstract Mapper concept to construct an Event instance ( PublicationEvent ) representing the Domain Model ( Publication ) prior to producing to Kafka. private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author/*key*/, mapper.map(newPub)); An application wishing to convert PublicationEvent instances to Domain Object updates, can do so with a Mapper that can do the reverse operation: private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = mapper.map(pubEvent); domainStore.update(newPub); TODO: Rick Feedback Request: How could we use ksqlDB or Kafka Streams her? The following KStreams example doesn't work to me because it's a stream processing application, but Mapper means to convert between domain objects and events. I'm not sure there is a logical way to \"push\" an event to a toplogy manually withouth producing to a topic. IMapper mapper = mapperFactory.buildMapper(Publication.class); builder.stream(inputTopic, Consumed.with(Serdes.String(), publicationSerde)) .map((name, publication) -> mapper.map(publication)) .to(outputTopic, Produced.with(Serdes.String(), publicationEventSerde)); Considerations TODO: Considerations? References This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf TODO: Reference to Event-Carried State Transfer? TODO: Reference to Document Message / Event vs Command Message / Event? TODO: Is Database Write Through / CDC a valid reference?","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution"},{"location":"event-processing/event-mapper/#implementation","text":"Using standard Kafka producer, you can use an abstract Mapper concept to construct an Event instance ( PublicationEvent ) representing the Domain Model ( Publication ) prior to producing to Kafka. private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author/*key*/, mapper.map(newPub)); An application wishing to convert PublicationEvent instances to Domain Object updates, can do so with a Mapper that can do the reverse operation: private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = mapper.map(pubEvent); domainStore.update(newPub); TODO: Rick Feedback Request: How could we use ksqlDB or Kafka Streams her? The following KStreams example doesn't work to me because it's a stream processing application, but Mapper means to convert between domain objects and events. I'm not sure there is a logical way to \"push\" an event to a toplogy manually withouth producing to a topic. IMapper mapper = mapperFactory.buildMapper(Publication.class); builder.stream(inputTopic, Consumed.with(Serdes.String(), publicationSerde)) .map((name, publication) -> mapper.map(publication)) .to(outputTopic, Produced.with(Serdes.String(), publicationEventSerde));","title":"Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"TODO: Considerations?","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf TODO: Reference to Event-Carried State Transfer? TODO: Reference to Document Message / Event vs Command Message / Event? TODO: Is Database Write Through / CDC a valid reference?","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application An Event Processing Application uses one or more Event Processor instances to handle streaming event driven data. Problem How can I build an application to work with streaming event data? Solution Using an Event Processing Application allows you to tie together indpendant event processors for working with streaming event recorods and are not aware of each other. Implementation StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); .... KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties); kafkaStreams.start() Considerations When building an Event Processing Application, it's important to generally confine the application to one problem domain. While it's true the application can have any number of event processors, they should be closely related. References","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"An Event Processing Application uses one or more Event Processor instances to handle streaming event driven data.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"How can I build an application to work with streaming event data?","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"Using an Event Processing Application allows you to tie together indpendant event processors for working with streaming event recorods and are not aware of each other.","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); .... KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties); kafkaStreams.start()","title":"Implementation"},{"location":"event-processing/event-processing-application/#considerations","text":"When building an Event Processing Application, it's important to generally confine the application to one problem domain. While it's true the application can have any number of event processors, they should be closely related.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor An event processor is a component of a larger Event Processing Application . The event processor applies discrete, idempotent operations to an event object. Problem How do I gain insight from event data? For example, how can I quickly address a customer issue? Solution You can define any number of event processors inside an Event Processing Application to perform such tasks as mapping an event type to a domain object, triggering alerts, real-time report updates, and writing out results for consumption by other applications. Implementation StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); stream.filter((key, value)-> value.contains(\"special-code\")) .mapValues(value -> to domain object) .to(\"special-output-events\"); Considerations While it could be tempting to build a \"multi-purpose\" event processor, it's important that processor performs a discrete, idempotent action. By building processors this way, it's easier to reason about what each processor does and by extension what the application does. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"An event processor is a component of a larger Event Processing Application . The event processor applies discrete, idempotent operations to an event object.","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"How do I gain insight from event data? For example, how can I quickly address a customer issue?","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"You can define any number of event processors inside an Event Processing Application to perform such tasks as mapping an event type to a domain object, triggering alerts, real-time report updates, and writing out results for consumption by other applications.","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"StreamsBuilder builder = new StreamsBuilder(); KStream<String, String> stream = builder.stream(\"input-events\"); stream.filter((key, value)-> value.contains(\"special-code\")) .mapValues(value -> to domain object) .to(\"special-output-events\");","title":"Implementation"},{"location":"event-processing/event-processor/#considerations","text":"While it could be tempting to build a \"multi-purpose\" event processor, it's important that processor performs a discrete, idempotent action. By building processors this way, it's easier to reason about what each processor does and by extension what the application does.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Event Streams may contain Events which can be separated logically by some attribute. The routing of Events to dedicated Streams may allow for simplified Event Processing and Event Sink solutions. Problem How can I isolate Events into a dedicated Event Stream based on some attribute of the Events? Solution Implementation With ksqlDB , continuously routing events to a different stream is as simple as using the CREATE STREAM syntax with the appropriate WHERE filter. CREATE STREAM actingevents_drama AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='drama'; CREATE STREAM actingevents_fantasy AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='fantasy'; If using Kafka Streams, the provided TopicNameExtractor interface can redirect events to topics. The TopicNameExtractor has one method, extract , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, ando ther contextual information about the event. You can use any of the given parameters to return the destination topic name, and Kafka Streams will complete the routing. GenreTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.genre) { case \"drama\": return \"drama-topic\"; case \"fantasy\": return \"fantasy-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new GenreTopicExtractor()); Considerations Event Routers should not modify the Event contents and instead only provide the proper Event routing. References This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of dynamically routing events at runtime","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"Event Streams may contain Events which can be separated logically by some attribute. The routing of Events to dedicated Streams may allow for simplified Event Processing and Event Sink solutions.","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How can I isolate Events into a dedicated Event Stream based on some attribute of the Events?","title":"Problem"},{"location":"event-processing/event-router/#solution","text":"","title":"Solution"},{"location":"event-processing/event-router/#implementation","text":"With ksqlDB , continuously routing events to a different stream is as simple as using the CREATE STREAM syntax with the appropriate WHERE filter. CREATE STREAM actingevents_drama AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='drama'; CREATE STREAM actingevents_fantasy AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='fantasy'; If using Kafka Streams, the provided TopicNameExtractor interface can redirect events to topics. The TopicNameExtractor has one method, extract , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, ando ther contextual information about the event. You can use any of the given parameters to return the destination topic name, and Kafka Streams will complete the routing. GenreTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.genre) { case \"drama\": return \"drama-topic\"; case \"fantasy\": return \"fantasy-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new GenreTopicExtractor());","title":"Implementation"},{"location":"event-processing/event-router/#considerations","text":"Event Routers should not modify the Event contents and instead only provide the proper Event routing.","title":"Considerations"},{"location":"event-processing/event-router/#references","text":"This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of dynamically routing events at runtime","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations. Problem How can applications or external systems, like a databases, connect to an event streaming platform so that it can receive events? Solution Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system. Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the event streaming platform, performs any necessary transformations, and writes the events to the specified Event Sink. Event Sink Connector is a specific implementation of an Event Sink . Considerations There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References Confluent Connector Hub TODO: add sink connector KT once made public","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations.","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can applications or external systems, like a databases, connect to an event streaming platform so that it can receive events?","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution","text":"Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system.","title":"Solution"},{"location":"event-sink/event-sink-connector/#implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the event streaming platform, performs any necessary transformations, and writes the events to the specified Event Sink. Event Sink Connector is a specific implementation of an Event Sink .","title":"Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"Confluent Connector Hub TODO: add sink connector KT once made public","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink A component that reads or receives events Problem How can an application consume events? Solution The event sink is an application capable of consuming events from an event streaming platform. This application can be a generic consumer or a more complex Event Processing Application such as Kafka Streams or ksqlDB. Implementation Generic Kafka Consumer application: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } ksqlDB streaming query: CREATE STREAM CLICKS (IP_ADDRESS VARCHAR, URL VARCHAR, TIMESTAMP VARCHAR) WITH (KAFKA_TOPIC = 'CLICKS', VALUE_FORMAT = 'JSON', TIMESTAMP = 'TIMESTAMP', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX', PARTITIONS = 1); References See this Kafka Tutorial for a full Kafka consumer example application","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"A component that reads or receives events","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can an application consume events?","title":"Problem"},{"location":"event-sink/event-sink/#solution","text":"The event sink is an application capable of consuming events from an event streaming platform. This application can be a generic consumer or a more complex Event Processing Application such as Kafka Streams or ksqlDB.","title":"Solution"},{"location":"event-sink/event-sink/#implementation","text":"Generic Kafka Consumer application: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } ksqlDB streaming query: CREATE STREAM CLICKS (IP_ADDRESS VARCHAR, URL VARCHAR, TIMESTAMP VARCHAR) WITH (KAFKA_TOPIC = 'CLICKS', VALUE_FORMAT = 'JSON', TIMESTAMP = 'TIMESTAMP', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX', PARTITIONS = 1);","title":"Implementation"},{"location":"event-sink/event-sink/#references","text":"See this Kafka Tutorial for a full Kafka consumer example application","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Applications which write directly to a database may want to produce an associated event to the Event Streaming Platform for each write operation allowing downstream Event Processing Applications to be notified and consume the Event . Problem How do I update a value in a database and create an associated event? Solution Within a transaction, write the data to the database and produce an Event to the Event Streaming Platform . If the produce to the Event Streaming Platform fails, abort the transaction. This pattern provides an atomic dual commit. Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"Applications which write directly to a database may want to produce an associated event to the Event Streaming Platform for each write operation allowing downstream Event Processing Applications to be notified and consume the Event .","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event?","title":"Problem"},{"location":"event-source/database-write-aside/#solution","text":"Within a transaction, write the data to the database and produce an Event to the Event Streaming Platform . If the produce to the Event Streaming Platform fails, abort the transaction. This pattern provides an atomic dual commit.","title":"Solution"},{"location":"event-source/database-write-aside/#implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-through/","text":"Database Write Through For architectural or legacy purposes, data centric applications may write directly to a database. Event Processing Applications will need to reliably consume data from these systems using Events on Event Streams . Problem How do I update a value in a database and create an associated Event with at-least-once guarantees? Solution Applications write directly to a database table, which is the Event Source . Deploy a Change Data Capture (CDC) solution to continuously capture writes (inserts, updates, deletes) to that table and produce them as Events onto an Event Stream. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Implementation TODO: Implementation for this pattern? Kafka Connect config / create example? Considerations This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an Event Source are captured in an Event Streaming Platform. The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the CDC and Database technology utilizied. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the Event Source Connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References See Integrate External Systems to Kafka on Confluent documentation for information on source connectors. TODO: What about well known CDC providers, like Debezium?","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"For architectural or legacy purposes, data centric applications may write directly to a database. Event Processing Applications will need to reliably consume data from these systems using Events on Event Streams .","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do I update a value in a database and create an associated Event with at-least-once guarantees?","title":"Problem"},{"location":"event-source/database-write-through/#solution","text":"Applications write directly to a database table, which is the Event Source . Deploy a Change Data Capture (CDC) solution to continuously capture writes (inserts, updates, deletes) to that table and produce them as Events onto an Event Stream. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution"},{"location":"event-source/database-write-through/#implementation","text":"TODO: Implementation for this pattern? Kafka Connect config / create example?","title":"Implementation"},{"location":"event-source/database-write-through/#considerations","text":"This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an Event Source are captured in an Event Streaming Platform. The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the CDC and Database technology utilizied. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the Event Source Connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"See Integrate External Systems to Kafka on Confluent documentation for information on source connectors. TODO: What about well known CDC providers, like Debezium?","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Event Processing Applications may want to consume data from existing data systems which are not themselves Event Sources . Problem How can I connect traditional applications or systems, like a Database, to an event streaming platform, converting it's data at rest to data in motion with Events . Solution When connecting a system like a relational database to Kafka , the most common option is to use Kafka Connect . The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform. Implementation ksqlDB provides an ability to manage Kafka Connect with a SQL like syntax. CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' ); Considerations End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform. References This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"Event Processing Applications may want to consume data from existing data systems which are not themselves Event Sources .","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can I connect traditional applications or systems, like a Database, to an event streaming platform, converting it's data at rest to data in motion with Events .","title":"Problem"},{"location":"event-source/event-source-connector/#solution","text":"When connecting a system like a relational database to Kafka , the most common option is to use Kafka Connect . The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform.","title":"Solution"},{"location":"event-source/event-source-connector/#implementation","text":"ksqlDB provides an ability to manage Kafka Connect with a SQL like syntax. CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' );","title":"Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, and can include databases, Event Processing Applications , and web services. Problem How can I record and distribute events generated by my application? Solution Implementation ksqlDB provides a builtin INSERT syntax to directly write new Events directly to the Event Stream . INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, and can include databases, Event Processing Applications , and web services.","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/event-source/#solution","text":"","title":"Solution"},{"location":"event-source/event-source/#implementation","text":"ksqlDB provides a builtin INSERT syntax to directly write new Events directly to the Event Stream . INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"},{"location":"event-storage/infinite-retention-event-stream/","text":"Infinite Retention Event Stream Storing all Events over time of an Event Stream enables systems to be agile and evolve by providing the capability of rebuilding global historical state. Problem How can an operator ensure that events in a stream are retained forever? Solution Implementation Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power. confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.<storage-provider>.bucket=<BUCKET_NAME> confluent.tier.<storage-provider>.region=<REGION> References An Event Sink Connector can be used to implement an infinite retention event stream by loading the event records into permanent external storage.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#infinite-retention-event-stream","text":"Storing all Events over time of an Event Stream enables systems to be agile and evolve by providing the capability of rebuilding global historical state.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#problem","text":"How can an operator ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infinite-retention-event-stream/#solution","text":"","title":"Solution"},{"location":"event-storage/infinite-retention-event-stream/#implementation","text":"Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power. confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.<storage-provider>.bucket=<BUCKET_NAME> confluent.tier.<storage-provider>.region=<REGION>","title":"Implementation"},{"location":"event-storage/infinite-retention-event-stream/#references","text":"An Event Sink Connector can be used to implement an infinite retention event stream by loading the event records into permanent external storage.","title":"References"},{"location":"event-stream/event-stream/","text":"Event Stream Events will typically be categorized by some attribute and will need to be grouped in order to be processed logically by Event Processors . Problem How can Events be organized such that Event Processing Applications can consume only the relevant Events on a particular Event Streaming Platform . Solution Typically Event Streams are given a name along with other configurations like retention length. Events are published to the Event Streams creating a logical categorization of Events for Event Processing Applications to consume from. Implementation ksqlDB supports STREAM processing natively and creating a stream from an existing stream can be accomplished using basic declarative SQL like syntax. CREATE STREAM filtered AS SELECT col1, col2, col3 FROM source_stream; References This pattern is derived from Message Channel and Publish-Subscribe Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Stream"},{"location":"event-stream/event-stream/#event-stream","text":"Events will typically be categorized by some attribute and will need to be grouped in order to be processed logically by Event Processors .","title":"Event Stream"},{"location":"event-stream/event-stream/#problem","text":"How can Events be organized such that Event Processing Applications can consume only the relevant Events on a particular Event Streaming Platform .","title":"Problem"},{"location":"event-stream/event-stream/#solution","text":"Typically Event Streams are given a name along with other configurations like retention length. Events are published to the Event Streams creating a logical categorization of Events for Event Processing Applications to consume from.","title":"Solution"},{"location":"event-stream/event-stream/#implementation","text":"ksqlDB supports STREAM processing natively and creating a stream from an existing stream can be accomplished using basic declarative SQL like syntax. CREATE STREAM filtered AS SELECT col1, col2, col3 FROM source_stream;","title":"Implementation"},{"location":"event-stream/event-stream/#references","text":"This pattern is derived from Message Channel and Publish-Subscribe Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-stream/event-streaming-platform/","text":"Event Streaming Platform Problem What is an architecture that enables separate applications to work together, but in a decoupled fashion such that applications can be easily added or removed without affecting the others? References This pattern is derived from Message Bus in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Streaming Platform"},{"location":"event-stream/event-streaming-platform/#event-streaming-platform","text":"","title":"Event Streaming Platform"},{"location":"event-stream/event-streaming-platform/#problem","text":"What is an architecture that enables separate applications to work together, but in a decoupled fashion such that applications can be easily added or removed without affecting the others?","title":"Problem"},{"location":"event-stream/event-streaming-platform/#references","text":"This pattern is derived from Message Bus in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"stream-processing/event-grouper/","text":"Event Grouper An event grouper is a specialized form of an Event Processor that groups events together by a common field such as a key in a key-value pair. Problem How can an application group individual but related events from the same stream/table so that they can subsequently be processed as a whole? Solution Using a group-by Event Processor in an Event Processing Application will group the related events together. Additionally you can use a windowing event processor to group events containing a timestamp within the size of the window. Implementation ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; Considerations If the field you want to group events with is embedded in the value, you can use a select-key event-processor to rey key the event-stream References Session Windows Hopping Windows Tumbling Windows","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"An event grouper is a specialized form of an Event Processor that groups events together by a common field such as a key in a key-value pair.","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can an application group individual but related events from the same stream/table so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution","text":"Using a group-by Event Processor in an Event Processing Application will group the related events together. Additionally you can use a windowing event processor to group events containing a timestamp within the size of the window.","title":"Solution"},{"location":"stream-processing/event-grouper/#implementation","text":"ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Implementation"},{"location":"stream-processing/event-grouper/#considerations","text":"If the field you want to group events with is embedded in the value, you can use a select-key event-processor to rey key the event-stream","title":"Considerations"},{"location":"stream-processing/event-grouper/#references","text":"Session Windows Hopping Windows Tumbling Windows","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Problem How do I process events from an Event Source irrespective of the timestamps when they were created originally at the source. Solution Depending on the use case, Event Processors may use the time when the event was ingested, receive on the event stream, or from a field provided by the Event itself. Implementation CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. Every record in ksqlDB has a system column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. References See the ksqlDB documentation on time semantics See this Kafka Tutorial for a full example on event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time.","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"How do I process events from an Event Source irrespective of the timestamps when they were created originally at the source.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution","text":"Depending on the use case, Event Processors may use the time when the event was ingested, receive on the event stream, or from a field provided by the Event itself.","title":"Solution"},{"location":"stream-processing/wallclock-time/#implementation","text":"CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. Every record in ksqlDB has a system column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"See the ksqlDB documentation on time semantics See this Kafka Tutorial for a full example on event-time semantics","title":"References"}]}