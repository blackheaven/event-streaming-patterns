{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Text about event streaming patterns","title":"Welcome to Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Text about event streaming patterns","title":"Welcome to Event Streaming Patterns"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Problem How can an event processing application handle event processing failures without terminating on error? Solution Pattern When an event stream processing system cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Example Implementation public class DeadLetterStreamer { \u2026 public Future<RecordMetadata> report(ConsumerRecord<byte[], byte[]> badRecord, Exception ex) { return this.producer.send(new ProducerRecord<>(/*badRecord and Exception data*/); } } ConsumerRecords<String, byte[]> records = consumer.poll(1000); for (ConsumerRecord<> record : records) { try { // process record } catch (Exception ex) { dlStreamer.report(record, ex); } } Considerations TODO: Considerations? References ksqlDB Processing Log","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle event processing failures without terminating on error?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution-pattern","text":"When an event stream processing system cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon.","title":"Solution Pattern"},{"location":"event-processing/dead-letter-stream/#example-implementation","text":"public class DeadLetterStreamer { \u2026 public Future<RecordMetadata> report(ConsumerRecord<byte[], byte[]> badRecord, Exception ex) { return this.producer.send(new ProducerRecord<>(/*badRecord and Exception data*/); } } ConsumerRecords<String, byte[]> records = consumer.poll(1000); for (ConsumerRecord<> record : records) { try { // process record } catch (Exception ex) { dlStreamer.report(record, ex); } }","title":"Example Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"TODO: Considerations?","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"ksqlDB Processing Log","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Problem How can an application discard uninteresting events? Solution Pattern The Kafka Streams DSL provides a filter operator, where only records matching a given predicate continue to progress in the event stream. Example Implementation KStream<String, Event> eventStream = builder.stream(.....); KStream<String, Event> eventStream = ....; eventStream.filter((key, value) -> value.eventCode() == 200)..to(...); References Kafka Tutorial : How to filter a stream of events","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution-pattern","text":"The Kafka Streams DSL provides a filter operator, where only records matching a given predicate continue to progress in the event stream.","title":"Solution Pattern"},{"location":"event-processing/event-filter/#example-implementation","text":"KStream<String, Event> eventStream = builder.stream(.....); KStream<String, Event> eventStream = ....; eventStream.filter((key, value) -> value.eventCode() == 200)..to(...);","title":"Example Implementation"},{"location":"event-processing/event-filter/#references","text":"Kafka Tutorial : How to filter a stream of events","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Pattern Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Example Implementation TODO: Example? Considerations The mapper may optionally validate the schema of the converted objects, see \"Schema Validation\". References TODO: Reference for Schema Validation Domain Model","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution-pattern","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution Pattern"},{"location":"event-processing/event-mapper/#example-implementation","text":"TODO: Example?","title":"Example Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"The mapper may optionally validate the schema of the converted objects, see \"Schema Validation\".","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"TODO: Reference for Schema Validation Domain Model","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Problem How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems? Solution Pattern Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes. Example Implementation CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor()); References Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems?","title":"Problem"},{"location":"event-processing/event-router/#solution-pattern","text":"Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes.","title":"Solution Pattern"},{"location":"event-processing/event-router/#example-implementation","text":"CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor());","title":"Example Implementation"},{"location":"event-processing/event-router/#references","text":"Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink Problem How can a stream of events be written into a sink, any destination that wants to receive those events? This can be a generic consumer application that reads a stream of events (or for a more specific example, see Event Sink Connector ). Solution Pattern An application can read a stream of events from an event streaming platform. Example Implementation consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } References Kafka Tutorial : Kafka consumer application","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can a stream of events be written into a sink, any destination that wants to receive those events? This can be a generic consumer application that reads a stream of events (or for a more specific example, see Event Sink Connector ).","title":"Problem"},{"location":"event-sink/event-sink/#solution-pattern","text":"An application can read a stream of events from an event streaming platform.","title":"Solution Pattern"},{"location":"event-sink/event-sink/#example-implementation","text":"consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); }","title":"Example Implementation"},{"location":"event-sink/event-sink/#references","text":"Kafka Tutorial : Kafka consumer application","title":"References"},{"location":"event-source/db-write-through/","text":"Database Write Through (Change Data Capture) Problem How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform. Solution Pattern Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Example Implementation TODO: Example for CDC? Considerations The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"Database Write Through (Change Data Capture)"},{"location":"event-source/db-write-through/#database-write-through-change-data-capture","text":"","title":"Database Write Through (Change Data Capture)"},{"location":"event-source/db-write-through/#problem","text":"How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform.","title":"Problem"},{"location":"event-source/db-write-through/#solution-pattern","text":"Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution Pattern"},{"location":"event-source/db-write-through/#example-implementation","text":"TODO: Example for CDC?","title":"Example Implementation"},{"location":"event-source/db-write-through/#considerations","text":"The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/db-write-through/#references","text":"TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Problem How can I connect an application or system like a DB to an event streaming platform so that it can send events? Solution Pattern When connecting a system like a relational database to Kafka, the most common option is to use Kafka connectors. The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform. Example Implementation CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' ); Considerations End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform. References Kafka Tutorials : Kafka Connect Source example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can I connect an application or system like a DB to an event streaming platform so that it can send events?","title":"Problem"},{"location":"event-source/event-source-connector/#solution-pattern","text":"When connecting a system like a relational database to Kafka, the most common option is to use Kafka connectors. The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform.","title":"Solution Pattern"},{"location":"event-source/event-source-connector/#example-implementation","text":"CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' );","title":"Example Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"Kafka Tutorials : Kafka Connect Source example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Problem How can I record and distribute events generated by my application? Solution Pattern Example Implementation INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/event-source/#solution-pattern","text":"","title":"Solution Pattern"},{"location":"event-source/event-source/#example-implementation","text":"INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Example Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Problem Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested. Solution Pattern Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. Example Implementation By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); References Kafka Tutorial : Event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution-pattern","text":"Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Solution Pattern"},{"location":"stream-processing/wallclock-time/#example-implementation","text":"By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime');","title":"Example Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"Kafka Tutorial : Event-time semantics","title":"References"}]}