{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Welcome to Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Welcome to Event Streaming Patterns"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processors may need to operate over a subset of Events over a particular Event Stream . Problem How can an application discard uninteresting events? Solution Implementation The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processors may need to operate over a subset of Events over a particular Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa. Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Implementation Using standard Kafka producer, you can use an abstract Mapper concept to construct an Event instance ( PublicationEvent ) representing the Domain Model ( Publication ) prior to producing to Kafka. private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author/*key*/, mapper.map(newPub)); An application wishing to convert PublicationEvent instances to Domain Object updates, can do so with a Mapper that can do the reverse operation: private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = mapper.map(pubEvent); domainStore.update(newPub); TODO: Rick Feedback Request: How could we use ksqlDB or Kafka Streams her? The following KStreams example doesn't work to me because it's a stream processing application, but Mapper means to convert between domain objects and events. I'm not sure there is a logical way to \"push\" an event to a toplogy manually withouth producing to a topic. IMapper mapper = mapperFactory.buildMapper(Publication.class); builder.stream(inputTopic, Consumed.with(Serdes.String(), publicationSerde)) .map((name, publication) -> mapper.map(publication)) .to(outputTopic, Produced.with(Serdes.String(), publicationEventSerde)); Considerations TODO: Considerations? References This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf TODO: Reference to Event-Carried State Transfer? TODO: Reference to Document Message / Event vs Command Message / Event? TODO: Is Database Write Through / CDC a valid reference?","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution"},{"location":"event-processing/event-mapper/#implementation","text":"Using standard Kafka producer, you can use an abstract Mapper concept to construct an Event instance ( PublicationEvent ) representing the Domain Model ( Publication ) prior to producing to Kafka. private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author/*key*/, mapper.map(newPub)); An application wishing to convert PublicationEvent instances to Domain Object updates, can do so with a Mapper that can do the reverse operation: private final IMapper mapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = mapper.map(pubEvent); domainStore.update(newPub); TODO: Rick Feedback Request: How could we use ksqlDB or Kafka Streams her? The following KStreams example doesn't work to me because it's a stream processing application, but Mapper means to convert between domain objects and events. I'm not sure there is a logical way to \"push\" an event to a toplogy manually withouth producing to a topic. IMapper mapper = mapperFactory.buildMapper(Publication.class); builder.stream(inputTopic, Consumed.with(Serdes.String(), publicationSerde)) .map((name, publication) -> mapper.map(publication)) .to(outputTopic, Produced.with(Serdes.String(), publicationEventSerde));","title":"Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"TODO: Considerations?","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf TODO: Reference to Event-Carried State Transfer? TODO: Reference to Document Message / Event vs Command Message / Event? TODO: Is Database Write Through / CDC a valid reference?","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied. Problem TODO: Technology agnostic English explanation of the problem Solution TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first). Implementation TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code) Considerations TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"TODO: Technology agnostic English explanation of the problem","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first).","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code)","title":"Implementation"},{"location":"event-processing/event-processing-application/#considerations","text":"TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied. Problem TODO: Technology agnostic English explanation of the problem Solution TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first). Implementation TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code) Considerations TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied.","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"TODO: Technology agnostic English explanation of the problem","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first).","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code)","title":"Implementation"},{"location":"event-processing/event-processor/#considerations","text":"TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Event Streams may contain Events which can be separated logically by some attribute. The routing of Events to dedicated Streams may allow for simplified Event Processing and Event Sink solutions. Problem How can I isolate Events into a dedicated Event Stream based on some attribute of the Events? Solution Implementation With ksqlDB , continuously routing events to a different stream is as simple as using the CREATE STREAM syntax with the appropriate WHERE filter. CREATE STREAM actingevents_drama AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='drama'; CREATE STREAM actingevents_fantasy AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='fantasy'; CREATE STREAM actingevents_other AS SELECT NAME, TITLE, GENRE FROM ACTINGEVENTS WHERE GENRE != 'drama' AND GENRE != 'fantasy'; If using Kafka Streams, the provided TopicNameExtractor interface can redirect events to topics. The TopicNameExtractor has one method, extract , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, ando ther contextual information about the event. You can use any of the given parameters to return the destination topic name, and Kafka Streams will complete the routing. GenreTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.genre) { case \"drama\": return \"drama-topic\"; case \"fantasy\": return \"fantasy-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new GenreTopicExtractor()); Considerations Event Routers should not modify the Event contents and instead only provide the proper Event routing. References This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of dynamically routing events at runtime","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"Event Streams may contain Events which can be separated logically by some attribute. The routing of Events to dedicated Streams may allow for simplified Event Processing and Event Sink solutions.","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How can I isolate Events into a dedicated Event Stream based on some attribute of the Events?","title":"Problem"},{"location":"event-processing/event-router/#solution","text":"","title":"Solution"},{"location":"event-processing/event-router/#implementation","text":"With ksqlDB , continuously routing events to a different stream is as simple as using the CREATE STREAM syntax with the appropriate WHERE filter. CREATE STREAM actingevents_drama AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='drama'; CREATE STREAM actingevents_fantasy AS SELECT NAME, TITLE FROM ACTINGEVENTS WHERE GENRE='fantasy'; CREATE STREAM actingevents_other AS SELECT NAME, TITLE, GENRE FROM ACTINGEVENTS WHERE GENRE != 'drama' AND GENRE != 'fantasy'; If using Kafka Streams, the provided TopicNameExtractor interface can redirect events to topics. The TopicNameExtractor has one method, extract , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, ando ther contextual information about the event. You can use any of the given parameters to return the destination topic name, and Kafka Streams will complete the routing. GenreTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.genre) { case \"drama\": return \"drama-topic\"; case \"fantasy\": return \"fantasy-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new GenreTopicExtractor());","title":"Implementation"},{"location":"event-processing/event-router/#considerations","text":"Event Routers should not modify the Event contents and instead only provide the proper Event routing.","title":"Considerations"},{"location":"event-processing/event-router/#references","text":"This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of dynamically routing events at runtime","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Problem How can I connect an application or system like a DB to an event streaming platform so that it can receive events? Unlike Event Sink , this is a specific example that doesn't use generic consumers or a REST API. Solution Pattern When connecting a system like a relational database to Kafka, the most common option is to use Kafka connectors. The connector reads events from the event streaming platform, performs any necessary transformations, and writes the events to the specified sink. Example Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); Considerations Existing Kafka connectors: there are many such event sink connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References Confluent Connector Hub TODO: add sink connector KT once made public","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can I connect an application or system like a DB to an event streaming platform so that it can receive events? Unlike Event Sink , this is a specific example that doesn't use generic consumers or a REST API.","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution-pattern","text":"When connecting a system like a relational database to Kafka, the most common option is to use Kafka connectors. The connector reads events from the event streaming platform, performs any necessary transformations, and writes the events to the specified sink.","title":"Solution Pattern"},{"location":"event-sink/event-sink-connector/#example-implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' );","title":"Example Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"Existing Kafka connectors: there are many such event sink connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"Confluent Connector Hub TODO: add sink connector KT once made public","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink Problem How can a stream of events be written into a sink, any destination that wants to receive those events? This can be a generic consumer application that reads a stream of events (or for a more specific example, see Event Sink Connector ). Solution Pattern An application can read a stream of events from an event streaming platform. Example Implementation consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } References Kafka Tutorial : Kafka consumer application","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can a stream of events be written into a sink, any destination that wants to receive those events? This can be a generic consumer application that reads a stream of events (or for a more specific example, see Event Sink Connector ).","title":"Problem"},{"location":"event-sink/event-sink/#solution-pattern","text":"An application can read a stream of events from an event streaming platform.","title":"Solution Pattern"},{"location":"event-sink/event-sink/#example-implementation","text":"consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); }","title":"Example Implementation"},{"location":"event-sink/event-sink/#references","text":"Kafka Tutorial : Kafka consumer application","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Problem How do I update a value in a database and create an associated event with the least amount of effort? Solution Pattern Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails). Example Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled. References TODO: Add references?","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event with the least amount of effort?","title":"Problem"},{"location":"event-source/database-write-aside/#solution-pattern","text":"Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails).","title":"Solution Pattern"},{"location":"event-source/database-write-aside/#example-implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Example Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-aside/#references","text":"TODO: Add references?","title":"References"},{"location":"event-source/database-write-through/","text":"Database Write Through Problem How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform. Solution Pattern Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Example Implementation TODO: Example for CDC? Considerations The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform.","title":"Problem"},{"location":"event-source/database-write-through/#solution-pattern","text":"Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution Pattern"},{"location":"event-source/database-write-through/#example-implementation","text":"TODO: Example for CDC?","title":"Example Implementation"},{"location":"event-source/database-write-through/#considerations","text":"The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Problem How can I connect an application or system like a DB to an event streaming platform so that it can send events? Solution Pattern When connecting a system like a relational database to Kafka, the most common option is to use Kafka connectors. The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform. Example Implementation CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' ); Considerations End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform. References Kafka Tutorials : Kafka Connect Source example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can I connect an application or system like a DB to an event streaming platform so that it can send events?","title":"Problem"},{"location":"event-source/event-source-connector/#solution-pattern","text":"When connecting a system like a relational database to Kafka, the most common option is to use Kafka connectors. The connector reads data from the event source, then generate events from that data, and finally sends these events to the event streaming platform.","title":"Solution Pattern"},{"location":"event-source/event-source-connector/#example-implementation","text":"CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_01 WITH ( 'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector', 'connection.url'= 'jdbc:postgresql://postgres:5432/postgres', 'connection.user'= 'postgres', 'connection.password'= 'postgres', 'mode'= 'incrementing', 'incrementing.column.name'= 'city_id', 'topic.prefix'= 'postgres_' );","title":"Example Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (e.g., at-least-once or exactly-once delivery; cf. \"Guaranteed Delivery\") depend primarily on three factors: (1) the capabilities of the event source, such as a relational or NoSQL database; (2) the capabilities of the destination event streaming platform, such as Apache Kafka; and (3) the capabilities of the event source connector. Existing Kafka connectors: there are many such event source connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event source, event source connector, and the destination event streaming platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"Kafka Tutorials : Kafka Connect Source example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Problem How can I record and distribute events generated by my application? Solution Pattern Example Implementation INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/event-source/#solution-pattern","text":"","title":"Solution Pattern"},{"location":"event-source/event-source/#example-implementation","text":"INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Example Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"},{"location":"event-storage/infiite-retention-event-stream/","text":"Infinite Retention Event Stream Problem How can an operator ensure that events in a stream are retained forever? Solution Pattern Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power. Example Implementation confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.s3.bucket=<BUCKET_NAME> confluent.tier.s3.region=<REGION> # Confluent also supports using Google Cloud Storage (GCS)","title":"Infinite Retention Event Stream"},{"location":"event-storage/infiite-retention-event-stream/#infinite-retention-event-stream","text":"","title":"Infinite Retention Event Stream"},{"location":"event-storage/infiite-retention-event-stream/#problem","text":"How can an operator ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infiite-retention-event-stream/#solution-pattern","text":"Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power.","title":"Solution Pattern"},{"location":"event-storage/infiite-retention-event-stream/#example-implementation","text":"confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.s3.bucket=<BUCKET_NAME> confluent.tier.s3.region=<REGION> # Confluent also supports using Google Cloud Storage (GCS)","title":"Example Implementation"},{"location":"stream-processing/event-grouper/","text":"Event Grouper Problem How can I group individual but related events from the same stream/table so that they can subsequently be processed as a whole? Solution Pattern ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. Example Implementation SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; References Session Windows Hopping Windows Tumbling Windows","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can I group individual but related events from the same stream/table so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution-pattern","text":"ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window.","title":"Solution Pattern"},{"location":"stream-processing/event-grouper/#example-implementation","text":"SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Example Implementation"},{"location":"stream-processing/event-grouper/#references","text":"Session Windows Hopping Windows Tumbling Windows","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Problem Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested. Solution Pattern Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. Example Implementation By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); References Kafka Tutorial : Event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution-pattern","text":"Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Solution Pattern"},{"location":"stream-processing/wallclock-time/#example-implementation","text":"By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime');","title":"Example Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"Kafka Tutorial : Event-time semantics","title":"References"}]}