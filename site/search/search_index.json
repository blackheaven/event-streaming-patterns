{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Welcome to Event Streaming Patterns"},{"location":"compositional-patterns/command-query-responsibility-segregation/","text":"Command Query Responsibility Segregation (CQRS) Databases conflate the writing of data and the reading of data in the same place: the database. In some situations, it is preferable to separate reads from writes. There are several reasons to do this but the most prevalent is that the application can now save data in the exact form in which it arrives, accurately reflecting what happened in the real world, while reading it in a different form, one that is optimized for reading. For example, a user adding and removing items from their cart would all be recorded as a stream of immutable events: t-shirt added, t-shirt removed, etc. These are then summarized into a separate view that used to serve reads, for example summarizing the various user events to represent the accurate contents of the cart. Problem How can we store and hold data in the exact form in which it arrived but read from a summarized and curated view? Solution Represent changes that happen in the real world as Events - an order is shipped, a ride is accepted, etc. - and retain these events as the system of record. Subsequently, aggregate those Events into a view that summarizes the events to represent the current state, allowing applications to query the current values. So for example, the current balance of your account would be the total of all the payment events that added money to or removed it from your account. The system of record is the stream of payment events. The view you read from would be the account balance. Implementation The streaming database ksqlDB can implement a CQRS using an Event Stream and Table . Event Streams are built into to the streaming database design. Creating a new stream is straightforward: CREATE STREAM purchases (customer VARCHAR, item VARCHAR, qty INT WITH (kafka_topic='purchases-topic', value_format='json', partitions=1); Events can be directly using familiar SQL syntax. INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'sweaters', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', -1); We can create a Materialized View of the data as a Table : CREATE TABLE customer_purchases WITH (KEY_FORMAT='JSON') AS SELECT customer, item, SUM(qty) as total_qty from purchases GROUP BY customer, item emit changes; And continuously query for changes to the state of the customer_purchases table: SELECT * FROM customer_purchases EMIT CHANGES; Considerations CQRS adds complexity over a traditional simple CRUD database implementation. High performance applications may benefit from a CQRS design. Isolating the load of writing and reading of data may allow you to scale those aspects independently and properly. Microserivces applications often use CQRS to scale-out with many views provided for different services. The same pattern is applicable to geographically dispersed applications such as a flight booking system which are read heavy across many locations. A write to a CQRS system is eventually consistent. Writes cannot be read immediately as there is a delay between the write of the command Event and the query-model being updated. This can cause complexity for some client applications, particularly online services. References See Martin Fowler's detailed explanation of CQRS for more information.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"compositional-patterns/command-query-responsibility-segregation/#command-query-responsibility-segregation-cqrs","text":"Databases conflate the writing of data and the reading of data in the same place: the database. In some situations, it is preferable to separate reads from writes. There are several reasons to do this but the most prevalent is that the application can now save data in the exact form in which it arrives, accurately reflecting what happened in the real world, while reading it in a different form, one that is optimized for reading. For example, a user adding and removing items from their cart would all be recorded as a stream of immutable events: t-shirt added, t-shirt removed, etc. These are then summarized into a separate view that used to serve reads, for example summarizing the various user events to represent the accurate contents of the cart.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"compositional-patterns/command-query-responsibility-segregation/#problem","text":"How can we store and hold data in the exact form in which it arrived but read from a summarized and curated view?","title":"Problem"},{"location":"compositional-patterns/command-query-responsibility-segregation/#solution","text":"Represent changes that happen in the real world as Events - an order is shipped, a ride is accepted, etc. - and retain these events as the system of record. Subsequently, aggregate those Events into a view that summarizes the events to represent the current state, allowing applications to query the current values. So for example, the current balance of your account would be the total of all the payment events that added money to or removed it from your account. The system of record is the stream of payment events. The view you read from would be the account balance.","title":"Solution"},{"location":"compositional-patterns/command-query-responsibility-segregation/#implementation","text":"The streaming database ksqlDB can implement a CQRS using an Event Stream and Table . Event Streams are built into to the streaming database design. Creating a new stream is straightforward: CREATE STREAM purchases (customer VARCHAR, item VARCHAR, qty INT WITH (kafka_topic='purchases-topic', value_format='json', partitions=1); Events can be directly using familiar SQL syntax. INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'sweaters', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', -1); We can create a Materialized View of the data as a Table : CREATE TABLE customer_purchases WITH (KEY_FORMAT='JSON') AS SELECT customer, item, SUM(qty) as total_qty from purchases GROUP BY customer, item emit changes; And continuously query for changes to the state of the customer_purchases table: SELECT * FROM customer_purchases EMIT CHANGES;","title":"Implementation"},{"location":"compositional-patterns/command-query-responsibility-segregation/#considerations","text":"CQRS adds complexity over a traditional simple CRUD database implementation. High performance applications may benefit from a CQRS design. Isolating the load of writing and reading of data may allow you to scale those aspects independently and properly. Microserivces applications often use CQRS to scale-out with many views provided for different services. The same pattern is applicable to geographically dispersed applications such as a flight booking system which are read heavy across many locations. A write to a CQRS system is eventually consistent. Writes cannot be read immediately as there is a delay between the write of the command Event and the query-model being updated. This can cause complexity for some client applications, particularly online services.","title":"Considerations"},{"location":"compositional-patterns/command-query-responsibility-segregation/#references","text":"See Martin Fowler's detailed explanation of CQRS for more information.","title":"References"},{"location":"compositional-patterns/event-collaboration/","text":"Event Collaboration Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states. Problem How can we build a distributed workflow in a way that allows components to evolve independently? Solution Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete. Considerations In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asyncronously by way of a global identifier which traverses the workflow within the events. References Event Collaboration by Martin Fowler","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#event-collaboration","text":"Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states.","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#problem","text":"How can we build a distributed workflow in a way that allows components to evolve independently?","title":"Problem"},{"location":"compositional-patterns/event-collaboration/#solution","text":"Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete.","title":"Solution"},{"location":"compositional-patterns/event-collaboration/#considerations","text":"In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asyncronously by way of a global identifier which traverses the workflow within the events.","title":"Considerations"},{"location":"compositional-patterns/event-collaboration/#references","text":"Event Collaboration by Martin Fowler","title":"References"},{"location":"compositional-patterns/geo-replication/","text":"Geo Replication Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply. Problem How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others? Solution Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data. Implementation Practically, replication is not enabled completely on all data streams, as there are always exceptions, organizational limitations, technical constraints, or other reasons why you wouldn't want to copy absolutely everything. Instead, you can do this on a per topic basis, where you can map a source topic to a destination topic. With Apache Kafka, you can do this in one of several ways. Option 1: Cluster Linking Cluster Linking enables easy data sharing between event streaming platforms, mirroring topics across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ, Active MQ, etc., provide similar functionality but without the same levels of parallelism. Option 2: Connect-based Replication Operators can set up such inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers. Considerations Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain. References This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#geo-replication","text":"Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply.","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#problem","text":"How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others?","title":"Problem"},{"location":"compositional-patterns/geo-replication/#solution","text":"Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data.","title":"Solution"},{"location":"compositional-patterns/geo-replication/#implementation","text":"Practically, replication is not enabled completely on all data streams, as there are always exceptions, organizational limitations, technical constraints, or other reasons why you wouldn't want to copy absolutely everything. Instead, you can do this on a per topic basis, where you can map a source topic to a destination topic. With Apache Kafka, you can do this in one of several ways.","title":"Implementation"},{"location":"compositional-patterns/geo-replication/#option-1-cluster-linking","text":"Cluster Linking enables easy data sharing between event streaming platforms, mirroring topics across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ, Active MQ, etc., provide similar functionality but without the same levels of parallelism.","title":"Option 1: Cluster Linking"},{"location":"compositional-patterns/geo-replication/#option-2-connect-based-replication","text":"Operators can set up such inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers.","title":"Option 2: Connect-based Replication"},{"location":"compositional-patterns/geo-replication/#considerations","text":"Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain.","title":"Considerations"},{"location":"compositional-patterns/geo-replication/#references","text":"This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"compositional-patterns/pipeline/","text":"Pipeline A single Event Stream or Table can be used by multiple Event Processing Applications , and its Events may go through multiple processing stages along the way (e.g., filters, transformations, joins, aggregations) to implement more complex use cases. Problem How can a single processing objective for a set of Event Streams and/or Tables be achieved through a series of independent processing stages? Solution We can compose Event Streams and Tables in an Event Streaming Platform via an Event Processing Application to a create a pipeline\u2014also called a topology\u2014of Event Processors , which continuously process the events flowing through them. Here, the output of one processor is the input for one or more downstream processors. Pipelines, notably when created for use cases such as Streaming ETL , may include Event Source Connectors and Event Sink Connectors , which continuously import and export data as streams from/to external services and systems, respectively. Connectors are particularly useful for turning data at rest in such systems into data in motion. Taking a step back, we can see that pipelines in an Event Streaming Platform help companies build a \"central nervous system\" for data in motion. Implementation As an example we can use the streaming database ksqlDB to run a stream of events through a series of processing stages, thus creating a Pipeline that continuously processes data in motion. CREATE STREAM orders ( customer_id INTEGER, items ARRAY<STRUCT<name VARCHAR, price DOUBLE>> ) WITH ( KAFKA_TOPIC = 'orders', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); We'll also create a (continuously updated) customers table that will contain the latest profile information about each customer, such as their current home address. CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, name VARCHAR, ADDRESS VARCHAR ) WITH ( KAFKA_TOPIC = 'customers', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); Next, we create a new stream by joining the orders stream with our customer table: CREATE STREAM orders_enriched WITH (KAFKA_TOPIC='orders_enriched', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT o.customer_id AS cust_id, o.items, c.name, c.address FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id EMIT CHANGES; Next, we create a stream, where we add the order total to each order by aggregating the price of the individual items in the order: CREATE STREAM orders_with_totals WITH (KAFKA_TOPIC='orders_totaled', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT cust_id, items, name, address, REDUCE(TRANSFORM(items, i=> i->price ), 0e0, (i,x) => (i + x)) AS total FROM orders_enriched EMIT CHANGES; Considerations The same event stream or table can participate in multiple pipelines. Because streams and tables are stored durably, applications have a lot of flexibility how and when they process the respective data, and they can do so independently from each other. The various processing stages in a pipeline create their own derived streams/tables (such as the orders_enriched stream in the ksqlDB example above), which in turn can be used as input for other pipelines and applications. This allows for further and more complex composition and re-use of events throughout an organization. References This pattern was influenced by Pipes and Filters in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. However, it is much more powerful and flexible because it is using Event Streams as the pipes.","title":"Pipeline"},{"location":"compositional-patterns/pipeline/#pipeline","text":"A single Event Stream or Table can be used by multiple Event Processing Applications , and its Events may go through multiple processing stages along the way (e.g., filters, transformations, joins, aggregations) to implement more complex use cases.","title":"Pipeline"},{"location":"compositional-patterns/pipeline/#problem","text":"How can a single processing objective for a set of Event Streams and/or Tables be achieved through a series of independent processing stages?","title":"Problem"},{"location":"compositional-patterns/pipeline/#solution","text":"We can compose Event Streams and Tables in an Event Streaming Platform via an Event Processing Application to a create a pipeline\u2014also called a topology\u2014of Event Processors , which continuously process the events flowing through them. Here, the output of one processor is the input for one or more downstream processors. Pipelines, notably when created for use cases such as Streaming ETL , may include Event Source Connectors and Event Sink Connectors , which continuously import and export data as streams from/to external services and systems, respectively. Connectors are particularly useful for turning data at rest in such systems into data in motion. Taking a step back, we can see that pipelines in an Event Streaming Platform help companies build a \"central nervous system\" for data in motion.","title":"Solution"},{"location":"compositional-patterns/pipeline/#implementation","text":"As an example we can use the streaming database ksqlDB to run a stream of events through a series of processing stages, thus creating a Pipeline that continuously processes data in motion. CREATE STREAM orders ( customer_id INTEGER, items ARRAY<STRUCT<name VARCHAR, price DOUBLE>> ) WITH ( KAFKA_TOPIC = 'orders', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); We'll also create a (continuously updated) customers table that will contain the latest profile information about each customer, such as their current home address. CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, name VARCHAR, ADDRESS VARCHAR ) WITH ( KAFKA_TOPIC = 'customers', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); Next, we create a new stream by joining the orders stream with our customer table: CREATE STREAM orders_enriched WITH (KAFKA_TOPIC='orders_enriched', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT o.customer_id AS cust_id, o.items, c.name, c.address FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id EMIT CHANGES; Next, we create a stream, where we add the order total to each order by aggregating the price of the individual items in the order: CREATE STREAM orders_with_totals WITH (KAFKA_TOPIC='orders_totaled', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT cust_id, items, name, address, REDUCE(TRANSFORM(items, i=> i->price ), 0e0, (i,x) => (i + x)) AS total FROM orders_enriched EMIT CHANGES;","title":"Implementation"},{"location":"compositional-patterns/pipeline/#considerations","text":"The same event stream or table can participate in multiple pipelines. Because streams and tables are stored durably, applications have a lot of flexibility how and when they process the respective data, and they can do so independently from each other. The various processing stages in a pipeline create their own derived streams/tables (such as the orders_enriched stream in the ksqlDB example above), which in turn can be used as input for other pipelines and applications. This allows for further and more complex composition and re-use of events throughout an organization.","title":"Considerations"},{"location":"compositional-patterns/pipeline/#references","text":"This pattern was influenced by Pipes and Filters in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. However, it is much more powerful and flexible because it is using Event Streams as the pipes.","title":"References"},{"location":"event/correlation-identifier/","text":"Correlation Identifier Event Processing Applications may want to implement an Event Collaboration pattern where Events are used to transport requests and responses. The applications which collaborate via the Events , will need a method for correlating Event response data for specific requests. Problem How does an application, that has requested information and received a response, know for which request a particular response is for? Solution An Event Processor generates an Event which acts as the request. A globally unique identifier is added to the request Event prior to sending. This allows the responding Event Processor to include the identifier in the response Event, allowing the requesting processor to correlate the request and response. Implementation In Kafka, we can add a globally unique identifier to the Kafka record headers when producing the request event. The following code example uses Kafka's Java producer client. ProducerRecord<String, String> requestEvent = new ProducerRecord<>(\"request-event-key\", \"request-event-value\"); requestEvent.headers().add(\"requestID\", UUID.randomUUID().toString()); requestEvent.send(producerRecord); In the responding event processor, we first extract the correlation identifier from the request event (here, requestID ) and then add the identifier to the response event. ProducerRecord<String, String> responseEvent = new ProducerRecord<>(\"response-event-key\", \"response-event-value\"); requestEvent.headers().add(\"requestID\", requestEvent.headers().lastHeader(\"requestID\").value()); requestEvent.send(producerRecord); References This pattern is derived from Correlation Identifier in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf For a case study on coordinating microservices towards higher level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB Correlation Identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution. The idea of tagging requests and their related responses exists in many other protocols. For example, an email client connecting over IMAP will send commands prefixed with a unique ID (typically, a001 , a002 , etc.) and the server will respond asynchronously, tagging its responses with the matching ID.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#correlation-identifier","text":"Event Processing Applications may want to implement an Event Collaboration pattern where Events are used to transport requests and responses. The applications which collaborate via the Events , will need a method for correlating Event response data for specific requests.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#problem","text":"How does an application, that has requested information and received a response, know for which request a particular response is for?","title":"Problem"},{"location":"event/correlation-identifier/#solution","text":"An Event Processor generates an Event which acts as the request. A globally unique identifier is added to the request Event prior to sending. This allows the responding Event Processor to include the identifier in the response Event, allowing the requesting processor to correlate the request and response.","title":"Solution"},{"location":"event/correlation-identifier/#implementation","text":"In Kafka, we can add a globally unique identifier to the Kafka record headers when producing the request event. The following code example uses Kafka's Java producer client. ProducerRecord<String, String> requestEvent = new ProducerRecord<>(\"request-event-key\", \"request-event-value\"); requestEvent.headers().add(\"requestID\", UUID.randomUUID().toString()); requestEvent.send(producerRecord); In the responding event processor, we first extract the correlation identifier from the request event (here, requestID ) and then add the identifier to the response event. ProducerRecord<String, String> responseEvent = new ProducerRecord<>(\"response-event-key\", \"response-event-value\"); requestEvent.headers().add(\"requestID\", requestEvent.headers().lastHeader(\"requestID\").value()); requestEvent.send(producerRecord);","title":"Implementation"},{"location":"event/correlation-identifier/#references","text":"This pattern is derived from Correlation Identifier in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf For a case study on coordinating microservices towards higher level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB Correlation Identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution. The idea of tagging requests and their related responses exists in many other protocols. For example, an email client connecting over IMAP will send commands prefixed with a unique ID (typically, a001 , a002 , etc.) and the server will respond asynchronously, tagging its responses with the matching ID.","title":"References"},{"location":"event/event-envelope/","text":"Event Envelope Event Streaming Platforms allow many different types of applications to work together. Event Envelopes provide a standard set of well-known fields across all Event s sent through the Event Streaming Applications . The envelope is independent of the underlying event format and often references attributes such as the encryption type, schema, key, serialization format. Envelopes are analogous to protocol headers in networking (TCP-IP etc.) Problem How to convey information to all participants in an Event Streaming Platform independently of the event payload, e.g. how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event? Solution Use an Event Envelope to wrap the event data using a standard format agreed by all participants of the Event Streaming Platform or more broadly. Cloud Events \u2013which standardize access to ID, Schema, Key, and other common event attributes\u2013are an industry-standard example of the Event Envelope pattern. Example Implementation Using the basic Java consumers and producers, a helper function could be used to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform . static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; } References This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf How to Choose Between Strict and Dynamic Schemas See Cloud Events for a specification on describing event header information in a common way.","title":"Event Envelope"},{"location":"event/event-envelope/#event-envelope","text":"Event Streaming Platforms allow many different types of applications to work together. Event Envelopes provide a standard set of well-known fields across all Event s sent through the Event Streaming Applications . The envelope is independent of the underlying event format and often references attributes such as the encryption type, schema, key, serialization format. Envelopes are analogous to protocol headers in networking (TCP-IP etc.)","title":"Event Envelope"},{"location":"event/event-envelope/#problem","text":"How to convey information to all participants in an Event Streaming Platform independently of the event payload, e.g. how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event?","title":"Problem"},{"location":"event/event-envelope/#solution","text":"Use an Event Envelope to wrap the event data using a standard format agreed by all participants of the Event Streaming Platform or more broadly. Cloud Events \u2013which standardize access to ID, Schema, Key, and other common event attributes\u2013are an industry-standard example of the Event Envelope pattern.","title":"Solution"},{"location":"event/event-envelope/#example-implementation","text":"Using the basic Java consumers and producers, a helper function could be used to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform . static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; }","title":"Example Implementation"},{"location":"event/event-envelope/#references","text":"This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf How to Choose Between Strict and Dynamic Schemas See Cloud Events for a specification on describing event header information in a common way.","title":"References"},{"location":"event/event/","text":"Event Events represent facts and can help facilitate decoupled applications, services, and systems exchanging data across an Event Streaming Platform . Problem How do I represent a fact about something that has happened? Solution An event represents an immutable fact about something that happened. Examples of Events might be: orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains at least one or more data fields that describe the fact, as well as a timestamp that denotes when this Event was created by its Event Source . The Event may also contain various metadata about itself, such as its source of origin (e.g., the application or cloud services that created the event) and storage-level information (e.g., its position in the event stream). Considerations To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event Schemas are commonly defined in Avro , Protobuf , or JSON schema . For cloud-based architectures, you may want to evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (think: instructions, actions) that an Event Processor reading the events should carry out. See the Command Event for details. References This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event"},{"location":"event/event/#event","text":"Events represent facts and can help facilitate decoupled applications, services, and systems exchanging data across an Event Streaming Platform .","title":"Event"},{"location":"event/event/#problem","text":"How do I represent a fact about something that has happened?","title":"Problem"},{"location":"event/event/#solution","text":"An event represents an immutable fact about something that happened. Examples of Events might be: orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains at least one or more data fields that describe the fact, as well as a timestamp that denotes when this Event was created by its Event Source . The Event may also contain various metadata about itself, such as its source of origin (e.g., the application or cloud services that created the event) and storage-level information (e.g., its position in the event stream).","title":"Solution"},{"location":"event/event/#considerations","text":"To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event Schemas are commonly defined in Avro , Protobuf , or JSON schema . For cloud-based architectures, you may want to evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (think: instructions, actions) that an Event Processor reading the events should carry out. See the Command Event for details.","title":"Considerations"},{"location":"event/event/#references","text":"This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/claim-check/","text":"Claim Check Sometimes compression can reduce message size but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc. Problem How can we handle use cases where the Event payload is too large or too expensive to move through the Event Streaming Platform ? Solution Instead of storing the entire event in the event streaming platform, store the event payload in a persistent external store that can be shared between producers and consumers. The producer can write the reference address into the event streaming platform, and downstream clients use the address to retrieve the event from the external store and then process it as needed. Implementation The event stored in Kafka contains only a reference to the object in the external store. This can be a full URI string, an abstract data type (e.g., Java object) with separate fields for bucket name and filename, or whatever fields are required to identify the object. Optionally, the event may contain additional data fields to better describe the object (e.g., metadata such as who created the object). The following example uses Kafka's Java producer client. Here, we keep things simple as the event's value stores no information other than the reference (URI) to its respective object in external storage. // Write object to external storage storageClient.putObject(bucketName, objectName, object); // Write URI to Kafka URI eventValue = new URI(bucketName, objectName); producer.send(new ProducerRecord<String, URI>(topic, eventKey, eventValue)); Considerations The Event Source is responsible for ensuring that the data is properly stored in the external store, such that the reference passed within the Event is valid. Since the producer should be doing this atomically, take into consideration the same issues as mentioned in Database Write Aside . Also, if a Compacted Event Stream is used for storing the \"reference\" events (e.g., topic compaction in the case of Kafka), then the compaction will remove just the event with the reference. However, it will not remove the referenced (large) object itself from the external store, so that object needs a different expiry mechanism. References This pattern is similar in idea to Claim Check in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf An alternative approach to handling large messages is Event Chunking","title":"Claim Check"},{"location":"event-processing/claim-check/#claim-check","text":"Sometimes compression can reduce message size but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc.","title":"Claim Check"},{"location":"event-processing/claim-check/#problem","text":"How can we handle use cases where the Event payload is too large or too expensive to move through the Event Streaming Platform ?","title":"Problem"},{"location":"event-processing/claim-check/#solution","text":"Instead of storing the entire event in the event streaming platform, store the event payload in a persistent external store that can be shared between producers and consumers. The producer can write the reference address into the event streaming platform, and downstream clients use the address to retrieve the event from the external store and then process it as needed.","title":"Solution"},{"location":"event-processing/claim-check/#implementation","text":"The event stored in Kafka contains only a reference to the object in the external store. This can be a full URI string, an abstract data type (e.g., Java object) with separate fields for bucket name and filename, or whatever fields are required to identify the object. Optionally, the event may contain additional data fields to better describe the object (e.g., metadata such as who created the object). The following example uses Kafka's Java producer client. Here, we keep things simple as the event's value stores no information other than the reference (URI) to its respective object in external storage. // Write object to external storage storageClient.putObject(bucketName, objectName, object); // Write URI to Kafka URI eventValue = new URI(bucketName, objectName); producer.send(new ProducerRecord<String, URI>(topic, eventKey, eventValue));","title":"Implementation"},{"location":"event-processing/claim-check/#considerations","text":"The Event Source is responsible for ensuring that the data is properly stored in the external store, such that the reference passed within the Event is valid. Since the producer should be doing this atomically, take into consideration the same issues as mentioned in Database Write Aside . Also, if a Compacted Event Stream is used for storing the \"reference\" events (e.g., topic compaction in the case of Kafka), then the compaction will remove just the event with the reference. However, it will not remove the referenced (large) object itself from the external store, so that object needs a different expiry mechanism.","title":"Considerations"},{"location":"event-processing/claim-check/#references","text":"This pattern is similar in idea to Claim Check in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf An alternative approach to handling large messages is Event Chunking","title":"References"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-chunking/","text":"Event Chunking Sometimes compression can reduce message size, but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc. Problem How do I handle use cases where the event payload is too large to move through the event streaming platform as a single event? Solution Instead of storing the entire event as a single event in the event streaming platform, break it into chunks (an approach called \"chunking\") so that the large event is sent across as multiple smaller events. The producer can do the chunking when writing events into the event streaming platform. Downstream clients consume the chunks and, when all the smaller chunks have been received, recombine (\"unchunk\") them to restore the original event. Implementation Use metadata to track each chunk so that they can be associated to their respective parent event: Association between any given chunk and its parent event The chunk\u2019s position in the parent event The total number of chunks of the parent event Considerations Chunking places additional burden on client applications. First, implementing the chunking and unchunking logic requires more application development. Second, the consumer application needs to be able to cache the chunks as it waits to receive all the smaller chunks that comprise the original event. This, in turn, can have implications on memory fragmentation and longer garbage collection (GC). Mitigating this depends on the programming language: in Java, for example, the JVM heap size and GC can be tuned. Client applications that are not aware of the protocol used for chunking events may not be able to reconstruct the original event accurately. References To handle large events, an alternative approach that may be preferred is Claim Check","title":"Event Chunking"},{"location":"event-processing/event-chunking/#event-chunking","text":"Sometimes compression can reduce message size, but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc.","title":"Event Chunking"},{"location":"event-processing/event-chunking/#problem","text":"How do I handle use cases where the event payload is too large to move through the event streaming platform as a single event?","title":"Problem"},{"location":"event-processing/event-chunking/#solution","text":"Instead of storing the entire event as a single event in the event streaming platform, break it into chunks (an approach called \"chunking\") so that the large event is sent across as multiple smaller events. The producer can do the chunking when writing events into the event streaming platform. Downstream clients consume the chunks and, when all the smaller chunks have been received, recombine (\"unchunk\") them to restore the original event.","title":"Solution"},{"location":"event-processing/event-chunking/#implementation","text":"Use metadata to track each chunk so that they can be associated to their respective parent event: Association between any given chunk and its parent event The chunk\u2019s position in the parent event The total number of chunks of the parent event","title":"Implementation"},{"location":"event-processing/event-chunking/#considerations","text":"Chunking places additional burden on client applications. First, implementing the chunking and unchunking logic requires more application development. Second, the consumer application needs to be able to cache the chunks as it waits to receive all the smaller chunks that comprise the original event. This, in turn, can have implications on memory fragmentation and longer garbage collection (GC). Mitigating this depends on the programming language: in Java, for example, the JVM heap size and GC can be tuned. Client applications that are not aware of the protocol used for chunking events may not be able to reconstruct the original event accurately.","title":"Considerations"},{"location":"event-processing/event-chunking/#references","text":"To handle large events, an alternative approach that may be preferred is Claim Check","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processors may need to operate over a subset of Events over a particular Event Stream . Problem How can an application discard uninteresting events? Solution Implementation The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processors may need to operate over a subset of Events over a particular Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa. Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Implementation In this example, we use Kafka's Java producer client to implement a Mapper that constructs an Event ( PublicationEvent ) from the Domain Model ( Publication ) before the event is written to an Event Stream a.k.a. topic in Kafka. private final IMapper domainToEventMapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author /* event key */, domainToEventMapper.map(newPub)); We can implement the reverse operation in a second Mapper that converts PublicationEvent instances back into Domain Object updates: private final IMapper eventToDomainMapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = eventToDomainMapper.map(pubEvent); domainStore.update(newPub); Considerations The mapper may optionally validate the schema of the converted objects, see the Schema Validator pattern. References Related patterns: Event Serializer and Event Deserializer This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution"},{"location":"event-processing/event-mapper/#implementation","text":"In this example, we use Kafka's Java producer client to implement a Mapper that constructs an Event ( PublicationEvent ) from the Domain Model ( Publication ) before the event is written to an Event Stream a.k.a. topic in Kafka. private final IMapper domainToEventMapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author /* event key */, domainToEventMapper.map(newPub)); We can implement the reverse operation in a second Mapper that converts PublicationEvent instances back into Domain Object updates: private final IMapper eventToDomainMapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = eventToDomainMapper.map(pubEvent); domainStore.update(newPub);","title":"Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"The mapper may optionally validate the schema of the converted objects, see the Schema Validator pattern.","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"Related patterns: Event Serializer and Event Deserializer This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied. Problem TODO: Technology agnostic English explanation of the problem Solution TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first). Implementation TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code) Considerations TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"TODO: Technology agnostic English explanation of the problem","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first).","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code)","title":"Implementation"},{"location":"event-processing/event-processing-application/#considerations","text":"TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor Once data such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. is set in motion as streams of events on an Event Streaming Platform , we want to put it to use and create value from it. How do we do this? Problem How do we process Events in an Event Streaming Platform ? Solution We build an Event Processor, which is a component that reads Events and processes them, and possibly writes new Events as the result of its processing. As such, it may act as an Event Source and/or Event Sink , and in practice often acts as both. An event processor can be distributed, which means it has multiple instances that run across different machines. In this case the processing of Events happens concurrently across these instances. An important characteristic of an event processor is that it should allow for composition with other event processors. That's because, in practice, we rarely use a single event processor in isolation. Instead, we compose and connect (via Event Streams ) one or more event processors inside an Event Processing Application that fully implements one particular use case end-to-end, or (e.g., in the case of microservices) that implements a subset of the overall business logic limited to the bounded context of a particular domain. An event processor performs a specific task within the event processing application. You can think of it as one processing node (or processing step) of a larger processing topology. Examples are the mapping of an event type to a domain object, filtering only the important events out of an Event Stream , enriching an event stream with additional data by joining it to another stream or database table, triggering alerts, or creating new events for consumption by other applications. Implementation There are multiple ways to create an Event Processing Application using Event Processors, we will look at two. ksqlDB The streaming database ksqlDB provides a familiar SQL syntax that allows us to create Event Processing Applications . ksqlDB takes parses SQL commands and constructs and manages the Event Processors we define as part of an Event Processing Application . In the following example, we create a ksqlDB query to reading data from the readings Event Stream and \"cleaning\" the Event values. The query publishes the clean readings to a new stream called clean_readings . Here, this query acts as an event processing application comprised of multiple event processors that are connected to each other. CREATE STREAM clean_readings AS SELECT sensor, reading, UCASE(location) AS location FROM readings EMIT CHANGES; With ksqlDB, you can view each section of the command as the construction of a different Event Processor: CREATE STREAM defines the new output Event Stream that this application will produce Events to. SELECT ... is a mapping function, taking each input Event and \"cleaning\" it as defined. In this example, this simply means upper casing the location field in each input reading. FROM ... is a source Event Processor that defines the input Event Stream for the overall application. EMIT CHANGES is ksqlDB syntax which defines our query as continuously running, and that incremental changes will be produced as the query runs perpetually. Kafka Streams The Kafka Streams DSL provides abstractions for Event Streams and Tables as well as stateful and stateless transformation functions ( map , filter , etc...). These functions act as the Event Processor in the larger Event Processing Application you build with the Kafka Streams library. builder .stream(\"readings\"); .mapValues((key, value)-> new Reading(value.sensor, value.reading, value.location.toUpperCase()) .to(\"clean\"); In the above example we use the Kafka Streams Builder to construct the stream processing topology. First we create an input stream with the stream function. This creates an Event Stream from the designated Kafka topic. Next we transform the Events using the mapValues function. This function accepts each Event and returns a new Event with any desired transformations to the values. Finally we write our transformed Events to a destination Kafka topic using the to function. This function terminates our stream processing topology. Considerations While it could be tempting to build a \"multi-purpose\" event processor, it's important that processors are designed in a composable way. By building processors as discrete units, it's easier to reason about what each processor does and, by extension, what the Event Processing Application does. References Event Processing Applications are composed of Event Processors. In Kafka Streams , a processor is a node in the processor topology representing a step to transform Events . Blog post: How real-time stream processing works with ksqlDB, Animated . Introduction to Apache Kafka: How Kafka works provides details on the core Kafka concepts like Events and topics.","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"Once data such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. is set in motion as streams of events on an Event Streaming Platform , we want to put it to use and create value from it. How do we do this?","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"How do we process Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"We build an Event Processor, which is a component that reads Events and processes them, and possibly writes new Events as the result of its processing. As such, it may act as an Event Source and/or Event Sink , and in practice often acts as both. An event processor can be distributed, which means it has multiple instances that run across different machines. In this case the processing of Events happens concurrently across these instances. An important characteristic of an event processor is that it should allow for composition with other event processors. That's because, in practice, we rarely use a single event processor in isolation. Instead, we compose and connect (via Event Streams ) one or more event processors inside an Event Processing Application that fully implements one particular use case end-to-end, or (e.g., in the case of microservices) that implements a subset of the overall business logic limited to the bounded context of a particular domain. An event processor performs a specific task within the event processing application. You can think of it as one processing node (or processing step) of a larger processing topology. Examples are the mapping of an event type to a domain object, filtering only the important events out of an Event Stream , enriching an event stream with additional data by joining it to another stream or database table, triggering alerts, or creating new events for consumption by other applications.","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"There are multiple ways to create an Event Processing Application using Event Processors, we will look at two.","title":"Implementation"},{"location":"event-processing/event-processor/#ksqldb","text":"The streaming database ksqlDB provides a familiar SQL syntax that allows us to create Event Processing Applications . ksqlDB takes parses SQL commands and constructs and manages the Event Processors we define as part of an Event Processing Application . In the following example, we create a ksqlDB query to reading data from the readings Event Stream and \"cleaning\" the Event values. The query publishes the clean readings to a new stream called clean_readings . Here, this query acts as an event processing application comprised of multiple event processors that are connected to each other. CREATE STREAM clean_readings AS SELECT sensor, reading, UCASE(location) AS location FROM readings EMIT CHANGES; With ksqlDB, you can view each section of the command as the construction of a different Event Processor: CREATE STREAM defines the new output Event Stream that this application will produce Events to. SELECT ... is a mapping function, taking each input Event and \"cleaning\" it as defined. In this example, this simply means upper casing the location field in each input reading. FROM ... is a source Event Processor that defines the input Event Stream for the overall application. EMIT CHANGES is ksqlDB syntax which defines our query as continuously running, and that incremental changes will be produced as the query runs perpetually.","title":"ksqlDB"},{"location":"event-processing/event-processor/#kafka-streams","text":"The Kafka Streams DSL provides abstractions for Event Streams and Tables as well as stateful and stateless transformation functions ( map , filter , etc...). These functions act as the Event Processor in the larger Event Processing Application you build with the Kafka Streams library. builder .stream(\"readings\"); .mapValues((key, value)-> new Reading(value.sensor, value.reading, value.location.toUpperCase()) .to(\"clean\"); In the above example we use the Kafka Streams Builder to construct the stream processing topology. First we create an input stream with the stream function. This creates an Event Stream from the designated Kafka topic. Next we transform the Events using the mapValues function. This function accepts each Event and returns a new Event with any desired transformations to the values. Finally we write our transformed Events to a destination Kafka topic using the to function. This function terminates our stream processing topology.","title":"Kafka Streams"},{"location":"event-processing/event-processor/#considerations","text":"While it could be tempting to build a \"multi-purpose\" event processor, it's important that processors are designed in a composable way. By building processors as discrete units, it's easier to reason about what each processor does and, by extension, what the Event Processing Application does.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"Event Processing Applications are composed of Event Processors. In Kafka Streams , a processor is a node in the processor topology representing a step to transform Events . Blog post: How real-time stream processing works with ksqlDB, Animated . Introduction to Apache Kafka: How Kafka works provides details on the core Kafka concepts like Events and topics.","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Event Streams may contain a subset of Events which need to be processed in isolation. For example, an inventory check system may be distributed across multiple physical systems, and the target system depends on the category of the item being checked. Problem How can we isolate Events into a dedicated Event Stream based on some attribute of the Events ? Solution Implementation With ksqlDB , you can continuously route events to a different stream using the CREATE STREAM syntax with an appropriate WHERE filter. CREATE STREAM payments ...; CREATE STREAM payments_france AS SELECT * FROM payments WHERE country = 'france'; CREATE STREAM payments_spain AS SELECT * FROM payments WHERE country = 'spain'; With the Kafka Streams library , use a TopicNameExtractor to route events to different streams (topics). The TopicNameExtractor has one method to implement, extract() , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, and other contextual information about the event. You can use any of the given parameters to generate and return the desired destination topic name for the given event, and Kafka Streams will complete the routing. CountryTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.country) { case \"france\": return \"france-topic\"; case \"spain\": return \"spain-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to(new CountryTopicExtractor()); Considerations Event Routers should not modify the Event itself and instead only provide the proper routing to the desired destinations. Consider the use of an Event Envelope if an event router should attach additional information or context to an event. References This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See the tutorial How to dynamically choose the output topic at runtime for a full example of dynamically routing events at runtime.","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"Event Streams may contain a subset of Events which need to be processed in isolation. For example, an inventory check system may be distributed across multiple physical systems, and the target system depends on the category of the item being checked.","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How can we isolate Events into a dedicated Event Stream based on some attribute of the Events ?","title":"Problem"},{"location":"event-processing/event-router/#solution","text":"","title":"Solution"},{"location":"event-processing/event-router/#implementation","text":"With ksqlDB , you can continuously route events to a different stream using the CREATE STREAM syntax with an appropriate WHERE filter. CREATE STREAM payments ...; CREATE STREAM payments_france AS SELECT * FROM payments WHERE country = 'france'; CREATE STREAM payments_spain AS SELECT * FROM payments WHERE country = 'spain'; With the Kafka Streams library , use a TopicNameExtractor to route events to different streams (topics). The TopicNameExtractor has one method to implement, extract() , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, and other contextual information about the event. You can use any of the given parameters to generate and return the desired destination topic name for the given event, and Kafka Streams will complete the routing. CountryTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.country) { case \"france\": return \"france-topic\"; case \"spain\": return \"spain-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to(new CountryTopicExtractor());","title":"Implementation"},{"location":"event-processing/event-router/#considerations","text":"Event Routers should not modify the Event itself and instead only provide the proper routing to the desired destinations. Consider the use of an Event Envelope if an event router should attach additional information or context to an event.","title":"Considerations"},{"location":"event-processing/event-router/#references","text":"This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See the tutorial How to dynamically choose the output topic at runtime for a full example of dynamically routing events at runtime.","title":"References"},{"location":"event-processing/event-splitter/","text":"Event Splitter One Event may actually contain multiple child events within it, each of which may need to be processed in different ways. Problem How can an Event be split into multiple events? Solution First, split the original event into multiple child events. Then, publish one event per child. Implementation Many event processing technologies support this operation. ksqlDB has the EXPLODE() table function which takes an array and outputs one value for each of the elements of the array. The example below processes each input event, un-nesting the array and generating new events for each element with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; Kafka Streams has an analogous method called flatMap() . The example below processes each input event, generating new events with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } ); Considerations If child events need to be routed to different streams, see Event Router for routing events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: Your use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier. References This pattern is derived from Splitter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Splitter"},{"location":"event-processing/event-splitter/#event-splitter","text":"One Event may actually contain multiple child events within it, each of which may need to be processed in different ways.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#problem","text":"How can an Event be split into multiple events?","title":"Problem"},{"location":"event-processing/event-splitter/#solution","text":"First, split the original event into multiple child events. Then, publish one event per child.","title":"Solution"},{"location":"event-processing/event-splitter/#implementation","text":"Many event processing technologies support this operation. ksqlDB has the EXPLODE() table function which takes an array and outputs one value for each of the elements of the array. The example below processes each input event, un-nesting the array and generating new events for each element with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; Kafka Streams has an analogous method called flatMap() . The example below processes each input event, generating new events with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } );","title":"Implementation"},{"location":"event-processing/event-splitter/#considerations","text":"If child events need to be routed to different streams, see Event Router for routing events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: Your use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier.","title":"Considerations"},{"location":"event-processing/event-splitter/#references","text":"This pattern is derived from Splitter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/event-streaming-api/","text":"Event Streaming API Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way. Problem How can my application connect to an Event Streaming Platform to send and receive Events ? Solution The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The highler level libraries allow the application to focus on business logic leaving the details of the platform communication to the API. References This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#event-streaming-api","text":"Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#problem","text":"How can my application connect to an Event Streaming Platform to send and receive Events ?","title":"Problem"},{"location":"event-processing/event-streaming-api/#solution","text":"The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The highler level libraries allow the application to focus on business logic leaving the details of the platform communication to the API.","title":"Solution"},{"location":"event-processing/event-streaming-api/#references","text":"This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"References"},{"location":"event-processing/event-translator/","text":"Event Translator Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them. Problem How can systems using different data formats communicate with each other using Events ? Solution An Event Translator converts a data format into a standard format familiar to downstream Event Processors . This can take the form of field manipulation, for example mapping one event schema (ref) to another event schema. Another common form is different serialization types, for example, translating Avro to Json or Protobuf to Avro. Implementation The streaming database ksqlDB provides the ability to create Event Streams with SQL statements. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream Considerations In some cases translations will be unidirectional if data is lost, for example translating XML to json will often lose information meaning the original form cannot be recreated. References This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Translator"},{"location":"event-processing/event-translator/#event-translator","text":"Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them.","title":"Event Translator"},{"location":"event-processing/event-translator/#problem","text":"How can systems using different data formats communicate with each other using Events ?","title":"Problem"},{"location":"event-processing/event-translator/#solution","text":"An Event Translator converts a data format into a standard format familiar to downstream Event Processors . This can take the form of field manipulation, for example mapping one event schema (ref) to another event schema. Another common form is different serialization types, for example, translating Avro to Json or Protobuf to Avro.","title":"Solution"},{"location":"event-processing/event-translator/#implementation","text":"The streaming database ksqlDB provides the ability to create Event Streams with SQL statements. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream","title":"Implementation"},{"location":"event-processing/event-translator/#considerations","text":"In some cases translations will be unidirectional if data is lost, for example translating XML to json will often lose information meaning the original form cannot be recreated.","title":"Considerations"},{"location":"event-processing/event-translator/#references","text":"This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations. Problem How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ? Solution Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system. Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink . Considerations There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations.","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ?","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution","text":"Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system.","title":"Solution"},{"location":"event-sink/event-sink-connector/#implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink .","title":"Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink. Problem How can we read (or consume / subscribe to) Events in an Event Streaming Platform ? Solution Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB . Implementation ksqlDB example: Reading events from an existing Kafka topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } References The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink.","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can we read (or consume / subscribe to) Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-sink/event-sink/#solution","text":"Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB .","title":"Solution"},{"location":"event-sink/event-sink/#implementation","text":"ksqlDB example: Reading events from an existing Kafka topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); }","title":"Implementation"},{"location":"event-sink/event-sink/#references","text":"The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Problem How do I update a value in a database and create an associated event with the least amount of effort? Solution Pattern Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails). Example Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled. References TODO: Add references?","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event with the least amount of effort?","title":"Problem"},{"location":"event-source/database-write-aside/#solution-pattern","text":"Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails).","title":"Solution Pattern"},{"location":"event-source/database-write-aside/#example-implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Example Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-aside/#references","text":"TODO: Add references?","title":"References"},{"location":"event-source/database-write-through/","text":"Database Write Through Problem How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform. Solution Pattern Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Example Implementation TODO: Example for CDC? Considerations The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform.","title":"Problem"},{"location":"event-source/database-write-through/#solution-pattern","text":"Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution Pattern"},{"location":"event-source/database-write-through/#example-implementation","text":"TODO: Example for CDC?","title":"Example Implementation"},{"location":"event-source/database-write-through/#considerations","text":"The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources . Problem How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events . Solution Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform . Implementation When connecting a cloud services and traditional systems to Apache Kafka , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username'); Considerations End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery, cf. the Guaranteed Delivery pattern) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform. References This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources .","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events .","title":"Problem"},{"location":"event-source/event-source-connector/#solution","text":"Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform .","title":"Solution"},{"location":"event-source/event-source-connector/#implementation","text":"When connecting a cloud services and traditional systems to Apache Kafka , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username');","title":"Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery, cf. the Guaranteed Delivery pattern) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an event source is the opposite of an Event Sink . In practice, however, components such as an event processing application can act as both an event source and an event sink. Problem How can we create Events in an Event Streaming Platform ? Solution Use an Event Source, which typically acts as a client in an Event Streaming Platform . Examples are an Event Source Connector (which continuously imports data as Event Streams into the Event Streaming Platform from an external system such as a cloud services or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB . Implementation Normally, an actual component such as an application for Kafka would be writing Events into an Event Stream , via a client library, API, gateway, etc. You can also write events directly using SQL syntax: the streaming database ksqlDB , for example, provides an INSERT statement. CREATE STREAM users (username VARCHAR, name VARCHAR, phone VARCHAR) with (kafka_topic='users-topic', value_format='json'); INSERT INTO users (username, name, phone) VALUES ('awilson', 'Allison', '+1 555-555-1234'); References ksqlDB The event streaming database purpose-built for stream processing applications. How to build client applications for writing events into an Event Stream .","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an event source is the opposite of an Event Sink . In practice, however, components such as an event processing application can act as both an event source and an event sink.","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can we create Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-source/event-source/#solution","text":"Use an Event Source, which typically acts as a client in an Event Streaming Platform . Examples are an Event Source Connector (which continuously imports data as Event Streams into the Event Streaming Platform from an external system such as a cloud services or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB .","title":"Solution"},{"location":"event-source/event-source/#implementation","text":"Normally, an actual component such as an application for Kafka would be writing Events into an Event Stream , via a client library, API, gateway, etc. You can also write events directly using SQL syntax: the streaming database ksqlDB , for example, provides an INSERT statement. CREATE STREAM users (username VARCHAR, name VARCHAR, phone VARCHAR) with (kafka_topic='users-topic', value_format='json'); INSERT INTO users (username, name, phone) VALUES ('awilson', 'Allison', '+1 555-555-1234');","title":"Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications. How to build client applications for writing events into an Event Stream .","title":"References"},{"location":"event-source/schema-validator/","text":"Schema Validator In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process these events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events, which is an important aspect of putting Data Contracts in place for data governance purposes. Problem How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream? Solution Validate whether an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. Such schema validation can be done: On the server side by the Event Streaming Platform that receives the Event . Events that fail schema validation and thus violate the Data Contract are rejected. On the client side by the Event Source that creates the Event . For example, an Event Source Connector can validate Events prior to ingestion into the Event Streaming Platform . Or, an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (e.g., Confluent's serializer/deserializers for Kafka). Implementation With Confluent, schema validation is fully supported with a per-environment hosted Schema Registry . Use the cloud UI to enable schema registry in your cloud provider of choice. Schemas can be managed per topic using the cloud UI or the Confluent Cloud CLI . An example command to create a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO Considerations Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write. Instead, consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (e.g., in regulated industries). Schema validation results in a load increase because it impacts the write path of every event. Client-side validation impacts primarily the load of the client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ). References See the Schema Compatibility pattern for information on how schemas can evolve over time and be verified. Learn more how to Manage and Validate Schemas with Confluent and Kafka .","title":"Schema Validator"},{"location":"event-source/schema-validator/#schema-validator","text":"In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process these events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events, which is an important aspect of putting Data Contracts in place for data governance purposes.","title":"Schema Validator"},{"location":"event-source/schema-validator/#problem","text":"How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream?","title":"Problem"},{"location":"event-source/schema-validator/#solution","text":"Validate whether an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. Such schema validation can be done: On the server side by the Event Streaming Platform that receives the Event . Events that fail schema validation and thus violate the Data Contract are rejected. On the client side by the Event Source that creates the Event . For example, an Event Source Connector can validate Events prior to ingestion into the Event Streaming Platform . Or, an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (e.g., Confluent's serializer/deserializers for Kafka).","title":"Solution"},{"location":"event-source/schema-validator/#implementation","text":"With Confluent, schema validation is fully supported with a per-environment hosted Schema Registry . Use the cloud UI to enable schema registry in your cloud provider of choice. Schemas can be managed per topic using the cloud UI or the Confluent Cloud CLI . An example command to create a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO","title":"Implementation"},{"location":"event-source/schema-validator/#considerations","text":"Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write. Instead, consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (e.g., in regulated industries). Schema validation results in a load increase because it impacts the write path of every event. Client-side validation impacts primarily the load of the client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ).","title":"Considerations"},{"location":"event-source/schema-validator/#references","text":"See the Schema Compatibility pattern for information on how schemas can evolve over time and be verified. Learn more how to Manage and Validate Schemas with Confluent and Kafka .","title":"References"},{"location":"event-storage/compacted-event-stream/","text":"Compacted Event Stream Event Streams often represent keyed snapshots of state, similar to a table in a relational database. That is, the Events contain a primary key (identifier) and data that represents the latest information of the business entity related to the Event, such as the latest balance per customer account. Event Processing Applications will need to process these Events to determine the current state of the business entity. However, processing the entire Event Stream history is often not practical. Problem How can a (keyed) table be stored in an Event Stream forever, using the minimum amount of space? Solution Remove events from the Event Stream that represent outdated information and have been superseded by new Events . The table's current data (i.e., its latest state) is represented by the remaining Events in the stream. This approach bounds the storage space of the table's Event Stream to \u0398(number of unique keys currently in table), rather than \u0398(total number of change events for table). In practice, the number of unique keys (e.g., unique customer IDs) is typically much smaller than the number of table changes (e.g., total number of changes across all customer profiles). A Compacted Event Stream thus reduces the storage space significantly in most cases. Implementation Apache Kafka\u00ae provides this functionality natively through its Topic Compaction feature. An Event Stream (topic in Kafka) is scanned periodically to remove any old Events that have been superseded by newer Events that have the same key, such as as the same customer ID. Note that compaction is an asynchronous process in Kafka, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted Event Stream called customer-profiles with Kafka: \u279c kafka-topics --create \\ --bootstrap-server <bootstrap-url> \\ --replication-factor 3 \\ --partitions 3 \\ --topic customer-profiles \\ --config cleanup.policy=compact Created topic customer-profiles. The kafka-topics command can also verify the current topic's configuration: \u279c kafka-topics --bootstrap-server localhost:9092 --topic customer-profiles --describe Topic: customer-profiles PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: customer-profiles Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline: Considerations Compacted Event Streams allow for some optimizations: First, they allow the Event Streaming Platform to limit the storage growth of the Event Stream in a data-specific way, rather than removing Events universally after a pre-configured period of time. Second, having smaller Event Streams allows for faster recovery and system migration strategies. It is important to understand that compaction, on purpose, removes historical data from an Event Stream by removing superseded Events as defined above. In many use cases, however, historical data should not be removed, such as for a stream of financial transactions, where every single transaction needs to be recorded and stored. Here, if the storage of the Event Stream is the primary concern, use an Infinite Retention Event Stream instead of a compacted stream. References Compacted Event Streams are highly related to the State Table pattern. Compacted Event Streams work a bit like simple Log Structured Merge Trees . Cleanup policy configuration of Kafka topics.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#compacted-event-stream","text":"Event Streams often represent keyed snapshots of state, similar to a table in a relational database. That is, the Events contain a primary key (identifier) and data that represents the latest information of the business entity related to the Event, such as the latest balance per customer account. Event Processing Applications will need to process these Events to determine the current state of the business entity. However, processing the entire Event Stream history is often not practical.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#problem","text":"How can a (keyed) table be stored in an Event Stream forever, using the minimum amount of space?","title":"Problem"},{"location":"event-storage/compacted-event-stream/#solution","text":"Remove events from the Event Stream that represent outdated information and have been superseded by new Events . The table's current data (i.e., its latest state) is represented by the remaining Events in the stream. This approach bounds the storage space of the table's Event Stream to \u0398(number of unique keys currently in table), rather than \u0398(total number of change events for table). In practice, the number of unique keys (e.g., unique customer IDs) is typically much smaller than the number of table changes (e.g., total number of changes across all customer profiles). A Compacted Event Stream thus reduces the storage space significantly in most cases.","title":"Solution"},{"location":"event-storage/compacted-event-stream/#implementation","text":"Apache Kafka\u00ae provides this functionality natively through its Topic Compaction feature. An Event Stream (topic in Kafka) is scanned periodically to remove any old Events that have been superseded by newer Events that have the same key, such as as the same customer ID. Note that compaction is an asynchronous process in Kafka, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted Event Stream called customer-profiles with Kafka: \u279c kafka-topics --create \\ --bootstrap-server <bootstrap-url> \\ --replication-factor 3 \\ --partitions 3 \\ --topic customer-profiles \\ --config cleanup.policy=compact Created topic customer-profiles. The kafka-topics command can also verify the current topic's configuration: \u279c kafka-topics --bootstrap-server localhost:9092 --topic customer-profiles --describe Topic: customer-profiles PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: customer-profiles Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline:","title":"Implementation"},{"location":"event-storage/compacted-event-stream/#considerations","text":"Compacted Event Streams allow for some optimizations: First, they allow the Event Streaming Platform to limit the storage growth of the Event Stream in a data-specific way, rather than removing Events universally after a pre-configured period of time. Second, having smaller Event Streams allows for faster recovery and system migration strategies. It is important to understand that compaction, on purpose, removes historical data from an Event Stream by removing superseded Events as defined above. In many use cases, however, historical data should not be removed, such as for a stream of financial transactions, where every single transaction needs to be recorded and stored. Here, if the storage of the Event Stream is the primary concern, use an Infinite Retention Event Stream instead of a compacted stream.","title":"Considerations"},{"location":"event-storage/compacted-event-stream/#references","text":"Compacted Event Streams are highly related to the State Table pattern. Compacted Event Streams work a bit like simple Log Structured Merge Trees . Cleanup policy configuration of Kafka topics.","title":"References"},{"location":"event-storage/event-store/","text":"Event Store When considering an architecture based on an Event Streaming Platform , the first fundamental question is, \"How do we store our events?\" This isn't as obvious as it first sounds, as we have to consider persistence, query performance, write throughput, availability, auditing and many other concerns. This decision will affect all the ones that follow. Problem How can events be stored such that they form a reliable source of truth for applications? Solution Incoming events are stored in an Event Stream , implemented as an append-only log. By choosing this data structure, we can guarantee constant-time (\u0398(1)) writes, lock-free concurrent reads, and straightforward replication across multiple machines. Implementation Apache Kafka\u00ae is an event store that maintains a persistent, append-only stream \u2014 a topic \u2014 for each kind of event you need to store. These topics are: Write-efficient - an append-only log is one of the fastest, cheapest data structures to write to. Read efficient - multiple readers (cf. Event Processor ) can consume the same stream without blocking. Durable - all events are written to storage (e.g., local disk, network storage device), either synchronously (for maximum reliability) or asynchronously (for maximum throughput). Events can be as long-lived as needed, and even stored forever. Highly-available - each event is written to multiple storage devices and replicated across multiple machines, and in the case of failure one of the redundant machines takes over. Auditable - every change is captured and persisted. Every result can be traced back to its source event(s). Considerations It's worth briefly contrasting Apache Kafka\u00ae with message queues and relational databases. While queues also concern themselves with a stream of events, they often consider events as short-lived, independent messages. A message may only exist in memory, or it may be durable enough for data to survive server restarts, but in general they aren't intended to hold on to events for months or even years. Further, their querying capabilities may be limited to simple filtering, offloading more complex queries like joins and aggregations to the application level. In contrast, relational databases are very good at maintaining a persistent state of the world in perpetuity, and answering arbitrary questions about it, but they often fall short on auditing - answering which events led up to the current state - and on liveness - what new events do we need to consider. They are predominantly designed for use cases that operate on data at rest, whereas an Event Store is designed from the ground up for data in motion and event streaming. By beginning with a fundamental data-structure for event capture, and building on that to provide long-term persistence and arbitrary analysis capabilities, Apache Kafka\u00ae provides an ideal choice of event store for modern, data-driven architectures. References See also: Geo-Replication . Using logs to build a solid data infrastructure What Is Apache Kafka? Kafka: The Definitive Guide free Ebook.","title":"Event Store"},{"location":"event-storage/event-store/#event-store","text":"When considering an architecture based on an Event Streaming Platform , the first fundamental question is, \"How do we store our events?\" This isn't as obvious as it first sounds, as we have to consider persistence, query performance, write throughput, availability, auditing and many other concerns. This decision will affect all the ones that follow.","title":"Event Store"},{"location":"event-storage/event-store/#problem","text":"How can events be stored such that they form a reliable source of truth for applications?","title":"Problem"},{"location":"event-storage/event-store/#solution","text":"Incoming events are stored in an Event Stream , implemented as an append-only log. By choosing this data structure, we can guarantee constant-time (\u0398(1)) writes, lock-free concurrent reads, and straightforward replication across multiple machines.","title":"Solution"},{"location":"event-storage/event-store/#implementation","text":"Apache Kafka\u00ae is an event store that maintains a persistent, append-only stream \u2014 a topic \u2014 for each kind of event you need to store. These topics are: Write-efficient - an append-only log is one of the fastest, cheapest data structures to write to. Read efficient - multiple readers (cf. Event Processor ) can consume the same stream without blocking. Durable - all events are written to storage (e.g., local disk, network storage device), either synchronously (for maximum reliability) or asynchronously (for maximum throughput). Events can be as long-lived as needed, and even stored forever. Highly-available - each event is written to multiple storage devices and replicated across multiple machines, and in the case of failure one of the redundant machines takes over. Auditable - every change is captured and persisted. Every result can be traced back to its source event(s).","title":"Implementation"},{"location":"event-storage/event-store/#considerations","text":"It's worth briefly contrasting Apache Kafka\u00ae with message queues and relational databases. While queues also concern themselves with a stream of events, they often consider events as short-lived, independent messages. A message may only exist in memory, or it may be durable enough for data to survive server restarts, but in general they aren't intended to hold on to events for months or even years. Further, their querying capabilities may be limited to simple filtering, offloading more complex queries like joins and aggregations to the application level. In contrast, relational databases are very good at maintaining a persistent state of the world in perpetuity, and answering arbitrary questions about it, but they often fall short on auditing - answering which events led up to the current state - and on liveness - what new events do we need to consider. They are predominantly designed for use cases that operate on data at rest, whereas an Event Store is designed from the ground up for data in motion and event streaming. By beginning with a fundamental data-structure for event capture, and building on that to provide long-term persistence and arbitrary analysis capabilities, Apache Kafka\u00ae provides an ideal choice of event store for modern, data-driven architectures.","title":"Considerations"},{"location":"event-storage/event-store/#references","text":"See also: Geo-Replication . Using logs to build a solid data infrastructure What Is Apache Kafka? Kafka: The Definitive Guide free Ebook.","title":"References"},{"location":"event-storage/infinite-retention-event-stream/","text":"Infinite Retention Event Stream Many use cases demand that Events in an Event Stream will be stored for forever so that the dataset is available in its entirity. Problem How can we ensure that events in a stream are retained forever? Solution The solution for infinite retention depends on the specific Event Streaming Platform . Some platforms support infinite retention \"out of the box\", requiring no action on behalf of the end users. If an Event Streaming Platform does not support infinite storage, infinite retention can be partially achieved with an Event Sink Connector pattern which offloads Events into permanent external storage. Implementation When using Confluent Cloud , infinite retention is built into the Event Streaming Platform ( availability may be limited based on cluster type and cloud provider ). Users of the platform can benefit from infinite storage without any changes to their client applications or operations. For on-premises Event Streaming Platforms , Confluent Platform adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived Events are considered \"hot\", but as time moves on, they become \"colder\" and migrate to more cost-effective external storage like an AWS S3 bucket. As cloud-native object stores can effectively scale to infinite size, the Kafka cluster can act as the system of record for infinite Event Streams . Considerations Infinite Retention Streams are typically used to store entire datasets which will be used by many subscribers. For example, storing the canonical customer dataset in an Infinite Retention Event Stream makes it available to any other system, regardless of their database technology. The customer's dataset can be easily imported or reimported as a whole. Compacted Event Streams are often used as a form of Infinite Retention Event Stream. However compacted streams are not infinite. Instead, they retain only the most recent Events for each key, meaning their contents matches the dataset held in an equivalent CRUD database table. References The blog post Infinite Storage in Confluent goes describes the tiered storage approach in more detail. An Event Sink Connector can be used to implement an infinite retention event stream by loading Event into permanent external storage.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#infinite-retention-event-stream","text":"Many use cases demand that Events in an Event Stream will be stored for forever so that the dataset is available in its entirity.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#problem","text":"How can we ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infinite-retention-event-stream/#solution","text":"The solution for infinite retention depends on the specific Event Streaming Platform . Some platforms support infinite retention \"out of the box\", requiring no action on behalf of the end users. If an Event Streaming Platform does not support infinite storage, infinite retention can be partially achieved with an Event Sink Connector pattern which offloads Events into permanent external storage.","title":"Solution"},{"location":"event-storage/infinite-retention-event-stream/#implementation","text":"When using Confluent Cloud , infinite retention is built into the Event Streaming Platform ( availability may be limited based on cluster type and cloud provider ). Users of the platform can benefit from infinite storage without any changes to their client applications or operations. For on-premises Event Streaming Platforms , Confluent Platform adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived Events are considered \"hot\", but as time moves on, they become \"colder\" and migrate to more cost-effective external storage like an AWS S3 bucket. As cloud-native object stores can effectively scale to infinite size, the Kafka cluster can act as the system of record for infinite Event Streams .","title":"Implementation"},{"location":"event-storage/infinite-retention-event-stream/#considerations","text":"Infinite Retention Streams are typically used to store entire datasets which will be used by many subscribers. For example, storing the canonical customer dataset in an Infinite Retention Event Stream makes it available to any other system, regardless of their database technology. The customer's dataset can be easily imported or reimported as a whole. Compacted Event Streams are often used as a form of Infinite Retention Event Stream. However compacted streams are not infinite. Instead, they retain only the most recent Events for each key, meaning their contents matches the dataset held in an equivalent CRUD database table.","title":"Considerations"},{"location":"event-storage/infinite-retention-event-stream/#references","text":"The blog post Infinite Storage in Confluent goes describes the tiered storage approach in more detail. An Event Sink Connector can be used to implement an infinite retention event stream by loading Event into permanent external storage.","title":"References"},{"location":"event-stream/event-broker/","text":"Event Broker Loosely coupled components in a software architecture allow services and applications to change with minimal impact on dependent systems and applications. On the side of the organization, this loose coupling also allows different development teams to efficiently work independently from each another. Problem How can we decouple Event Sources from Event Sinks , both of which may include cloud services, systems like relational databases, as well as applications and microservices? Solution We use the Event Broker of an Event Streaming Platform for this decoupling. Typically, multiple such brokers are deployed as a distributed cluster to ensure elasticity, scalability, and fault-tolerance during operations. Event brokers collaborate on receiving and durably storing (write operations) as well as serving (read operations) Events into Event Streams from one or many clients in parallel. Clients that produce events are called the Event Sources , which are decoupled and isolated through the brokers from the clients that consume events, called the Event Sinks . Typically, the technical architecture follows the design of \"dumb brokers, smart clients\". Here, the broker intentionally limits its client-facing functionality to achieve the best performance and scalability. This means that additional work has to be performed by the broker's clients. For example, unlike in traditional messaging brokers, it is the responsibility of an event sink (consumer) to track its individual progress of reading and processing from an event stream. Implementation Apache Kafka is an open-source distributed Event Streaming Platform , which implements this Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. Many Event Processing Applications can produce, consume, and process Events from the cluster in parallel with strong guarantees such as transactions, using a fully decoupled yet coordinated architecture. Additionally, Kafka's protocol provides strong backwards and forwards compatibility guarantees between the server-side brokers and their client applications that produce, consume, and process events. For example, a cluster of brokers running an older version of Kafka can be used by client applications using a new version of Kafka. Similarly, older client applications keep working even when the cluster of brokers is upgraded to a newer version of Kafka (and Kafka also supports in-place version upgrades of clusters). This is another facet of decoupling the various components in a Kafka-based architecture, which results in even better flexibility during design and operations. Considerations Counter to traditional message brokers, event brokers provide a distributed, durable, and fault-tolerant storage layer. This has several important benefits. For instance, client applications can initiate and resume event production and consumption independently from each other. Similarly, they don't need to be connected to the brokers perpetually in order to not miss any events. When an application is taken offline for maintenance and subsequently restarted, then it will automatically resume its consumption and processing of an event stream exactly at the point where it stopped before. The strong guarantees provided by the brokers in the event streaming platform ensure that applications do not suffer from duplicate data or from data loss (e.g., missing out on events that were written during the maintenance window) in these situations, even in the face of failures such as machine or network outages. Another benefit is that client applications can \"rewind the time\" and re-consume historical data in event streams as often as needed. This is useful in many situations, including training and retraining models for machine learning, A/B testing, auditing and compliance, as well as fixing unexpected application errors and bugs that happened in production. References This pattern is derived from Message Broker in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Broker"},{"location":"event-stream/event-broker/#event-broker","text":"Loosely coupled components in a software architecture allow services and applications to change with minimal impact on dependent systems and applications. On the side of the organization, this loose coupling also allows different development teams to efficiently work independently from each another.","title":"Event Broker"},{"location":"event-stream/event-broker/#problem","text":"How can we decouple Event Sources from Event Sinks , both of which may include cloud services, systems like relational databases, as well as applications and microservices?","title":"Problem"},{"location":"event-stream/event-broker/#solution","text":"We use the Event Broker of an Event Streaming Platform for this decoupling. Typically, multiple such brokers are deployed as a distributed cluster to ensure elasticity, scalability, and fault-tolerance during operations. Event brokers collaborate on receiving and durably storing (write operations) as well as serving (read operations) Events into Event Streams from one or many clients in parallel. Clients that produce events are called the Event Sources , which are decoupled and isolated through the brokers from the clients that consume events, called the Event Sinks . Typically, the technical architecture follows the design of \"dumb brokers, smart clients\". Here, the broker intentionally limits its client-facing functionality to achieve the best performance and scalability. This means that additional work has to be performed by the broker's clients. For example, unlike in traditional messaging brokers, it is the responsibility of an event sink (consumer) to track its individual progress of reading and processing from an event stream.","title":"Solution"},{"location":"event-stream/event-broker/#implementation","text":"Apache Kafka is an open-source distributed Event Streaming Platform , which implements this Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. Many Event Processing Applications can produce, consume, and process Events from the cluster in parallel with strong guarantees such as transactions, using a fully decoupled yet coordinated architecture. Additionally, Kafka's protocol provides strong backwards and forwards compatibility guarantees between the server-side brokers and their client applications that produce, consume, and process events. For example, a cluster of brokers running an older version of Kafka can be used by client applications using a new version of Kafka. Similarly, older client applications keep working even when the cluster of brokers is upgraded to a newer version of Kafka (and Kafka also supports in-place version upgrades of clusters). This is another facet of decoupling the various components in a Kafka-based architecture, which results in even better flexibility during design and operations.","title":"Implementation"},{"location":"event-stream/event-broker/#considerations","text":"Counter to traditional message brokers, event brokers provide a distributed, durable, and fault-tolerant storage layer. This has several important benefits. For instance, client applications can initiate and resume event production and consumption independently from each other. Similarly, they don't need to be connected to the brokers perpetually in order to not miss any events. When an application is taken offline for maintenance and subsequently restarted, then it will automatically resume its consumption and processing of an event stream exactly at the point where it stopped before. The strong guarantees provided by the brokers in the event streaming platform ensure that applications do not suffer from duplicate data or from data loss (e.g., missing out on events that were written during the maintenance window) in these situations, even in the face of failures such as machine or network outages. Another benefit is that client applications can \"rewind the time\" and re-consume historical data in event streams as often as needed. This is useful in many situations, including training and retraining models for machine learning, A/B testing, auditing and compliance, as well as fixing unexpected application errors and bugs that happened in production.","title":"Considerations"},{"location":"event-stream/event-broker/#references","text":"This pattern is derived from Message Broker in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-stream/event-stream/","text":"Event Stream Event Processing Applications need to communicate, and ideally the communication is facilitated with Events . The applications need a standard mechanism to use for this communication. Problem How can Event Processors and applications communicate with each other, using event streaming? Solution Connect the Event Processing Applications with an Event Stream. Event Sources produce Events to the Event Stream, and Event Processors and Event Sinks consume them. Event Streams are named, allowing communication over a specific stream of Events . Note how Event Streams decouple the source and sink applications, which communicate indirectly and asynchronously with each other through events. Additionally, Event data formats are often validated in order to govern the communication between applications. Generally speaking, an Event Stream records the history of what has happened in the world as a sequence of events (think: a sequence of facts). An example stream is a sales ledger or the sequence of moves in a chess match. This history is an ordered sequence or chain of events, so we know which event happened before another event to infer causality (e.g., \u201cWhite moved the e2 pawn to e4, then Black moved the e7 pawn to e5\u201d). A stream thus represents both the past and the present: as we go from today to tomorrow\u2014or from one millisecond to the next\u2014new events are constantly being appended to the history. Technically, a stream provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and you can have many events for one key, such as the customer ID as the key for a stream of payments of all customers. Implementation The streaming database ksqlDB supports Event Streams using a familiar SQL syntax. The following example creates a stream of events named riderLocations , representing locations of riders in a car-sharing service. The data format is JSON . CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) WITH (kafka_topic='locations', value_format='json'); New Events can be written to the riderLocations stream using the INSERT syntax: INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('c2309eec', 37.7877, -122.4205); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('18f4ea86', 37.3903, -122.0643); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ab5cbad', 37.3952, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('8b6eae59', 37.3944, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4a7c7b41', 37.4049, -122.0822); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ddad000', 37.7857, -122.4011); A push query a.k.a. a streaming query can be ran continuously over the stream using a SELECT , using the EMIT CHANGES clause. As new events arrive, this query will emit new results matching the WHERE conditionals. The following query looks for riders in close proximity to Mountain View, California, in the United States. -- Mountain View lat, long: 37.4133, -122.1162 SELECT * FROM riderLocations WHERE GEO_DISTANCE(latitude, longitude, 37.4133, -122.1162) <= 5 EMIT CHANGES; References This pattern is derived from Message Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The blog post series, Streams and Tables in Apache Kafka: A Primer goes into detail on streams, tables and other Kafka fundamentals.","title":"Event Stream"},{"location":"event-stream/event-stream/#event-stream","text":"Event Processing Applications need to communicate, and ideally the communication is facilitated with Events . The applications need a standard mechanism to use for this communication.","title":"Event Stream"},{"location":"event-stream/event-stream/#problem","text":"How can Event Processors and applications communicate with each other, using event streaming?","title":"Problem"},{"location":"event-stream/event-stream/#solution","text":"Connect the Event Processing Applications with an Event Stream. Event Sources produce Events to the Event Stream, and Event Processors and Event Sinks consume them. Event Streams are named, allowing communication over a specific stream of Events . Note how Event Streams decouple the source and sink applications, which communicate indirectly and asynchronously with each other through events. Additionally, Event data formats are often validated in order to govern the communication between applications. Generally speaking, an Event Stream records the history of what has happened in the world as a sequence of events (think: a sequence of facts). An example stream is a sales ledger or the sequence of moves in a chess match. This history is an ordered sequence or chain of events, so we know which event happened before another event to infer causality (e.g., \u201cWhite moved the e2 pawn to e4, then Black moved the e7 pawn to e5\u201d). A stream thus represents both the past and the present: as we go from today to tomorrow\u2014or from one millisecond to the next\u2014new events are constantly being appended to the history. Technically, a stream provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and you can have many events for one key, such as the customer ID as the key for a stream of payments of all customers.","title":"Solution"},{"location":"event-stream/event-stream/#implementation","text":"The streaming database ksqlDB supports Event Streams using a familiar SQL syntax. The following example creates a stream of events named riderLocations , representing locations of riders in a car-sharing service. The data format is JSON . CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) WITH (kafka_topic='locations', value_format='json'); New Events can be written to the riderLocations stream using the INSERT syntax: INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('c2309eec', 37.7877, -122.4205); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('18f4ea86', 37.3903, -122.0643); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ab5cbad', 37.3952, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('8b6eae59', 37.3944, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4a7c7b41', 37.4049, -122.0822); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ddad000', 37.7857, -122.4011); A push query a.k.a. a streaming query can be ran continuously over the stream using a SELECT , using the EMIT CHANGES clause. As new events arrive, this query will emit new results matching the WHERE conditionals. The following query looks for riders in close proximity to Mountain View, California, in the United States. -- Mountain View lat, long: 37.4133, -122.1162 SELECT * FROM riderLocations WHERE GEO_DISTANCE(latitude, longitude, 37.4133, -122.1162) <= 5 EMIT CHANGES;","title":"Implementation"},{"location":"event-stream/event-stream/#references","text":"This pattern is derived from Message Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The blog post series, Streams and Tables in Apache Kafka: A Primer goes into detail on streams, tables and other Kafka fundamentals.","title":"References"},{"location":"stream-processing/event-grouper/","text":"Event Grouper Problem How can I group individual but related events from the same stream/table so that they can subsequently be processed as a whole? Solution Pattern ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. Example Implementation SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; References Session Windows Hopping Windows Tumbling Windows","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can I group individual but related events from the same stream/table so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution-pattern","text":"ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window.","title":"Solution Pattern"},{"location":"stream-processing/event-grouper/#example-implementation","text":"SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Example Implementation"},{"location":"stream-processing/event-grouper/#references","text":"Session Windows Hopping Windows Tumbling Windows","title":"References"},{"location":"stream-processing/event-stream-merger/","text":"Event Stream Merger An Event Streaming Application may contain multiple Event Stream instances. But in some cases it may make sense for the application to merge the different event streams into a single event stream, without changing the individual events. While this may seem logically related to a join, the merge is a completely different operation. A join produces results by combining events with the same key to produce a new event, possibly of a different type. Whereas the merge combines the events from multiple streams into a single stream, but the individual events are unchanged and remain independent of each other. Problem How can an application merge different event streams? Solution Implementation The Kafka Streams DSL provides a merge operator which merges two streams into a single stream. You can then take the merged stream and use any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()... Considerations Kafka Streams provides no guarantees on the processing order of records from the underlying streams. When merging streams the key and value types must be the same. References Kafka Tutorial : Merging with Kafka Streams. Kafka Tutorial : Merging streams with ksqlDB.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#event-stream-merger","text":"An Event Streaming Application may contain multiple Event Stream instances. But in some cases it may make sense for the application to merge the different event streams into a single event stream, without changing the individual events. While this may seem logically related to a join, the merge is a completely different operation. A join produces results by combining events with the same key to produce a new event, possibly of a different type. Whereas the merge combines the events from multiple streams into a single stream, but the individual events are unchanged and remain independent of each other.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#problem","text":"How can an application merge different event streams?","title":"Problem"},{"location":"stream-processing/event-stream-merger/#solution","text":"","title":"Solution"},{"location":"stream-processing/event-stream-merger/#implementation","text":"The Kafka Streams DSL provides a merge operator which merges two streams into a single stream. You can then take the merged stream and use any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()...","title":"Implementation"},{"location":"stream-processing/event-stream-merger/#considerations","text":"Kafka Streams provides no guarantees on the processing order of records from the underlying streams. When merging streams the key and value types must be the same.","title":"Considerations"},{"location":"stream-processing/event-stream-merger/#references","text":"Kafka Tutorial : Merging with Kafka Streams. Kafka Tutorial : Merging streams with ksqlDB.","title":"References"},{"location":"stream-processing/logical-and/","text":"Logical AND Event Streams become more interesting when they're considered together. It's often the case that when two separate Events occur, it triggers a new fact that we want to capture. A product can only be dispatched when there's an order and a successful payment. If someone places a bet and their horse wins, then we transfer money to them. How do we combine information from several different event streams and use them to make new events? Problem How can an application trigger processing when two (or more) related events arrive on different streams? Solution Multiple streams of events can be joined together, similar to joins in a relational database. We watch the streams and remember their most recent events (e.g., via an in-memory cache, a local or network storage device) for a certain amount of time. Whenever a new event arrives, we consider it alongside the other recently-captured events and look for matches. If we find one, we emit a new event. For stream-stream joins, it's important to think about what we consider to be a \"recent\" event. We can't join brand new events with arbitrarily-old ones - to join potentially-infinite streams would require potentially-infinite memory. Instead we decide on a retention period that counts as \"new enough\", and only hold on to events in that period. This is often just fine - a payment will usually happen soon after a order is placed. If it doesn't go through within the hour, we can reasonably expect a different process to chase the user for updated credit card details. Implementation As an example, imagine a bank that captures logins to their website, and withdrawals from an ATM. The fraud department might be keen to hear if the same user_id logs in in one country, and makes a withdrawal in a different country, within the same day. (This would not necessarily be fraud, but it's certainly suspicious!) To implement this example, we'll use ksqlDB. We start with two event streams: -- For simplicity's sake, we'll assume that IP addresses -- have already been converted into country codes. CREATE OR REPLACE STREAM logins ( user_id BIGINT, country_code VARCHAR ) WITH ( KAFKA_TOPIC = 'logins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); CREATE OR REPLACE STREAM withdrawals ( user_id BIGINT, country_code VARCHAR, amount DECIMAL(10,2), success BOOLEAN ) WITH ( KAFKA_TOPIC = 'withdrawals_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); We can now join those two streams. Events with the same user_id are considered equal, and we'll specifically look at events that happen WITHIN 1 DAY : CREATE STREAM possible_frauds AS SELECT l.user_id, l.country_code, w.country_code, w.amount, w.success FROM logins l JOIN withdrawals w WITHIN 1 DAY ON l.user_id = w.user_id WHERE l.country_code != w.country_code EMIT CHANGES; Querying that stream in one terminal: SELECT * FROM possible_frauds EMIT CHANGES; ...and inserting some data in another: INSERT INTO logins (user_id, country_code) VALUES (1, 'gb'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO logins (user_id, country_code) VALUES (3, 'be'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'gb', 10.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'au', 250.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'us', 50.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (3, 'be', 20.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'fr', 20.00, true); Results in a stream of possible fraud cases that need further investigation: +-----------+----------------+----------------+--------+---------+ |L_USER_ID |L_COUNTRY_CODE |W_COUNTRY_CODE |AMOUNT |SUCCESS | +-----------+----------------+----------------+--------+---------+ |1 |gb |au |250.00 |true | |2 |us |fr |20.00 |true | |2 |us |fr |20.00 |true | Considerations Joining event streams is fairly simple. The big consideration is how large a retention period you need, and so the resources your join will use. Planning that tradeoff requires careful consideration of the specific problem you're solving. For large retention periods, consider joining a stream to a Projection Table instead. References See also: Joining Streams and Tables in the ksqlDB documentation. The Pipeline pattern, for considering events in series (rather than in parallel). The Projection Table pattern, for a memory-efficient way of considering a stream over a potentially-infinite time-period. Designing Event Driven Systems , chapter 14 for further discussion.","title":"Logical AND"},{"location":"stream-processing/logical-and/#logical-and","text":"Event Streams become more interesting when they're considered together. It's often the case that when two separate Events occur, it triggers a new fact that we want to capture. A product can only be dispatched when there's an order and a successful payment. If someone places a bet and their horse wins, then we transfer money to them. How do we combine information from several different event streams and use them to make new events?","title":"Logical AND"},{"location":"stream-processing/logical-and/#problem","text":"How can an application trigger processing when two (or more) related events arrive on different streams?","title":"Problem"},{"location":"stream-processing/logical-and/#solution","text":"Multiple streams of events can be joined together, similar to joins in a relational database. We watch the streams and remember their most recent events (e.g., via an in-memory cache, a local or network storage device) for a certain amount of time. Whenever a new event arrives, we consider it alongside the other recently-captured events and look for matches. If we find one, we emit a new event. For stream-stream joins, it's important to think about what we consider to be a \"recent\" event. We can't join brand new events with arbitrarily-old ones - to join potentially-infinite streams would require potentially-infinite memory. Instead we decide on a retention period that counts as \"new enough\", and only hold on to events in that period. This is often just fine - a payment will usually happen soon after a order is placed. If it doesn't go through within the hour, we can reasonably expect a different process to chase the user for updated credit card details.","title":"Solution"},{"location":"stream-processing/logical-and/#implementation","text":"As an example, imagine a bank that captures logins to their website, and withdrawals from an ATM. The fraud department might be keen to hear if the same user_id logs in in one country, and makes a withdrawal in a different country, within the same day. (This would not necessarily be fraud, but it's certainly suspicious!) To implement this example, we'll use ksqlDB. We start with two event streams: -- For simplicity's sake, we'll assume that IP addresses -- have already been converted into country codes. CREATE OR REPLACE STREAM logins ( user_id BIGINT, country_code VARCHAR ) WITH ( KAFKA_TOPIC = 'logins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); CREATE OR REPLACE STREAM withdrawals ( user_id BIGINT, country_code VARCHAR, amount DECIMAL(10,2), success BOOLEAN ) WITH ( KAFKA_TOPIC = 'withdrawals_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); We can now join those two streams. Events with the same user_id are considered equal, and we'll specifically look at events that happen WITHIN 1 DAY : CREATE STREAM possible_frauds AS SELECT l.user_id, l.country_code, w.country_code, w.amount, w.success FROM logins l JOIN withdrawals w WITHIN 1 DAY ON l.user_id = w.user_id WHERE l.country_code != w.country_code EMIT CHANGES; Querying that stream in one terminal: SELECT * FROM possible_frauds EMIT CHANGES; ...and inserting some data in another: INSERT INTO logins (user_id, country_code) VALUES (1, 'gb'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO logins (user_id, country_code) VALUES (3, 'be'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'gb', 10.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'au', 250.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'us', 50.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (3, 'be', 20.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'fr', 20.00, true); Results in a stream of possible fraud cases that need further investigation: +-----------+----------------+----------------+--------+---------+ |L_USER_ID |L_COUNTRY_CODE |W_COUNTRY_CODE |AMOUNT |SUCCESS | +-----------+----------------+----------------+--------+---------+ |1 |gb |au |250.00 |true | |2 |us |fr |20.00 |true | |2 |us |fr |20.00 |true |","title":"Implementation"},{"location":"stream-processing/logical-and/#considerations","text":"Joining event streams is fairly simple. The big consideration is how large a retention period you need, and so the resources your join will use. Planning that tradeoff requires careful consideration of the specific problem you're solving. For large retention periods, consider joining a stream to a Projection Table instead.","title":"Considerations"},{"location":"stream-processing/logical-and/#references","text":"See also: Joining Streams and Tables in the ksqlDB documentation. The Pipeline pattern, for considering events in series (rather than in parallel). The Projection Table pattern, for a memory-efficient way of considering a stream over a potentially-infinite time-period. Designing Event Driven Systems , chapter 14 for further discussion.","title":"References"},{"location":"stream-processing/wait-for-n-events/","text":"Wait for N Events Sometimes Events become significant after they've happened several times. A user can try to log in 5 times, but after that we'll lock their account. A parcel delivery will be attempted 3 times before asking the customer to collect it from the depot. A gamer gets a trophy after they've killed their 100th Blarg. How do we efficiently watch for logically similar events? Problem How can an application wait for a certain number of events to occur before performing processing? Solution To consider related events as a group, we need to group them by a given key, and then count the occurrences of that key. Implementation In ksqlDB we can easily create a Projection Table that groups and counts events by a particular key. As an example, imagine we are handling very large financial transactions. We only want to process them once they've been reviewed and approved by 2 managers. We'll start with a stream of signed events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Querying that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; ...and inserting some data in another: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); Results in a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 | Considerations Note that in the example above, we queried for an exact number of approvals WHERE approvals = 2 . We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ) but that would have emitted a new event for a 3rd approval, and a 4th, and so on. That would be the wrong behavior here, but it might be useful feature in a system where you wanted to reward loyal customers, and send out a discount email for every order after their first 10. References The Event Grouping pattern, for a more general consideration of GROUP BY operations. Designing Event Driven Systems , chapter 15 for further discussion.","title":"Wait for N Events"},{"location":"stream-processing/wait-for-n-events/#wait-for-n-events","text":"Sometimes Events become significant after they've happened several times. A user can try to log in 5 times, but after that we'll lock their account. A parcel delivery will be attempted 3 times before asking the customer to collect it from the depot. A gamer gets a trophy after they've killed their 100th Blarg. How do we efficiently watch for logically similar events?","title":"Wait for N Events"},{"location":"stream-processing/wait-for-n-events/#problem","text":"How can an application wait for a certain number of events to occur before performing processing?","title":"Problem"},{"location":"stream-processing/wait-for-n-events/#solution","text":"To consider related events as a group, we need to group them by a given key, and then count the occurrences of that key.","title":"Solution"},{"location":"stream-processing/wait-for-n-events/#implementation","text":"In ksqlDB we can easily create a Projection Table that groups and counts events by a particular key. As an example, imagine we are handling very large financial transactions. We only want to process them once they've been reviewed and approved by 2 managers. We'll start with a stream of signed events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Querying that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; ...and inserting some data in another: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); Results in a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 |","title":"Implementation"},{"location":"stream-processing/wait-for-n-events/#considerations","text":"Note that in the example above, we queried for an exact number of approvals WHERE approvals = 2 . We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ) but that would have emitted a new event for a 3rd approval, and a 4th, and so on. That would be the wrong behavior here, but it might be useful feature in a system where you wanted to reward loyal customers, and send out a discount email for every order after their first 10.","title":"Considerations"},{"location":"stream-processing/wait-for-n-events/#references","text":"The Event Grouping pattern, for a more general consideration of GROUP BY operations. Designing Event Driven Systems , chapter 15 for further discussion.","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Problem Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested. Solution Pattern Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. Example Implementation By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); References Kafka Tutorial : Event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution-pattern","text":"Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Solution Pattern"},{"location":"stream-processing/wallclock-time/#example-implementation","text":"By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime');","title":"Example Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"Kafka Tutorial : Event-time semantics","title":"References"},{"location":"table/projection-table/","text":"Projection Table One of the first questions we want to ask of a stream of events is, \"Where are we now?\" We have a stream of sales events, we'd like to have the total sales figures at our fingertips. We have a stream of login events, we'd like to know when each user last logged in. Our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we roll up data efficiently? How do we preserve a complete event log and enjoy the fast queries of an \"update in place\"-style database? Problem How can a stream of change events be summarized into the current state of the world, efficiently? Solution We can maintain projection tables that behave just like materialized views in a traditional database. As new events come in, the table is automatically updated, giving us an always-live picture of the system. Events with the same key are considered related, with newer events being interpreted as updates or deletions (depending on their contents) of older events. Like a materialized view, projection tables are read-only. To change them, we change the underlying data by recording new events to their underlying streams. Implementation ksqlDB supports easy creation of summary tables/materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine we are shipping packages around the world. As they reach each point on their journey, they're logged with their current location. We'll start with a stream of check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and the newest location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Querying that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and inserting some data in another: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); Results in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, package_locations stays updated, so we can see the current location of each package without scanning through the event history every time. Considerations In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, in turn ensuring that for a given package_id , newer events have a higher offset value. Thus when we query for the LATEST_BY_OFFSET , we're always getting the newest event for each package. If we'd chosen a different partitioning key, or not specified one at all, we'd get very different results. LATEST_BY_OFFSET is only one of the many summary functions ksqlDB supports , from simple sums and averages to time-aware functions and histograms. And beyond those, you can easily define your own custom functions or look to Kafka Streams for complete control. References Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. Related patterns: State Table","title":"Projection Table"},{"location":"table/projection-table/#projection-table","text":"One of the first questions we want to ask of a stream of events is, \"Where are we now?\" We have a stream of sales events, we'd like to have the total sales figures at our fingertips. We have a stream of login events, we'd like to know when each user last logged in. Our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we roll up data efficiently? How do we preserve a complete event log and enjoy the fast queries of an \"update in place\"-style database?","title":"Projection Table"},{"location":"table/projection-table/#problem","text":"How can a stream of change events be summarized into the current state of the world, efficiently?","title":"Problem"},{"location":"table/projection-table/#solution","text":"We can maintain projection tables that behave just like materialized views in a traditional database. As new events come in, the table is automatically updated, giving us an always-live picture of the system. Events with the same key are considered related, with newer events being interpreted as updates or deletions (depending on their contents) of older events. Like a materialized view, projection tables are read-only. To change them, we change the underlying data by recording new events to their underlying streams.","title":"Solution"},{"location":"table/projection-table/#implementation","text":"ksqlDB supports easy creation of summary tables/materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine we are shipping packages around the world. As they reach each point on their journey, they're logged with their current location. We'll start with a stream of check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and the newest location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Querying that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and inserting some data in another: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); Results in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, package_locations stays updated, so we can see the current location of each package without scanning through the event history every time.","title":"Implementation"},{"location":"table/projection-table/#considerations","text":"In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, in turn ensuring that for a given package_id , newer events have a higher offset value. Thus when we query for the LATEST_BY_OFFSET , we're always getting the newest event for each package. If we'd chosen a different partitioning key, or not specified one at all, we'd get very different results. LATEST_BY_OFFSET is only one of the many summary functions ksqlDB supports , from simple sums and averages to time-aware functions and histograms. And beyond those, you can easily define your own custom functions or look to Kafka Streams for complete control.","title":"Considerations"},{"location":"table/projection-table/#references","text":"Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. Related patterns: State Table","title":"References"}]}