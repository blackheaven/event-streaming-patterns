{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Welcome to Event Streaming Patterns"},{"location":"compositional-patterns/event-collaboration/","text":"Event Collaboration Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states. Problem How can we build a distributed workflow in a way that allows components to evolve independently? Solution Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete. Considerations In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asyncronously by way of a global identifier which traverses the workflow within the events. References Event Collaboration by Martin Fowler","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#event-collaboration","text":"Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states.","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#problem","text":"How can we build a distributed workflow in a way that allows components to evolve independently?","title":"Problem"},{"location":"compositional-patterns/event-collaboration/#solution","text":"Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete.","title":"Solution"},{"location":"compositional-patterns/event-collaboration/#considerations","text":"In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asyncronously by way of a global identifier which traverses the workflow within the events.","title":"Considerations"},{"location":"compositional-patterns/event-collaboration/#references","text":"Event Collaboration by Martin Fowler","title":"References"},{"location":"compositional-patterns/geo-replication/","text":"Geo Replication Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply. Problem How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others? Solution Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data. Implementation Practically, replication is not enabled completely on all data streams, as there are always exceptions, organizational limitations, technical constraints, or other reasons why you wouldn't want to copy absolutely everything. Instead, you can do this on a per topic basis, where you can map a source topic to a destination topic. With Apache Kafka, you can do this in one of several ways. Option 1: Cluster Linking Cluster Linking enables easy data sharing between event streaming platforms, mirroring topics across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ, Active MQ, etc., provide similar functionality but without the same levels of parallelism. Option 2: Connect-based Replication Operators can set up such inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers. Considerations Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain. References This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#geo-replication","text":"Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply.","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#problem","text":"How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others?","title":"Problem"},{"location":"compositional-patterns/geo-replication/#solution","text":"Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data.","title":"Solution"},{"location":"compositional-patterns/geo-replication/#implementation","text":"Practically, replication is not enabled completely on all data streams, as there are always exceptions, organizational limitations, technical constraints, or other reasons why you wouldn't want to copy absolutely everything. Instead, you can do this on a per topic basis, where you can map a source topic to a destination topic. With Apache Kafka, you can do this in one of several ways.","title":"Implementation"},{"location":"compositional-patterns/geo-replication/#option-1-cluster-linking","text":"Cluster Linking enables easy data sharing between event streaming platforms, mirroring topics across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ, Active MQ, etc., provide similar functionality but without the same levels of parallelism.","title":"Option 1: Cluster Linking"},{"location":"compositional-patterns/geo-replication/#option-2-connect-based-replication","text":"Operators can set up such inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers.","title":"Option 2: Connect-based Replication"},{"location":"compositional-patterns/geo-replication/#considerations","text":"Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain.","title":"Considerations"},{"location":"compositional-patterns/geo-replication/#references","text":"This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event/event-envelope/","text":"Event Envelope Event Streaming Platforms allow many different types of applications to work together. Event Envelopes provide a standard set of well-known fields across all Event s sent through the Event Streaming Applications . The envelope is independent of the underlying event format and often references attributes such as the encryption type, schema, key, serialization format. Envelopes are analogous to protocol headers in networking (TCP-IP etc.) Problem How to convey information to all participants in an Event Streaming Platform independently of the event payload, e.g. how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event? Solution Use an Event Envelope to wrap the event data using a standard format agreed by all participants of the Event Streaming Platform or more broadly. Cloud Events \u2013which standardize access to ID, Schema, Key, and other common event attributes\u2013are an industry-standard example of the Event Envelope pattern. Example Implementation Using the basic Java consumers and producers, a helper function could be used to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform . static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; } References This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf How to Choose Between Strict and Dynamic Schemas See Cloud Events for a specification on describing event header information in a common way.","title":"Event Envelope"},{"location":"event/event-envelope/#event-envelope","text":"Event Streaming Platforms allow many different types of applications to work together. Event Envelopes provide a standard set of well-known fields across all Event s sent through the Event Streaming Applications . The envelope is independent of the underlying event format and often references attributes such as the encryption type, schema, key, serialization format. Envelopes are analogous to protocol headers in networking (TCP-IP etc.)","title":"Event Envelope"},{"location":"event/event-envelope/#problem","text":"How to convey information to all participants in an Event Streaming Platform independently of the event payload, e.g. how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event?","title":"Problem"},{"location":"event/event-envelope/#solution","text":"Use an Event Envelope to wrap the event data using a standard format agreed by all participants of the Event Streaming Platform or more broadly. Cloud Events \u2013which standardize access to ID, Schema, Key, and other common event attributes\u2013are an industry-standard example of the Event Envelope pattern.","title":"Solution"},{"location":"event/event-envelope/#example-implementation","text":"Using the basic Java consumers and producers, a helper function could be used to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform . static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; }","title":"Example Implementation"},{"location":"event/event-envelope/#references","text":"This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf How to Choose Between Strict and Dynamic Schemas See Cloud Events for a specification on describing event header information in a common way.","title":"References"},{"location":"event/event/","text":"Event Events represent facts and can help facilitate decoupled applications, services, and systems exchanging data across an Event Streaming Platform . Problem How do I represent a fact about something that has happened? Solution An event represents an immutable fact about something that happened. Examples of Events might be: orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains at least one or more data fields that describe the fact, as well as a timestamp that denotes when this Event was created by its Event Source . The Event may also contain various metadata about itself, such as its source of origin (e.g., the application or cloud services that created the event) and storage-level information (e.g., its position in the event stream). Considerations To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event Schemas are commonly defined in Avro , Protobuf , or JSON schema . For cloud-based architectures, you may want to evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (think: instructions, actions) that an Event Processor reading the events should carry out. See the Command Event for details. References This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event"},{"location":"event/event/#event","text":"Events represent facts and can help facilitate decoupled applications, services, and systems exchanging data across an Event Streaming Platform .","title":"Event"},{"location":"event/event/#problem","text":"How do I represent a fact about something that has happened?","title":"Problem"},{"location":"event/event/#solution","text":"An event represents an immutable fact about something that happened. Examples of Events might be: orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains at least one or more data fields that describe the fact, as well as a timestamp that denotes when this Event was created by its Event Source . The Event may also contain various metadata about itself, such as its source of origin (e.g., the application or cloud services that created the event) and storage-level information (e.g., its position in the event stream).","title":"Solution"},{"location":"event/event/#considerations","text":"To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event Schemas are commonly defined in Avro , Protobuf , or JSON schema . For cloud-based architectures, you may want to evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (think: instructions, actions) that an Event Processor reading the events should carry out. See the Command Event for details.","title":"Considerations"},{"location":"event/event/#references","text":"This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processors may need to operate over a subset of Events over a particular Event Stream . Problem How can an application discard uninteresting events? Solution Implementation The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processors may need to operate over a subset of Events over a particular Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Pattern Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Example Implementation TODO: Example? Considerations The mapper may optionally validate the schema of the converted objects, see \"Schema Validation\". References TODO: Reference for Schema Validation Domain Model","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution-pattern","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution Pattern"},{"location":"event-processing/event-mapper/#example-implementation","text":"TODO: Example?","title":"Example Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"The mapper may optionally validate the schema of the converted objects, see \"Schema Validation\".","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"TODO: Reference for Schema Validation Domain Model","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied. Problem TODO: Technology agnostic English explanation of the problem Solution TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first). Implementation TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code) Considerations TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"TODO: Technology agnostic English explanation of the problem","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first).","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code)","title":"Implementation"},{"location":"event-processing/event-processing-application/#considerations","text":"TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied. Problem TODO: Technology agnostic English explanation of the problem Solution TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first). Implementation TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code) Considerations TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern. References TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"TODO: Short (2 sentence max) description that describes the pattern generally. Consider explaining situations in which the pattern might be applied.","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"TODO: Technology agnostic English explanation of the problem","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"TODO: Provide a technology agnostic diagram and supporting text explaining the pattern's implementation (placing the diagram first).","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"TODO: Technology specific code example, ksqlDB preferred. (Not every pattern will have code)","title":"Implementation"},{"location":"event-processing/event-processor/#considerations","text":"TODO: Technology specific reflection on implmenting the pattern 'in the real world'. Considerations may include optional subsequent decisions or consequences of implementing the pattern.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"TODO: Reference link to the EIP pattern as citation TODO: pointers to related patterns? TODO: pointers to external material?","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Problem How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems? Solution Pattern Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes. Example Implementation CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor()); References Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How do I handle a situation where the implementation of a single logical function (e.g., inventory check) is spread across multiple physical systems?","title":"Problem"},{"location":"event-processing/event-router/#solution-pattern","text":"Use the TopicNameExtractor to determine the topic to send records to. The TopicNameExtractor has one method, extract , which accepts three parameters: The Key of the record The Value of the record The RecordContext You can use any or all of these three to pull the required information to route records to different topics at runtime. The RecordContext provides access to the headers of the record, which can contain user provided information for routing purposes.","title":"Solution Pattern"},{"location":"event-processing/event-router/#example-implementation","text":"CustomExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { // Assuming the first ten characters of the key // contains the information determining where // Kafka Streams forwards the record. return key.substring(0,10); } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to( new CustomExtractor());","title":"Example Implementation"},{"location":"event-processing/event-router/#references","text":"Kafka Tutorial : How to dynamically choose the output topic at runtime","title":"References"},{"location":"event-processing/event-splitter/","text":"Event Splitter One Event may actually contain multiple child events within it, each of which may need to be processed in different ways. Problem How can an Event be split into multiple events? Solution First, split the original event into multiple child events. Then, publish one event per child. Implementation Many event processing technologies support this operation. ksqlDB has the EXPLODE() table function which takes an array and outputs one value for each of the elements of the array. The example below processes each input event, un-nesting the array and generating new events for each element with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; Kafka Streams has an analogous method called flatMap() . The example below processes each input event, generating new events with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } ); Considerations If child events need to be routed to different streams, see Event Router for routing events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: Your use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier. References This pattern is derived from Splitter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Splitter"},{"location":"event-processing/event-splitter/#event-splitter","text":"One Event may actually contain multiple child events within it, each of which may need to be processed in different ways.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#problem","text":"How can an Event be split into multiple events?","title":"Problem"},{"location":"event-processing/event-splitter/#solution","text":"First, split the original event into multiple child events. Then, publish one event per child.","title":"Solution"},{"location":"event-processing/event-splitter/#implementation","text":"Many event processing technologies support this operation. ksqlDB has the EXPLODE() table function which takes an array and outputs one value for each of the elements of the array. The example below processes each input event, un-nesting the array and generating new events for each element with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; Kafka Streams has an analogous method called flatMap() . The example below processes each input event, generating new events with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } );","title":"Implementation"},{"location":"event-processing/event-splitter/#considerations","text":"If child events need to be routed to different streams, see Event Router for routing events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: Your use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier.","title":"Considerations"},{"location":"event-processing/event-splitter/#references","text":"This pattern is derived from Splitter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/event-streaming-api/","text":"Event Streaming API Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way. Problem How can my application connect to an Event Streaming Platform to send and receive Events ? Solution The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The highler level libraries allow the application to focus on business logic leaving the details of the platform communication to the API. References This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#event-streaming-api","text":"Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#problem","text":"How can my application connect to an Event Streaming Platform to send and receive Events ?","title":"Problem"},{"location":"event-processing/event-streaming-api/#solution","text":"The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The highler level libraries allow the application to focus on business logic leaving the details of the platform communication to the API.","title":"Solution"},{"location":"event-processing/event-streaming-api/#references","text":"This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"References"},{"location":"event-processing/event-translator/","text":"Event Translator Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them. Problem How can systems using different data formats communicate with each other using Events ? Solution An Event Translator converts a data format into a standard format familiar to downstream Event Processors . This can take the form of field manipulation, for example mapping one event schema (ref) to another event schema. Another common form is different serialization types, for example, translating Avro to Json or Protobuf to Avro. Implementation The streaming database ksqlDB provides the ability to create Event Streams with SQL statements. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream Considerations In some cases translations will be unidirectional if data is lost, for example translating XML to json will often lose information meaning the original form cannot be recreated. References This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Translator"},{"location":"event-processing/event-translator/#event-translator","text":"Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them.","title":"Event Translator"},{"location":"event-processing/event-translator/#problem","text":"How can systems using different data formats communicate with each other using Events ?","title":"Problem"},{"location":"event-processing/event-translator/#solution","text":"An Event Translator converts a data format into a standard format familiar to downstream Event Processors . This can take the form of field manipulation, for example mapping one event schema (ref) to another event schema. Another common form is different serialization types, for example, translating Avro to Json or Protobuf to Avro.","title":"Solution"},{"location":"event-processing/event-translator/#implementation","text":"The streaming database ksqlDB provides the ability to create Event Streams with SQL statements. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream","title":"Implementation"},{"location":"event-processing/event-translator/#considerations","text":"In some cases translations will be unidirectional if data is lost, for example translating XML to json will often lose information meaning the original form cannot be recreated.","title":"Considerations"},{"location":"event-processing/event-translator/#references","text":"This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations. Problem How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ? Solution Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system. Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink . Considerations There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations.","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ?","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution","text":"Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system.","title":"Solution"},{"location":"event-sink/event-sink-connector/#implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Kafka, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink .","title":"Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink. Problem How can we read (or consume / subscribe to) Events in an Event Streaming Platform ? Solution Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB . Implementation ksqlDB example: Reading events from an existing Kafka topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } References The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink.","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can we read (or consume / subscribe to) Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-sink/event-sink/#solution","text":"Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB .","title":"Solution"},{"location":"event-sink/event-sink/#implementation","text":"ksqlDB example: Reading events from an existing Kafka topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); }","title":"Implementation"},{"location":"event-sink/event-sink/#references","text":"The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Problem How do I update a value in a database and create an associated event with the least amount of effort? Solution Pattern Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails). Example Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled. References TODO: Add references?","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event with the least amount of effort?","title":"Problem"},{"location":"event-source/database-write-aside/#solution-pattern","text":"Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails).","title":"Solution Pattern"},{"location":"event-source/database-write-aside/#example-implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Example Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-aside/#references","text":"TODO: Add references?","title":"References"},{"location":"event-source/database-write-through/","text":"Database Write Through Problem How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform. Solution Pattern Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Example Implementation TODO: Example for CDC? Considerations The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform.","title":"Problem"},{"location":"event-source/database-write-through/#solution-pattern","text":"Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution Pattern"},{"location":"event-source/database-write-through/#example-implementation","text":"TODO: Example for CDC?","title":"Example Implementation"},{"location":"event-source/database-write-through/#considerations","text":"The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources . Problem How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events . Solution Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform . Implementation When connecting a cloud services and traditional systems to Apache Kafka , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username'); Considerations End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery, cf. the Guaranteed Delivery pattern) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform. References This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources .","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events .","title":"Problem"},{"location":"event-source/event-source-connector/#solution","text":"Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform .","title":"Solution"},{"location":"event-source/event-source-connector/#implementation","text":"When connecting a cloud services and traditional systems to Apache Kafka , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username');","title":"Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery, cf. the Guaranteed Delivery pattern) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Problem How can I record and distribute events generated by my application? Solution Pattern Example Implementation INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A'); References ksqlDB The event streaming database purpose-built for stream processing applications.","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can I record and distribute events generated by my application?","title":"Problem"},{"location":"event-source/event-source/#solution-pattern","text":"","title":"Solution Pattern"},{"location":"event-source/event-source/#example-implementation","text":"INSERT INTO foo (ROWTIME, KEY_COL, COL_A) VALUES (1510923225000, 'key', 'A');","title":"Example Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications.","title":"References"},{"location":"event-source/schema-validator/","text":"Schema Validator In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process these events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events, which is an important aspect of putting Data Contracts in place for data governance purposes. Problem How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream? Solution Validate whether an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. Such schema validation can be done: On the server side by the Event Streaming Platform that receives the Event . Events that fail schema validation and thus violate the Data Contract are rejected. On the client side by the Event Source that creates the Event . For example, an Event Source Connector can validate Events prior to ingestion into the Event Streaming Platform . Or, an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (e.g., Confluent's serializer/deserializers for Kafka). Implementation With Confluent, schema validation is fully supported with a per-environment hosted Schema Registry . Use the cloud UI to enable schema registry in your cloud provider of choice. Schemas can be managed per topic using the cloud UI or the Confluent Cloud CLI . An example command to create a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO Considerations Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write. Instead, consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (e.g., in regulated industries). Schema validation results in a load increase because it impacts the write path of every event. Client-side validation impacts primarily the load of the client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ). References See the Schema Compatibility pattern for information on how schemas can evolve over time and be verified. Learn more how to Manage and Validate Schemas with Confluent and Kafka .","title":"Schema Validator"},{"location":"event-source/schema-validator/#schema-validator","text":"In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process these events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events, which is an important aspect of putting Data Contracts in place for data governance purposes.","title":"Schema Validator"},{"location":"event-source/schema-validator/#problem","text":"How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream?","title":"Problem"},{"location":"event-source/schema-validator/#solution","text":"Validate whether an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. Such schema validation can be done: On the server side by the Event Streaming Platform that receives the Event . Events that fail schema validation and thus violate the Data Contract are rejected. On the client side by the Event Source that creates the Event . For example, an Event Source Connector can validate Events prior to ingestion into the Event Streaming Platform . Or, an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (e.g., Confluent's serializer/deserializers for Kafka).","title":"Solution"},{"location":"event-source/schema-validator/#implementation","text":"With Confluent, schema validation is fully supported with a per-environment hosted Schema Registry . Use the cloud UI to enable schema registry in your cloud provider of choice. Schemas can be managed per topic using the cloud UI or the Confluent Cloud CLI . An example command to create a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO","title":"Implementation"},{"location":"event-source/schema-validator/#considerations","text":"Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write. Instead, consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (e.g., in regulated industries). Schema validation results in a load increase because it impacts the write path of every event. Client-side validation impacts primarily the load of the client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ).","title":"Considerations"},{"location":"event-source/schema-validator/#references","text":"See the Schema Compatibility pattern for information on how schemas can evolve over time and be verified. Learn more how to Manage and Validate Schemas with Confluent and Kafka .","title":"References"},{"location":"event-storage/infiite-retention-event-stream/","text":"Infinite Retention Event Stream Problem How can an operator ensure that events in a stream are retained forever? Solution Pattern Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power. Example Implementation confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.s3.bucket=<BUCKET_NAME> confluent.tier.s3.region=<REGION> # Confluent also supports using Google Cloud Storage (GCS)","title":"Infinite Retention Event Stream"},{"location":"event-storage/infiite-retention-event-stream/#infinite-retention-event-stream","text":"","title":"Infinite Retention Event Stream"},{"location":"event-storage/infiite-retention-event-stream/#problem","text":"How can an operator ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infiite-retention-event-stream/#solution-pattern","text":"Confluent adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived records are considered \"hot\", but as time moves on, they get \"warm\" and migrate to more cost-effective external storage like an S3 bucket. By separating storage from compute, operators only need to add brokers to increase compute power.","title":"Solution Pattern"},{"location":"event-storage/infiite-retention-event-stream/#example-implementation","text":"confluent.tier.feature=true confluent.tier.enable=true confluent.tier.backend=S3 confluent.tier.s3.bucket=<BUCKET_NAME> confluent.tier.s3.region=<REGION> # Confluent also supports using Google Cloud Storage (GCS)","title":"Example Implementation"},{"location":"stream-processing/event-grouper/","text":"Event Grouper Problem How can I group individual but related events from the same stream/table so that they can subsequently be processed as a whole? Solution Pattern ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. Example Implementation SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; References Session Windows Hopping Windows Tumbling Windows","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can I group individual but related events from the same stream/table so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution-pattern","text":"ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window.","title":"Solution Pattern"},{"location":"stream-processing/event-grouper/#example-implementation","text":"SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Example Implementation"},{"location":"stream-processing/event-grouper/#references","text":"Session Windows Hopping Windows Tumbling Windows","title":"References"},{"location":"stream-processing/event-stream-merger/","text":"Event Stream Merger An Event Streaming Application may contain multiple Event Stream instances. But in some cases it may make sense for the application to merge the different event streams into a single event stream, without changing the individual events. While this may seem logically related to a join, the merge is a completely different operation. A join produces results by combining events with the same key to produce a new event, possibly of a different type. Whereas the merge combines the events from multiple streams into a single stream, but the individual events are unchanged and remain independent of each other. Problem How can an application merge different event streams? Solution Implementation The Kafka Streams DSL provides a merge operator which merges two streams into a single stream. You can then take the merged stream and use any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()... Considerations Kafka Streams provides no guarantees on the processing order of records from the underlying streams. When merging streams the key and value types must be the same. References Kafka Tutorial : Merging with Kafka Streams. Kafka Tutorial : Merging streams with ksqlDB.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#event-stream-merger","text":"An Event Streaming Application may contain multiple Event Stream instances. But in some cases it may make sense for the application to merge the different event streams into a single event stream, without changing the individual events. While this may seem logically related to a join, the merge is a completely different operation. A join produces results by combining events with the same key to produce a new event, possibly of a different type. Whereas the merge combines the events from multiple streams into a single stream, but the individual events are unchanged and remain independent of each other.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#problem","text":"How can an application merge different event streams?","title":"Problem"},{"location":"stream-processing/event-stream-merger/#solution","text":"","title":"Solution"},{"location":"stream-processing/event-stream-merger/#implementation","text":"The Kafka Streams DSL provides a merge operator which merges two streams into a single stream. You can then take the merged stream and use any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()...","title":"Implementation"},{"location":"stream-processing/event-stream-merger/#considerations","text":"Kafka Streams provides no guarantees on the processing order of records from the underlying streams. When merging streams the key and value types must be the same.","title":"Considerations"},{"location":"stream-processing/event-stream-merger/#references","text":"Kafka Tutorial : Merging with Kafka Streams. Kafka Tutorial : Merging streams with ksqlDB.","title":"References"},{"location":"stream-processing/wait-for-n-events/","text":"Wait for N Events Sometimes Events become significant after they've happened several times. A user can try to log in 5 times, but after that we'll lock their account. A parcel delivery will be attempted 3 times before asking the customer to collect it from the depot. A gamer gets a trophy after they've killed their 100th Blarg. How do we efficiently watch for logically similar events? Problem How can an application wait for a certain number of events to occur before performing processing? Solution To consider related events as a group, we need to group them by a given key, and then count the occurrences of that key. Implementation In ksqlDB we can easily create a Projection Table that groups and counts events by a particular key. As an example, imagine we are handling very large financial transactions. We only want to process them once they've been reviewed and approved by 2 managers. We'll start with a stream of signed events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Querying that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; ...and inserting some data in another: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); Results in a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 | Considerations Note that in the example above, we queried for an exact number of approvals WHERE approvals = 2 . We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ) but that would have emitted a new event for a 3rd approval, and a 4th, and so on. That would be the wrong behavior here, but it might be useful feature in a system where you wanted to reward loyal customers, and send out a discount email for every order after their first 10. References The Event Grouping pattern, for a more general consideration of GROUP BY operations. Designing Event Driven Systems , chapter 15 for further discussion.","title":"Wait for N Events"},{"location":"stream-processing/wait-for-n-events/#wait-for-n-events","text":"Sometimes Events become significant after they've happened several times. A user can try to log in 5 times, but after that we'll lock their account. A parcel delivery will be attempted 3 times before asking the customer to collect it from the depot. A gamer gets a trophy after they've killed their 100th Blarg. How do we efficiently watch for logically similar events?","title":"Wait for N Events"},{"location":"stream-processing/wait-for-n-events/#problem","text":"How can an application wait for a certain number of events to occur before performing processing?","title":"Problem"},{"location":"stream-processing/wait-for-n-events/#solution","text":"To consider related events as a group, we need to group them by a given key, and then count the occurrences of that key.","title":"Solution"},{"location":"stream-processing/wait-for-n-events/#implementation","text":"In ksqlDB we can easily create a Projection Table that groups and counts events by a particular key. As an example, imagine we are handling very large financial transactions. We only want to process them once they've been reviewed and approved by 2 managers. We'll start with a stream of signed events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Querying that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; ...and inserting some data in another: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); Results in a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 |","title":"Implementation"},{"location":"stream-processing/wait-for-n-events/#considerations","text":"Note that in the example above, we queried for an exact number of approvals WHERE approvals = 2 . We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ) but that would have emitted a new event for a 3rd approval, and a 4th, and so on. That would be the wrong behavior here, but it might be useful feature in a system where you wanted to reward loyal customers, and send out a discount email for every order after their first 10.","title":"Considerations"},{"location":"stream-processing/wait-for-n-events/#references","text":"The Event Grouping pattern, for a more general consideration of GROUP BY operations. Designing Event Driven Systems , chapter 15 for further discussion.","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Problem Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested. Solution Pattern Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. Example Implementation By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); References Kafka Tutorial : Event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution-pattern","text":"Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Solution Pattern"},{"location":"stream-processing/wallclock-time/#example-implementation","text":"By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime');","title":"Example Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"Kafka Tutorial : Event-time semantics","title":"References"},{"location":"table/projection-table/","text":"Projection Table One of the first questions we want to ask of a stream of events is, \"Where are we now?\" We have a stream of sales events, we'd like to have the total sales figures at our fingertips. We have a stream of login events, we'd like to know when each user last logged in. Our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we roll up data efficiently? How do we preserve a complete event log and enjoy the fast queries of an \"update in place\"-style database? Problem How can a stream of change events be summarized into the current state of the world, efficiently? Solution We can maintain projection tables that behave just like materialized views in a traditional database. As new events come in, the table is automatically updated, giving us an always-live picture of the system. Events with the same key are considered related, with newer events being interpreted as updates or deletions (depending on their contents) of older events. Like a materialized view, projection tables are read-only. To change them, we change the underlying data by recording new events to their underlying streams. Implementation ksqlDB supports easy creation of summary tables/materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine we are shipping packages around the world. As they reach each point on their journey, they're logged with their current location. We'll start with a stream of check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and the newest location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Querying that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and inserting some data in another: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); Results in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, package_locations stays updated, so we can see the current location of each package without scanning through the event history every time. Considerations In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, in turn ensuring that for a given package_id , newer events have a higher offset value. Thus when we query for the LATEST_BY_OFFSET , we're always getting the newest event for each package. If we'd chosen a different partitioning key, or not specified one at all, we'd get very different results. LATEST_BY_OFFSET is only one of the many summary functions ksqlDB supports , from simple sums and averages to time-aware functions and histograms. And beyond those, you can easily define your own custom functions or look to Kafka Streams for complete control. References Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. Related patterns: State Table","title":"Projection Table"},{"location":"table/projection-table/#projection-table","text":"One of the first questions we want to ask of a stream of events is, \"Where are we now?\" We have a stream of sales events, we'd like to have the total sales figures at our fingertips. We have a stream of login events, we'd like to know when each user last logged in. Our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we roll up data efficiently? How do we preserve a complete event log and enjoy the fast queries of an \"update in place\"-style database?","title":"Projection Table"},{"location":"table/projection-table/#problem","text":"How can a stream of change events be summarized into the current state of the world, efficiently?","title":"Problem"},{"location":"table/projection-table/#solution","text":"We can maintain projection tables that behave just like materialized views in a traditional database. As new events come in, the table is automatically updated, giving us an always-live picture of the system. Events with the same key are considered related, with newer events being interpreted as updates or deletions (depending on their contents) of older events. Like a materialized view, projection tables are read-only. To change them, we change the underlying data by recording new events to their underlying streams.","title":"Solution"},{"location":"table/projection-table/#implementation","text":"ksqlDB supports easy creation of summary tables/materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine we are shipping packages around the world. As they reach each point on their journey, they're logged with their current location. We'll start with a stream of check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and the newest location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Querying that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and inserting some data in another: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); Results in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, package_locations stays updated, so we can see the current location of each package without scanning through the event history every time.","title":"Implementation"},{"location":"table/projection-table/#considerations","text":"In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, in turn ensuring that for a given package_id , newer events have a higher offset value. Thus when we query for the LATEST_BY_OFFSET , we're always getting the newest event for each package. If we'd chosen a different partitioning key, or not specified one at all, we'd get very different results. LATEST_BY_OFFSET is only one of the many summary functions ksqlDB supports , from simple sums and averages to time-aware functions and histograms. And beyond those, you can easily define your own custom functions or look to Kafka Streams for complete control.","title":"Considerations"},{"location":"table/projection-table/#references","text":"Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. Related patterns: State Table","title":"References"}]}